{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e4f687",
   "metadata": {},
   "source": [
    "### NOTEBOOK 1: APD DATA PIPELINE - STANDARDIZE, COMBINE, FEATURE ENGINEERING & EDA\n",
    "\n",
    "Save as: `01_wrangler.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496568f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports =================================================================\n",
    "# Required libraries:\n",
    "# pip install numpy pandas geopandas matplotlib seaborn contextily python-dateutil holidays\n",
    "# pip install shapely openmeteo-requests requests-cache retry-requests rich plotly\n",
    "\n",
    "# --- Core Python / Utilities ---\n",
    "import re\n",
    "from math import radians, sin, cos, sqrt, asin, atan2\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "# --- Data Handling ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd # Added import needed for notebook consistency\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import contextily as ctx\n",
    "\n",
    "# --- Date/Time Utilities ---\n",
    "from dateutil import parser\n",
    "import holidays\n",
    "\n",
    "# --- Geospatial Processing ---\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# --- Weather API / Networking ---\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "# --- Rich Console Debugging / Output ---\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.panel import Panel\n",
    "from rich import print as rprint\n",
    "\n",
    "# --- Spatial Density ---\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Initialize console\n",
    "console = Console()\n",
    "\n",
    "console.print(\n",
    "    Panel.fit(\n",
    "        \"Libraries imported successfully.\\n\\n\"\n",
    "        \"Groups loaded:\\n\"\n",
    "        \"- Core Python: regex, math, typing, paths\\n\"\n",
    "        \"- Data Handling: pandas, numpy, geopandas\\n\"\n",
    "        \"- Visualization: matplotlib, seaborn, plotly, contextily\\n\"\n",
    "        \"- Date & Time: dateutil, holidays\\n\"\n",
    "        \"- Geospatial: geopandas, shapely\\n\"\n",
    "        \"- Weather / Networking: open-meteo, caching, retry\\n\"\n",
    "        \"- Rich Output: console, tables, panels\",\n",
    "        title=\"Import Summary\",\n",
    "        border_style=\"cyan\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba527f13",
   "metadata": {},
   "source": [
    "#### APD Crime Data Processing Pipeline\n",
    "\n",
    "1. Configuration and logger\n",
    "2. Ingest and standardize raw CSVs\n",
    "3. Combine and deduplicate\n",
    "4. Clean and basic feature engineering\n",
    "5. Geospatial enrichment\n",
    "6. Date/context features\n",
    "7. Weather enrichment\n",
    "8. Campus distance/label features\n",
    "9. Final cleaning, export, and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe268224",
   "metadata": {},
   "source": [
    "#### Section 1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6207bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Logging Setup\n",
    "\n",
    "pipeline_log: List[Dict[str, Any]] = []\n",
    "\n",
    "# FIX: Stabilized log_step for ValueError: Cannot specify ',' with 's'\n",
    "def log_step(step_name: str, df: pd.DataFrame) -> None:\n",
    "    \"\"\"Record a pipeline step with shape info, safely handling N/A.\"\"\"\n",
    "    \n",
    "    # 1. Determine values and set format type\n",
    "    if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "        rows_val = \"N/A\"\n",
    "        cols_val = \"N/A\"\n",
    "        rows_str = rows_val\n",
    "        cols_str = cols_val\n",
    "    else:\n",
    "        rows_val = int(df.shape[0])\n",
    "        cols_val = int(df.shape[1])\n",
    "        # Apply comma formatting only when values are integers\n",
    "        rows_str = f\"{rows_val:,}\"\n",
    "        cols_str = str(cols_val)\n",
    "\n",
    "    # 2. Append the original raw values to the log (int or string 'N/A')\n",
    "    pipeline_log.append({\"step\": step_name, \"rows\": rows_val, \"cols\": cols_val})\n",
    "    \n",
    "    # 3. Print the result using the safely formatted strings\n",
    "    console.print(f\"[green]âœ“ {step_name}[/green] [cyan]â†’ shape: {rows_str} x {cols_str}[/cyan]\")\n",
    "\n",
    "\n",
    "def show_pipeline_table() -> None:\n",
    "    \"\"\"Display a Rich table summarizing all pipeline steps, safely formatting N/A values.\"\"\"\n",
    "    if not pipeline_log:\n",
    "        console.print(\"[red]No pipeline steps logged yet.[/red]\")\n",
    "        return\n",
    "\n",
    "    table = Table(title=\"ðŸ“Š Data Pipeline Summary\", show_lines=True)\n",
    "    table.add_column(\"Step\", style=\"cyan\", no_wrap=True)\n",
    "    table.add_column(\"Rows\", style=\"green\")\n",
    "    table.add_column(\"Cols\", style=\"yellow\")\n",
    "\n",
    "    for entry in pipeline_log:\n",
    "        # Determine values for printing\n",
    "        rows_val = entry['rows']\n",
    "        cols_val = entry['cols']\n",
    "        \n",
    "        # Safely format: use comma only for integers, use string otherwise\n",
    "        rows_str = f\"{rows_val:,}\" if isinstance(rows_val, int) else str(rows_val)\n",
    "        cols_str = str(cols_val) if isinstance(cols_val, int) else str(cols_val)\n",
    "        \n",
    "        table.add_row(entry[\"step\"], rows_str, cols_str)\n",
    "    \n",
    "    console.print(table)\n",
    "\n",
    "\n",
    "console.print(Panel(\"[bold green]Pipeline logger configured.[/bold green]\", border_style=\"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e07a49",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Configure Data Paths\n",
    "\n",
    "DATA_DIR = Path(\"../data\")\n",
    "RAW_DATA_FOLDER = DATA_DIR / \"raw\" / \"apd\"\n",
    "INTERIM_DATA_FOLDER = DATA_DIR / \"interim\" / \"apd\"\n",
    "PROCESSED_DATA_FOLDER = DATA_DIR / \"processed\" / \"apd\"\n",
    "EXTERNAL_DATA_FOLDER = DATA_DIR / \"external\"\n",
    "SHAPEFILES_DIR = DATA_DIR / \"raw\" / \"shapefiles\"\n",
    "\n",
    "# Create necessary folders\n",
    "EXTERNAL_DATA_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "INTERIM_DATA_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DATA_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Weather CSVs (initial run; later we fetch full 2021â€“2025)\n",
    "HOURLY_WEATHER_PATH = EXTERNAL_DATA_FOLDER / \"atlanta_hourly_weather_2024.csv\"\n",
    "DAILY_WEATHER_PATH = EXTERNAL_DATA_FOLDER / \"atlanta_daily_weather_2024.csv\"\n",
    "\n",
    "# Shapefiles\n",
    "CITIES_SHP = SHAPEFILES_DIR / \"census_boundary_2024_sf\" / \"ga_census_places_2024.shp\"\n",
    "CAMPUS_SHP = SHAPEFILES_DIR / \"area_landmark_2024_sf\" / \"ga_census_landmarks_2023.shp\"\n",
    "NEIGHBORHOOD_SHP = SHAPEFILES_DIR / \"atl_neighborhood_sf\" / \"atl_neighborhoods.shp\"\n",
    "NPU_SHP = SHAPEFILES_DIR / \"atl_npu_sf\" / \"atl_npu_boundaries.shp\"\n",
    "APD_ZONE_SHP = SHAPEFILES_DIR / \"apd_zone_2019_sf\" / \"apd_police_zones_2019.shp\"\n",
    "\n",
    "# School center coordinates for distance-based enrichment\n",
    "SCHOOL_CENTERS = {\n",
    "    \"GSU\": (33.7530, -84.3863),\n",
    "    \"GA_Tech\": (33.7756, -84.3963),\n",
    "    \"Emory\": (33.7925, -84.3239),\n",
    "    \"Clark\": (33.7533, -84.4124),\n",
    "    \"Spelman\": (33.7460, -84.4129),\n",
    "    \"Morehouse\": (33.7483, -84.4126),\n",
    "    \"Morehouse_Med\": (33.7505, -84.4131),\n",
    "    \"Atlanta_Metro\": (33.7145, -84.4020),\n",
    "    \"Atlanta_Tech\": (33.7126, -84.4034),\n",
    "    \"SCAD\": (33.7997, -84.3920),\n",
    "    \"John_Marshall\": (33.7621, -84.3896),\n",
    "}\n",
    "\n",
    "console.print(\n",
    "    Panel.fit(\n",
    "        \"[bold cyan]Data paths configured.[/bold cyan]\\n\\n\"\n",
    "        f\"Raw Data: [yellow]{RAW_DATA_FOLDER}[/yellow]\\n\"\n",
    "        f\"Interim Data: [yellow]{INTERIM_DATA_FOLDER}[/yellow]\\n\"\n",
    "        f\"External Data: [yellow]{EXTERNAL_DATA_FOLDER}[/yellow]\\n\"\n",
    "        f\"Processed Data: [yellow]{PROCESSED_DATA_FOLDER}[/yellow]\\n\"\n",
    "        f\"Shapefiles: [yellow]{SHAPEFILES_DIR}[/yellow]\",\n",
    "        title=\"[bold cyan]Data Paths[/bold cyan]\",\n",
    "        border_style=\"cyan\",\n",
    "    )\n",
    ")\n",
    "\n",
    "log_step(\"Step 1: Logger, paths, settings, and constants configured\", pd.DataFrame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea5fbd5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Initial weather fetch (pre-fetch for initial merge)\n",
    "def fetch_atlanta_weather(\n",
    "    start_date=\"2021-01-01\", end_date=\"2024-12-31\", lat=33.749, lon=-84.388\n",
    "):\n",
    "    \"\"\"Fetch weather data from Open-Meteo and save hourly/daily CSVs for reference.\"\"\"\n",
    "    console.print(\n",
    "        Panel(\n",
    "            \"[bold cyan]Fetching initial weather data[/bold cyan]\\n\"\n",
    "            f\"Location: ({lat:.3f} N, {lon:.3f} W)\\n\"\n",
    "            f\"Date Range: {start_date} to {end_date}\",\n",
    "            border_style=\"cyan\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cache_session = requests_cache.CachedSession(\".cache\", expire_after=-1)\n",
    "    retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "    openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"hourly\": [\n",
    "            \"temperature_2m\",\n",
    "            \"precipitation\",\n",
    "            \"rain\",\n",
    "            \"apparent_temperature\",\n",
    "            \"weather_code\",\n",
    "            \"is_day\",\n",
    "        ],\n",
    "        \"daily\": [\n",
    "            \"sunrise\",\n",
    "            \"daylight_duration\",\n",
    "            \"sunshine_duration\",\n",
    "            \"precipitation_hours\",\n",
    "            \"rain_sum\",\n",
    "            \"temperature_2m_mean\",\n",
    "            \"weather_code\",\n",
    "        ],\n",
    "        \"timezone\": \"America/New_York\",\n",
    "        \"temperature_unit\": \"fahrenheit\",\n",
    "    }\n",
    "\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "    response = responses[0]\n",
    "\n",
    "    # Hourly\n",
    "    hourly = response.Hourly()\n",
    "    hourly_df = pd.DataFrame(\n",
    "        {\n",
    "            \"datetime\": pd.date_range(\n",
    "                start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "                inclusive=\"left\",\n",
    "            ),\n",
    "            \"temp_f\": hourly.Variables(0).ValuesAsNumpy(),\n",
    "            \"precip_in\": hourly.Variables(1).ValuesAsNumpy(),\n",
    "            \"rain_in\": hourly.Variables(2).ValuesAsNumpy(),\n",
    "            \"apparent_temp_f\": hourly.Variables(3).ValuesAsNumpy(),\n",
    "            \"weather_code\": hourly.Variables(4).ValuesAsNumpy(),\n",
    "            \"is_daylight\": hourly.Variables(5).ValuesAsNumpy().astype(int),\n",
    "        }\n",
    "    )\n",
    "    hourly_df[\"datetime\"] = (\n",
    "        hourly_df[\"datetime\"].dt.tz_convert(\"America/New_York\").dt.tz_localize(None)\n",
    "    )\n",
    "\n",
    "    # Daily\n",
    "    daily = response.Daily()\n",
    "    daily_df = pd.DataFrame(\n",
    "        {\n",
    "            \"date\": pd.date_range(\n",
    "                start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=daily.Interval()),\n",
    "                inclusive=\"left\",\n",
    "            ),\n",
    "            \"sunrise\": daily.Variables(0).ValuesInt64AsNumpy(),\n",
    "            \"daylight_duration_sec\": daily.Variables(1).ValuesAsNumpy(),\n",
    "            \"sunshine_duration_sec\": daily.Variables(2).ValuesAsNumpy(),\n",
    "            \"precip_hours\": daily.Variables(3).ValuesAsNumpy(),\n",
    "            \"rain_sum_in\": daily.Variables(4).ValuesAsNumpy(),\n",
    "            \"temp_mean_f\": daily.Variables(5).ValuesAsNumpy(),\n",
    "            \"weather_code\": daily.Variables(6).ValuesAsNumpy(),\n",
    "        }\n",
    "    )\n",
    "    daily_df[\"date\"] = (\n",
    "        daily_df[\"date\"]\n",
    "        .dt.tz_convert(\"America/New_York\")\n",
    "        .dt.tz_localize(None)\n",
    "        .dt.date\n",
    "    )\n",
    "\n",
    "    hourly_df.to_csv(HOURLY_WEATHER_PATH, index=False)\n",
    "    daily_df.to_csv(DAILY_WEATHER_PATH, index=False)\n",
    "\n",
    "    console.print(f\"[green]âœ“ Saved hourly:[/green] {HOURLY_WEATHER_PATH}\")\n",
    "    console.print(f\"[green]âœ“ Saved daily:[/green] {DAILY_WEATHER_PATH}\")\n",
    "    console.print(\n",
    "        f\"[green]âœ“ Total rows: {len(hourly_df):,} hourly, {len(daily_df):,} daily[/green]\"\n",
    "    )\n",
    "\n",
    "    return hourly_df, daily_df\n",
    "\n",
    "\n",
    "fetch_atlanta_weather()\n",
    "log_step(\"Step 2: Initial Atlanta weather (2021â€“2024) saved to data/external\", pd.DataFrame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6261a3",
   "metadata": {},
   "source": [
    "#### Section 2: Standardize & Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c7a888",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def standardize_column_name(col: str) -> str:\n",
    "    \"\"\"Convert column name to snake_case format.\"\"\"\n",
    "    col = re.sub(r\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", col)\n",
    "    col = re.sub(r\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", col)\n",
    "    col = col.lower()\n",
    "    col = re.sub(r\"[\\s\\-\\.\\,\\(\\)\\[\\]\\{\\}]+\", \"_\", col)\n",
    "    col = re.sub(r\"[^\\w]\", \"\", col)\n",
    "    col = re.sub(r\"_+\", \"_\", col).strip(\"_\")\n",
    "    return col\n",
    "\n",
    "\n",
    "def combine_and_deduplicate(files: List[Path], dedupe_key: str) -> pd.DataFrame:\n",
    "    \"\"\"Combine multiple CSVs, standardize columns, and drop duplicates.\"\"\"\n",
    "    console.print(\"\\n[bold cyan]â•â•â• Combining data files â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "    dfs = []\n",
    "    for filepath in files:\n",
    "        console.print(f\"[cyan]Reading and standardizing:[/cyan] {filepath.name}\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        df.columns = [standardize_column_name(c) for c in df.columns]\n",
    "        dfs.append(df)\n",
    "\n",
    "    df_combined = pd.concat(dfs, ignore_index=True)\n",
    "    total_rows = len(df_combined)\n",
    "\n",
    "    # New logger step: Ingest Raw Data\n",
    "    log_step(\"Ingest Raw Data (standardized columns, pre-dedup)\", df_combined)\n",
    "\n",
    "    if dedupe_key not in df_combined.columns:\n",
    "        raise KeyError(\n",
    "            f\"dedupe_key='{dedupe_key}' not found in columns: {df_combined.columns.tolist()}\"\n",
    "        )\n",
    "\n",
    "    df_dedup = df_combined.drop_duplicates(subset=[dedupe_key])\n",
    "    duplicates = total_rows - len(df_dedup)\n",
    "\n",
    "    console.print(f\"[yellow]Combined:[/yellow] {total_rows:,} rows\")\n",
    "    console.print(f\"[red]Removed:[/red] {duplicates:,} duplicate rows\")\n",
    "\n",
    "    log_step(\"Step 3: Standardize & Combine, deduplicated by incident_number\", df_dedup)\n",
    "    return df_dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d344a93a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function Execution: Combine Raw Data CSVs\n",
    "\n",
    "input_files = list(RAW_DATA_FOLDER.glob(\"*.csv\"))\n",
    "\n",
    "if not input_files:\n",
    "    console.print(f\"[bold red]Warning:[/bold red] No CSV files found in '{RAW_DATA_FOLDER}'\")\n",
    "    df_combined = pd.DataFrame()\n",
    "else:\n",
    "    console.print(f\"[bold cyan]Found {len(input_files)} CSV file(s).[/bold cyan]\")\n",
    "    df_combined = combine_and_deduplicate(\n",
    "        files=input_files, dedupe_key=\"incident_number\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b78ff",
   "metadata": {},
   "source": [
    "#### Section 3: Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fdc1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_apd_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Column drops, normalization, and one-hot encoding.\"\"\"\n",
    "    console.print(\"\\n[bold cyan]â•â•â• Cleaning APD data â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Define and drop unnecessary columns\n",
    "    columns_to_drop = [\n",
    "        \"report_number\",\n",
    "        \"zone\",\n",
    "        \"fire_arm_involved\",\n",
    "        \"object_id\",\n",
    "        \"occurred_from_date\",\n",
    "        \"occurred_to_date\",\n",
    "        \"part\",\n",
    "        \"vic_count\",\n",
    "        \"is_bias_motivation_involved\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        \"beat_text\",\n",
    "    ]\n",
    "\n",
    "    existing_drops = [col for col in columns_to_drop if col in df.columns]\n",
    "    if existing_drops:\n",
    "        df = df.drop(columns=existing_drops)\n",
    "        console.print(f\"[yellow]Dropped {len(existing_drops)} columns:[/yellow] {', '.join(existing_drops)}\")\n",
    "\n",
    "    # 2. Rename columns\n",
    "    if \"objectid\" in df.columns:\n",
    "        df = df.rename(columns={\"objectid\": \"object_id\"})\n",
    "    \n",
    "    # 3. One-hot encode 'event_watch'\n",
    "    if \"event_watch\" in df.columns:\n",
    "        console.print(\"[cyan]One-hot encoding:[/cyan] 'event_watch'\")\n",
    "        one_hot = pd.get_dummies(df[\"event_watch\"], prefix=\"event_watch\", dummy_na=False)\n",
    "        one_hot.columns = [standardize_column_name(col) for col in one_hot.columns]\n",
    "        df = pd.concat([df.drop(columns=[\"event_watch\"]), one_hot], axis=1)\n",
    "        console.print(f\"[green]âœ“ Created {len(one_hot.columns)} standardized one-hot columns[/green]\")\n",
    "\n",
    "    # 4. Convert text to lowercase\n",
    "    text_columns = [\"location_type\", \"street_address\", \"nibrs_offense\"]\n",
    "    for col in text_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.lower()\n",
    "\n",
    "    log_step(\"Step 4: Clean and drop selected columns\", df)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_clean = clean_apd_data(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9af498",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Date Standardization\n",
    "\n",
    "console.print(\"\\n[bold cyan]â•â•â• Standardizing 'report_date' column with robust parser â•â•â•[/bold cyan]\\n\")\n",
    "total_rows = len(df_clean)\n",
    "\n",
    "# Preserve raw values for comparison\n",
    "df_clean[\"_raw_report_date\"] = df_clean[\"report_date\"].astype(str).str.strip()\n",
    "\n",
    "\n",
    "def parse_report_date(x):\n",
    "    \"\"\"Safely parse mixed APD/NIBRS date formats.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return pd.NaT\n",
    "\n",
    "    x = str(x).strip()\n",
    "\n",
    "    # Explicit patterns: MM/DD/YYYY HH:MM:SS AM/PM, etc.\n",
    "    if re.match(r\"^\\d{1,2}/\\d{1,2}/\\d{4} \\d{1,2}:\\d{2}:\\d{2} [APMapm]{2}$\", x):\n",
    "        return pd.to_datetime(x, format=\"%m/%d/%Y %I:%M:%S %p\", errors=\"coerce\")\n",
    "    if re.match(r\"^\\d{1,2}/\\d{1,2}/\\d{4} \\d{1,2}:\\d{2} [APMapm]{2}$\", x):\n",
    "        return pd.to_datetime(x, format=\"%m/%d/%Y %I:%M %p\", errors=\"coerce\")\n",
    "    if re.match(r\"^\\d{1,2}/\\d{1,2}/\\d{4} \\d{1,2}:\\d{2}$\", x):\n",
    "        return pd.to_datetime(x, format=\"%m/%d/%Y %H:%M\", errors=\"coerce\")\n",
    "\n",
    "    # Fallback\n",
    "    try:\n",
    "        return parser.parse(x, fuzzy=True)\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "\n",
    "df_clean[\"report_date\"] = df_clean[\"_raw_report_date\"].apply(parse_report_date)\n",
    "invalid = df_clean[\"report_date\"].isna().sum()\n",
    "parsed = total_rows - invalid\n",
    "\n",
    "console.print(f\"[cyan]Total rows:[/cyan] {total_rows:,}\")\n",
    "console.print(f\"[green]Successfully standardized:[/green] {parsed:,}\")\n",
    "console.print(f\"[yellow]Unrecoverable dates dropped:[/yellow] {invalid:,}\")\n",
    "\n",
    "df_clean = df_clean.dropna(subset=[\"report_date\"]).copy()\n",
    "df_clean[\"report_date\"] = pd.to_datetime(df_clean[\"report_date\"])\n",
    "\n",
    "log_step(\"Step 5: Robust date standardization\", df_clean)\n",
    "\n",
    "console.print(\"\\n[bold blue]Examples of corrected formats:[/bold blue]\")\n",
    "changed = df_clean[\n",
    "    df_clean[\"_raw_report_date\"] != df_clean[\"report_date\"].astype(str)\n",
    "]\n",
    "if len(changed) > 0:\n",
    "    console.print(\n",
    "        changed.sample(min(5, len(changed)))[[\"_raw_report_date\", \"report_date\"]]\n",
    "    )\n",
    "else:\n",
    "    console.print(\"[yellow]All dates already standardized.[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d3d86f",
   "metadata": {},
   "source": [
    "#### Section 4: Geospatial Enrichment (NPU, Zone, Campus Footprints, City, Neighborhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89871434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geospatial Helper Functions (NPU STANDARDIZED TO 'npu')\n",
    "\n",
    "def haversine_distance(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"Great-circle distance between two points in miles (R=3956).\"\"\"\n",
    "    R = 3956\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "\n",
    "    return c * R\n",
    "\n",
    "\n",
    "def to_gdf(df: pd.DataFrame, lon_col: str = \"longitude\", lat_col: str = \"latitude\"):\n",
    "    \"\"\"Convert a DataFrame with lon/lat into a GeoDataFrame in EPSG:4326.\"\"\"\n",
    "    for c in (lon_col, lat_col):\n",
    "        if c not in df.columns:\n",
    "            raise KeyError(f\"Expected coordinate column '{c}' not found.\")\n",
    "    return gpd.GeoDataFrame(\n",
    "        df,\n",
    "        geometry=gpd.points_from_xy(df[lon_col], df[lat_col]),\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "\n",
    "\n",
    "def load_shapefile(path: Path, target_crs) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Load shapefile and ensure CRS alignment.\"\"\"\n",
    "    try:\n",
    "        gdf = gpd.read_file(path)\n",
    "    except Exception:\n",
    "        console.print(f\"[bold red]FATAL: Could not load shapefile:[/bold red] {path}\")\n",
    "        raise\n",
    "\n",
    "    if gdf.crs is None:\n",
    "        gdf = gdf.set_crs(\"EPSG:4326\")\n",
    "    return gdf.to_crs(target_crs)\n",
    "\n",
    "\n",
    "def attach_campus(gdf: gpd.GeoDataFrame, campus_shp: Path) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Attach closest campus footprint label where available (MTFCC S1400).\"\"\"\n",
    "    console.print(\"[cyan]Attaching campus footprint labels...[/cyan]\")\n",
    "    campus = load_shapefile(campus_shp, gdf.crs)\n",
    "    if \"MTFCC\" in campus.columns:\n",
    "        campus = campus[campus[\"MTFCC\"] == \"S1400\"]\n",
    "    name_col = \"FULLNAME\" if \"FULLNAME\" in campus.columns else campus.columns[0]\n",
    "    campus = campus[[name_col, \"geometry\"]].rename(columns={name_col: \"campus_label\"})\n",
    "\n",
    "    gdf = gpd.sjoin_nearest(gdf, campus, how=\"left\")\n",
    "    gdf = gdf.drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def attach_neighborhood(\n",
    "    gdf: gpd.GeoDataFrame, nhood_shp: Path\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"Attach neighborhood labels (within polygon).\"\"\"\n",
    "    console.print(\"[cyan]Attaching neighborhood labels...[/cyan]\")\n",
    "    nhoods = load_shapefile(nhood_shp, gdf.crs)\n",
    "    name_col = \"NAME\" if \"NAME\" in nhoods.columns else nhoods.columns[0]\n",
    "    nhoods = nhoods[[name_col, \"geometry\"]].rename(\n",
    "        columns={name_col: \"neighborhood_label\"}\n",
    "    )\n",
    "\n",
    "    gdf = gpd.sjoin(gdf, nhoods, how=\"left\", predicate=\"within\")\n",
    "    gdf = gdf.drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def attach_city(gdf: gpd.GeoDataFrame, cities_shp: Path) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Attach city labels.\"\"\"\n",
    "    console.print(\"[cyan]Attaching city labels...[/cyan]\")\n",
    "    cities = load_shapefile(cities_shp, gdf.crs)\n",
    "    name_col = \"NAME\" if \"NAME\" in cities.columns else cities.columns[0]\n",
    "    cities = cities[[name_col, \"geometry\"]].rename(columns={name_col: \"city_label\"})\n",
    "\n",
    "    gdf = gpd.sjoin(gdf, cities, how=\"left\", predicate=\"within\")\n",
    "    gdf = gdf.drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# FIX: Standardize NPU column output name to 'npu'\n",
    "def attach_npu(gdf: gpd.GeoDataFrame, npu_shp: Path) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Attach NPU labels using nearest polygon, naming the output column 'npu'.\"\"\"\n",
    "    console.print(\"[cyan]Attaching NPU labels (output column: 'npu')...[/cyan]\")\n",
    "    npu_gdf = load_shapefile(npu_shp, gdf.crs)\n",
    "\n",
    "    candidate_cols = [\"NPU\", \"npu\", \"NPU_ID\", \"NPU_NUM\", \"NPU_NAME\", \"NAME\"]\n",
    "    name_col = None\n",
    "    for c in candidate_cols:\n",
    "        if c in npu_gdf.columns:\n",
    "            name_col = c\n",
    "            break\n",
    "    if name_col is None:\n",
    "        name_col = npu_gdf.columns[0]\n",
    "\n",
    "    # Rename the source NPU column to the desired final column name ('npu')\n",
    "    npu_gdf = npu_gdf[[name_col, \"geometry\"]].rename(columns={name_col: \"npu\"})\n",
    "\n",
    "    gdf = gpd.sjoin_nearest(gdf, npu_gdf, how=\"left\")\n",
    "    gdf = gdf.drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def attach_apd_zone(gdf: gpd.GeoDataFrame, zone_shp: Path) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Attach APD Zone numbers by predicate 'within'.\"\"\"\n",
    "    console.print(\"[cyan]Attaching APD zones...[/cyan]\")\n",
    "    zones = load_shapefile(zone_shp, gdf.crs)\n",
    "\n",
    "    zone_col = None\n",
    "    for c in [\"ZONE\", \"zone\", \"Zone\"]:\n",
    "        if c in zones.columns:\n",
    "            zone_col = c\n",
    "            break\n",
    "    if zone_col is None:\n",
    "        zone_col = zones.columns[0]\n",
    "\n",
    "    zones = zones[[zone_col, \"geometry\"]].rename(columns={zone_col: \"zone_raw\"})\n",
    "\n",
    "    gdf = gpd.sjoin(gdf, zones, how=\"left\", predicate=\"within\")\n",
    "    gdf = gdf.drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "\n",
    "    gdf[\"zone_int\"] = pd.to_numeric(\n",
    "        gdf[\"zone_raw\"].astype(str).str.extract(r\"(\\d+)\", expand=False),\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# FIX: Updated to use 'npu' as the grouping column\n",
    "def impute_missing_zone_from_npu(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Impute missing zone_int using the modal zone_int within its NPU.\"\"\"\n",
    "    if \"zone_int\" not in df.columns or \"npu\" not in df.columns:\n",
    "        return df\n",
    "\n",
    "    missing_before = df[\"zone_int\"].isna().sum()\n",
    "    if missing_before == 0:\n",
    "        return df\n",
    "\n",
    "    mapping = (\n",
    "        df.dropna(subset=[\"zone_int\"])\n",
    "        .groupby(\"npu\")[\"zone_int\"]\n",
    "        .apply(lambda s: s.mode().iloc[0] if not s.mode().empty else np.nan)\n",
    "    )\n",
    "\n",
    "    df[\"zone_int_cleaned\"] = df[\"zone_int\"].fillna(df[\"npu\"].map(mapping))\n",
    "    missing_after = df[\"zone_int_cleaned\"].isna().sum()\n",
    "\n",
    "    df = df.drop(columns=[\"zone_int\"]).rename(columns={\"zone_int_cleaned\": \"zone_int\"})\n",
    "    console.print(\n",
    "        f\"[green]âœ“ Imputed {missing_before - missing_after} zone_int values via NPU mode.[/green]\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_location_label(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create location_label with priority campus > neighborhood > city.\"\"\"\n",
    "    conditions = [\n",
    "        df.get(\"campus_label\").notna() if \"campus_label\" in df.columns else False,\n",
    "        df.get(\"neighborhood_label\").notna()\n",
    "        if \"neighborhood_label\" in df.columns\n",
    "        else False,\n",
    "        df.get(\"city_label\").notna() if \"city_label\" in df.columns else False,\n",
    "    ]\n",
    "    choices = [\n",
    "        df.get(\"campus_label\"),\n",
    "        df.get(\"neighborhood_label\"),\n",
    "        df.get(\"city_label\"),\n",
    "    ]\n",
    "    df[\"location_label\"] = np.select(conditions, choices, default=\"Other\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def enrich_spatial(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Spatial enrichment pipeline.\"\"\"\n",
    "    console.print(\"\\n[bold cyan]â•â•â• Spatial enrichment â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "    initial_count = len(df)\n",
    "    df_temp = df.dropna(subset=[\"longitude\", \"latitude\"]).copy()\n",
    "    dropped_count = initial_count - len(df_temp)\n",
    "    if dropped_count > 0:\n",
    "        console.print(f\"[yellow]Dropped {dropped_count} rows missing coordinates.[/yellow]\")\n",
    "\n",
    "    gdf = to_gdf(df_temp, lon_col=\"longitude\", lat_col=\"latitude\")\n",
    "\n",
    "    TARGET_PCS = \"EPSG:3857\"\n",
    "    gdf = gdf.to_crs(TARGET_PCS)\n",
    "    console.print(f\"[green]âœ“ Data projected to {TARGET_PCS} for joins.[/green]\")\n",
    "\n",
    "    # NPU is attached as 'npu'\n",
    "    gdf = attach_npu(gdf, NPU_SHP)\n",
    "    gdf = attach_apd_zone(gdf, APD_ZONE_SHP)\n",
    "    gdf = attach_campus(gdf, CAMPUS_SHP)\n",
    "    gdf = attach_neighborhood(gdf, NEIGHBORHOOD_SHP)\n",
    "    gdf = attach_city(gdf, CITIES_SHP)\n",
    "\n",
    "    df_enriched = pd.DataFrame(gdf.drop(columns=[\"geometry\", \"zone_raw\"], errors=\"ignore\"))\n",
    "    df_enriched = impute_missing_zone_from_npu(df_enriched)\n",
    "    df_enriched = build_location_label(df_enriched)\n",
    "\n",
    "    log_step(\"Step 6: Spatial enrichment (NPU, zone, campus footprints, city, neighborhood)\", df_enriched)\n",
    "    return df_enriched\n",
    "\n",
    "\n",
    "df_spatial = enrich_spatial(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d60963",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Fix raw date column name if present\n",
    "if \"_raw_report_date\" in df_spatial.columns:\n",
    "    df_spatial = df_spatial.rename(columns={\"_raw_report_date\": \"raw_report_date\"})\n",
    "    log_step(\"Step 7: Rename raw date column\", df_spatial)\n",
    "    console.print(\"[green]âœ“ Renamed '_raw_report_date' to 'raw_report_date'.[/green]\")\n",
    "else:\n",
    "    console.print(\"[yellow]No raw date column to rename.[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c905d777",
   "metadata": {},
   "source": [
    "#### Section 5: Weather + Date/Context Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fcde9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_holiday_flag(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str = \"report_date\",\n",
    "    country: str = \"US\",\n",
    "    subdiv: str = \"GA\",\n",
    "    years: Optional[List[int]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Add 'is_holiday' boolean column.\"\"\"\n",
    "    df = df.copy()\n",
    "    if years is None:\n",
    "        years = sorted(df[date_col].dt.year.unique().tolist())\n",
    "\n",
    "    console.print(f\"[cyan]Generating holidays for {country}-{subdiv}, years={years}[/cyan]\")\n",
    "    holiday_dates = holidays.country_holidays(country=country, subdiv=subdiv, years=years)\n",
    "    holiday_set = set(holiday_dates.keys())\n",
    "\n",
    "    df[\"_date_only\"] = df[date_col].dt.date\n",
    "    df[\"is_holiday\"] = df[\"_date_only\"].isin(holiday_set)\n",
    "    df = df.drop(columns=[\"_date_only\"])\n",
    "    console.print(\"[green]âœ“ 'is_holiday' column created.[/green]\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be12411",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Feature Engineering: Date & Contextual Features\n",
    "def engineer_date_context_features(\n",
    "    df: pd.DataFrame, date_col: str = \"report_date\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Create core temporal + contextual features. Standardized to 6 x 4-hour bins.\"\"\"\n",
    "    df = df.copy()\n",
    "    console.print(\"\\n[bold cyan]â•â•â• Engineering date and contextual features (Standardizing to 6 Bins) â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    dt = df[date_col].dt\n",
    "\n",
    "    # Core temporal features\n",
    "    df[\"incident_datetime\"] = df[date_col]\n",
    "    df[\"incident_date\"] = dt.date\n",
    "    df[\"incident_hour\"] = dt.hour\n",
    "    df[\"incident_datetime_hour\"] = dt.to_period(\"h\").astype(str)\n",
    "    df[\"day_of_week\"] = dt.day_name()\n",
    "    df[\"day_number\"] = dt.weekday + 1  # Monday=1\n",
    "    df[\"year\"] = dt.year\n",
    "    df[\"day_of_year\"] = dt.dayofyear\n",
    "    df[\"month\"] = dt.month\n",
    "    df[\"week_number\"] = dt.isocalendar().week.astype(int)\n",
    "\n",
    "    df = add_holiday_flag(df, date_col=date_col)\n",
    "\n",
    "    if \"nibrs_ucr_code\" in df.columns:\n",
    "        df[\"nibrs_code\"] = df[\"nibrs_ucr_code\"]\n",
    "\n",
    "    df[\"offense_category\"] = np.select(\n",
    "        [\n",
    "            df[\"nibrs_offense\"].str.contains(\"burglary|robbery\", case=False, na=False),\n",
    "            df[\"nibrs_offense\"].str.contains(\n",
    "                \"motor vehicle theft\", case=False, na=False\n",
    "            ),\n",
    "            df[\"nibrs_offense\"].str.contains(\n",
    "                \"theft|larceny|shoplift|fraud|swindle|embezzelment|stolen property|false pretenses\",\n",
    "                case=False,\n",
    "                na=False,\n",
    "            ),\n",
    "            df[\"nibrs_offense\"].str.contains(\n",
    "                \"assault|murder|rape|battery|intimidation|extortion|kidnapping\",\n",
    "                case=False,\n",
    "                na=False,\n",
    "            ),\n",
    "        ],\n",
    "        [\n",
    "            \"Burglary/Robbery\",\n",
    "            \"Motor Vehicle Theft\",\n",
    "            \"Theft/Larceny/Fraud\",\n",
    "            \"Violent Crime\",\n",
    "        ],\n",
    "        default=\"Other/Misc.\",\n",
    "    )\n",
    "\n",
    "    month = dt.month\n",
    "    df[\"semester\"] = np.select(\n",
    "        [month.isin([8, 9, 10, 11, 12]), month.isin([1, 2, 3, 4, 5])],\n",
    "        [\"Fall\", \"Spring\"],\n",
    "        default=\"Summer\",\n",
    "    )\n",
    "\n",
    "    # STANDARDIZED TO SIX 4-HOUR BINS (0-4, 5-8, ..., 21-24)\n",
    "    df['hour'] = df['incident_datetime'].dt.hour\n",
    "    bins = [0, 5, 9, 13, 17, 21, 25]\n",
    "    labels = [\n",
    "        \"Early Night (0â€“4)\",\n",
    "        \"Early Morning (5â€“8)\",\n",
    "        \"Late Morning (9â€“12)\",\n",
    "        \"Afternoon (13â€“16)\",\n",
    "        \"Evening (17â€“20)\",\n",
    "        \"Late Night (21â€“24)\",\n",
    "    ]\n",
    "    # This is the single, consistent binning column\n",
    "    df[\"hour_block\"] = pd.cut(df[\"hour\"], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "    \n",
    "    df[\"is_weekend\"] = dt.weekday >= 5\n",
    "\n",
    "    df[\"loc_acc\"] = np.where(\n",
    "        df[\"latitude\"].isna() | df[\"longitude\"].isna(), 1, 0\n",
    "    )\n",
    "\n",
    "    log_step(\"Step 8: Date and context features (Standardized to 6-bin hour_block)\", df)\n",
    "    return df\n",
    "\n",
    "df_features = engineer_date_context_features(df_spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571a64c3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def merge_weather_data_basic(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Only merge if weather columns don't exist yet.\"\"\"\n",
    "    if 'temp_f' in df.columns and df['temp_f'].notna().any():\n",
    "        console.print(\"[yellow]Weather data already present, skipping initial merge.[/yellow]\")\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    console.print(\"\\n[bold cyan]â•â•â• Merging initial weather data (external CSVs) â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "    try:\n",
    "        hourly_df = pd.read_csv(HOURLY_WEATHER_PATH)\n",
    "        daily_df = pd.read_csv(DAILY_WEATHER_PATH)\n",
    "    except FileNotFoundError:\n",
    "        console.print(\"[bold red]Warning:[/bold red] Initial weather CSVs not found. Skipping this merge.\")\n",
    "        return df\n",
    "\n",
    "    df[\"report_date_dt\"] = pd.to_datetime(df[\"report_date\"])\n",
    "    hourly_df[\"datetime\"] = pd.to_datetime(hourly_df[\"datetime\"])\n",
    "    daily_df[\"date\"] = pd.to_datetime(daily_df[\"date\"])\n",
    "\n",
    "    # Drop potential existing weather columns before merge\n",
    "    weather_cols_to_drop = [col for col in df.columns if any(c in col for c in hourly_df.columns if c != 'datetime') or any(c in col for c in daily_df.columns if c != 'date')]\n",
    "    df = df.drop(columns=weather_cols_to_drop, errors='ignore')\n",
    "\n",
    "    df = df.merge(\n",
    "        hourly_df,\n",
    "        left_on=df[\"report_date_dt\"].dt.floor(\"H\"),\n",
    "        right_on=hourly_df[\"datetime\"].dt.floor(\"H\"),\n",
    "        how=\"left\",\n",
    "    )\n",
    "    df = df.drop(columns=[\"key_0\"], errors=\"ignore\")\n",
    "\n",
    "    df[\"date_only\"] = df[\"report_date_dt\"].dt.date\n",
    "    daily_df[\"date_only\"] = daily_df[\"date\"].dt.date\n",
    "\n",
    "    df = df.merge(\n",
    "        daily_df,\n",
    "        on=\"date_only\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_hourly\", \"_daily\"),\n",
    "    )\n",
    "\n",
    "    df = df.drop(columns=[\"report_date_dt\", \"date_only\", \"datetime\", \"date\"], errors=\"ignore\")\n",
    "    console.print(\"[green]âœ“ Initial hourly + daily weather merged (2021-2024).[/green]\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df_features = merge_weather_data_basic(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879c3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_weather_flags(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create boolean flags is_raining, is_hot, is_cold based on merged columns.\"\"\"\n",
    "    df = df.copy()\n",
    "    console.print(\"\\n[bold cyan]â•â•â• Deriving basic weather flags â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "    temp_col = \"apparent_temp_f\"\n",
    "    precip_col = \"precip_in\"\n",
    "\n",
    "    if temp_col not in df.columns or precip_col not in df.columns:\n",
    "        console.print(\n",
    "            \"[yellow]Weather columns missing or incomplete; skipping flag derivation.[/yellow]\"\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    df[\"is_raining\"] = (df[precip_col].fillna(0) > 0.01).astype(int)\n",
    "    df[\"is_hot\"] = (df[temp_col].fillna(-999) >= 90).astype(int)\n",
    "    df[\"is_cold\"] = (df[temp_col].fillna(999) <= 40).astype(int)\n",
    "\n",
    "    log_step(\"Step 9: Basic weather flags\", df)\n",
    "    console.print(\"[green]âœ“ Weather flags (hot/cold/raining) created.[/green]\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df_features = derive_weather_flags(df_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7b989",
   "metadata": {},
   "source": [
    "#### Section 6: Filter Target Offenses & Interim Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc39e1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\"\\n[bold cyan]â•â•â• Final filtering for target offenses and interim export â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "TARGET_OFFENSES = [\n",
    "    \"larceny\",\n",
    "    \"theft\",\n",
    "    \"robbery\",\n",
    "    \"burglary\",\n",
    "    \"prowling\",\n",
    "    \"shoplifting\",\n",
    "    \"fraud\",\n",
    "    \"swindle\",\n",
    "    \"embezzelment\",\n",
    "    \"credit card\",\n",
    "    \"wire fraud\",\n",
    "    \"impersonation\",\n",
    "]\n",
    "\n",
    "if \"nibrs_offense\" not in df_features.columns:\n",
    "    raise KeyError(\"'nibrs_offense' column not found; cannot filter TARGET_OFFENSES.\")\n",
    "\n",
    "mask = df_features[\"nibrs_offense\"].str.contains(\n",
    "    \"|\".join(TARGET_OFFENSES), case=False, na=False\n",
    ")\n",
    "df_model = df_features[mask].copy()\n",
    "\n",
    "if \"raw_report_date\" in df_model.columns:\n",
    "    df_model = df_model.drop(columns=[\"raw_report_date\"])\n",
    "    console.print(\"[yellow]Dropped debug column 'raw_report_date' prior to export.[/yellow]\")\n",
    "\n",
    "log_step(\"Step 10: Filter for target crimes\", df_model)\n",
    "console.print(\n",
    "    f\"[bold yellow]Filtered for modeling:[/bold yellow] {len(df_model):,} rows match target offenses.\"\n",
    ")\n",
    "\n",
    "df_final = df_model.sort_values(\"report_date\", ascending=True, ignore_index=True)\n",
    "\n",
    "final_output = INTERIM_DATA_FOLDER / \"apd_model_data_target_crimes.csv\"\n",
    "df_final.to_csv(final_output, index=False)\n",
    "\n",
    "log_step(\"Interim Data Export\", df_final)\n",
    "console.print(\"[bold green]âœ“ Interim dataset exported.[/bold green]\")\n",
    "console.print(\n",
    "    f\"[bold]Final interim dataset:[/bold] {len(df_final):,} rows x {df_final.shape[1]} columns\"\n",
    ")\n",
    "console.print(f\"[bold]Saved to:[/bold] {final_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258df835",
   "metadata": {},
   "source": [
    "#### Section 7: Additional Spatial/Weather Repair (Load Interim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b788456",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\n",
    "    Panel.fit(\n",
    "        \"[bold cyan]STEP 11: Load interim data and repair spatial features[/bold cyan]\\n\\n\"\n",
    "        \"Tasks:\\n\"\n",
    "        \"â€¢ Load interim CSV\\n\"\n",
    "        \"â€¢ Show baseline missing data\\n\"\n",
    "        \"â€¢ Fill missing spatial data via spatial joins\\n\"\n",
    "        \"â€¢ Show before/after comparison\",\n",
    "        border_style=\"cyan\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load interim data\n",
    "INTERIM_PATH = INTERIM_DATA_FOLDER / \"apd_model_data_target_crimes.csv\"\n",
    "df = pd.read_csv(INTERIM_PATH)\n",
    "log_step(\"Step 11a: Loaded interim data\", df)\n",
    "\n",
    "# Show baseline missing data\n",
    "console.print(\"\\n[yellow]Missing data BEFORE spatial repair:[/yellow]\")\n",
    "missing_cols_initial = [\n",
    "    \"zone_int\", \n",
    "    \"npu\",      \n",
    "    \"neighborhood\",\n",
    "    \"neighborhood_label\",\n",
    "    \"campus_label\",\n",
    "    \"city_label\",\n",
    "]\n",
    "missing_table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "missing_table.add_column(\"Column\", style=\"cyan\")\n",
    "missing_table.add_column(\"Missing\", justify=\"right\", style=\"red\")\n",
    "missing_table.add_column(\"%\", justify=\"right\", style=\"yellow\")\n",
    "\n",
    "for col in missing_cols_initial:\n",
    "    if col in df.columns:\n",
    "        missing = df[col].isna().sum()\n",
    "        pct = (missing / len(df)) * 100\n",
    "        missing_table.add_row(col, f\"{missing:,}\", f\"{pct:.1f}%\")\n",
    "\n",
    "console.print(missing_table)\n",
    "\n",
    "spatial_cols_before: Dict[str, Any] = {}\n",
    "for col in [\"zone_int\", \"npu\", \"neighborhood\", \"neighborhood_label\", \"city_label\"]:\n",
    "    if col in df.columns:\n",
    "        missing = df[col].isna().sum()\n",
    "        pct = (missing / len(df)) * 100\n",
    "        spatial_cols_before[col] = (missing, pct)\n",
    "\n",
    "console.print(\"\\n[cyan]â†’ Beginning spatial joins to fill missing values...[/cyan]\")\n",
    "\n",
    "# Create GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df.dropna(subset=['longitude', 'latitude']).copy(),\n",
    "    geometry=gpd.points_from_xy(df.longitude, df.latitude), \n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af39bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(Panel(\"[bold cyan]STEP 12: Fill missing spatial data via spatial joins (Stabilized)[/bold cyan]\", border_style=\"cyan\"))\n",
    "\n",
    "\n",
    "def show_missing_comparison(df_local, cols_snapshot, step_name):\n",
    "    table = Table(\n",
    "        title=f\"{step_name} - Missing Data Comparison\",\n",
    "        show_header=True,\n",
    "        header_style=\"bold magenta\",\n",
    "    )\n",
    "    table.add_column(\"Column\", style=\"cyan\")\n",
    "    table.add_column(\"Before\", justify=\"right\", style=\"red\")\n",
    "    table.add_column(\"Before %\", justify=\"right\", style=\"yellow\")\n",
    "    table.add_column(\"After\", justify=\"right\", style=\"green\")\n",
    "    table.add_column(\"After %\", justify=\"right\", style=\"blue\")\n",
    "    table.add_column(\"Filled\", justify=\"right\", style=\"white\")\n",
    "\n",
    "    for col, (before, before_pct) in cols_snapshot.items():\n",
    "        if col in df_local.columns:\n",
    "            after = df_local[col].isna().sum()\n",
    "            after_pct = (after / len(df_local)) * 100\n",
    "            filled = before - after\n",
    "            table.add_row(\n",
    "                col,\n",
    "                f\"{before:,}\",\n",
    "                f\"{before_pct:.1f}%\",\n",
    "                f\"{after:,}\",\n",
    "                f\"{after_pct:.1f}%\",\n",
    "                f\"{filled:,}\",\n",
    "            )\n",
    "\n",
    "    console.print(table)\n",
    "\n",
    "\n",
    "spatial_cols_before: Dict[str, Any] = {}\n",
    "# FIX: Use standardized column names here\n",
    "for col in [\"zone_int\", \"npu\", \"neighborhood\", \"neighborhood_label\", \"city_label\"]:\n",
    "    if col in df.columns:\n",
    "        missing = df[col].isna().sum()\n",
    "        pct = (missing / len(df)) * 100\n",
    "        spatial_cols_before[col] = (missing, pct)\n",
    "\n",
    "# Create GeoDataFrame for ALL rows with valid coordinates\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df.dropna(subset=['longitude', 'latitude']).copy(),\n",
    "    geometry=gpd.points_from_xy(df.longitude, df.latitude), \n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "# Keep track of the indices we are working on (all non-null coordinate rows)\n",
    "gdf_indices = gdf.index.intersection(df.index)\n",
    "\n",
    "# --- Fill NPU ---\n",
    "if NPU_SHP.exists() and \"npu\" in df.columns: # Targeted column is 'npu'\n",
    "    console.print(\"\\n[cyan]â†’ Filling NPU via spatial join (intersects)...[/cyan]\")\n",
    "    gdf_npu = gpd.read_file(NPU_SHP).to_crs(\"EPSG:4326\")\n",
    "    npu_col_shp = 'NAME' if 'NAME' in gdf_npu.columns else 'NPU' # Source column in shapefile\n",
    "    npu_col = 'npu' # Target column in DataFrame\n",
    "    \n",
    "    # Identify rows that need NPU filling and have valid coordinates\n",
    "    missing_mask = df[npu_col].isna()\n",
    "    \n",
    "    if missing_mask.sum() > 0:\n",
    "        gdf_missing = gdf[gdf.index.isin(df[missing_mask].index)].copy()\n",
    "        \n",
    "        joined = gpd.sjoin(gdf_missing, gdf_npu[[npu_col_shp, 'geometry']], \n",
    "                           how=\"left\", predicate=\"intersects\").drop(columns=['index_right'])\n",
    "        \n",
    "        # Take the first match for each original index\n",
    "        joined_clean = joined.groupby(joined.index).first()\n",
    "        \n",
    "        # Safely fill the missing values in the main DataFrame 'df'\n",
    "        original_nans = df[npu_col].isna().sum()\n",
    "        \n",
    "        # Only fill if the join was successful (npu_col_shp value is not NaN)\n",
    "        valid_joins = joined_clean[npu_col_shp].dropna().index\n",
    "        \n",
    "        df.loc[valid_joins, npu_col] = df.loc[valid_joins, npu_col].fillna(\n",
    "            joined_clean.loc[valid_joins, npu_col_shp].values\n",
    "        )\n",
    "        \n",
    "        filled = original_nans - df[npu_col].isna().sum()\n",
    "        console.print(f\" Â [green]âœ“ Filled {filled:,} NPU values[/green]\")\n",
    "\n",
    "\n",
    "# --- Fill zone_int (District) ---\n",
    "if APD_ZONE_SHP.exists() and \"zone_int\" in df.columns:\n",
    "    console.print(\"\\n[cyan]â†’ Filling Zone Integer (District) via spatial join (intersects)...[/cyan]\")\n",
    "    gdf_zones = gpd.read_file(APD_ZONE_SHP).to_crs(\"EPSG:4326\")\n",
    "    zone_col = 'ZONE' \n",
    "    \n",
    "    missing_mask = df['zone_int'].isna()\n",
    "    \n",
    "    if missing_mask.sum() > 0:\n",
    "        gdf_missing = gdf[gdf.index.isin(df[missing_mask].index)].copy()\n",
    "        \n",
    "        joined = gpd.sjoin(gdf_missing, gdf_zones[[zone_col, 'geometry']], \n",
    "                           how=\"left\", predicate=\"intersects\").drop(columns=['index_right'])\n",
    "        joined_clean = joined.groupby(joined.index).first()\n",
    "        \n",
    "        original_nans = df['zone_int'].isna().sum()\n",
    "        \n",
    "        zone_int_filled = pd.to_numeric(\n",
    "            joined_clean[zone_col].astype(str).str.extract(r\"(\\d+)\", expand=False), \n",
    "            errors=\"coerce\"\n",
    "        )\n",
    "        \n",
    "        df.loc[zone_int_filled.index, 'zone_int'] = df.loc[zone_int_filled.index, 'zone_int'].fillna(zone_int_filled)\n",
    "        \n",
    "        filled = original_nans - df['zone_int'].isna().sum()\n",
    "        console.print(f\" Â [green]âœ“ Filled {filled:,} Zone Int values[/green]\")\n",
    "\n",
    "# --- Fill neighborhood ---\n",
    "if NEIGHBORHOOD_SHP.exists() and \"neighborhood\" in df.columns:\n",
    "    console.print(\"\\n[cyan]â†’ Filling Neighborhood via spatial join (intersects)...[/cyan]\")\n",
    "    gdf_neighborhoods = gpd.read_file(NEIGHBORHOOD_SHP).to_crs(\"EPSG:4326\")\n",
    "    name_col = 'NAME' \n",
    "    \n",
    "    missing_mask = df['neighborhood'].isna()\n",
    "    \n",
    "    if missing_mask.sum() > 0:\n",
    "        gdf_missing = gdf[gdf.index.isin(df[missing_mask].index)].copy()\n",
    "        \n",
    "        joined = gpd.sjoin(gdf_missing, gdf_neighborhoods[[name_col, 'geometry']], \n",
    "                           how=\"left\", predicate=\"intersects\").drop(columns=['index_right'])\n",
    "        joined_clean = joined.groupby(joined.index).first()\n",
    "        \n",
    "        original_nans = df['neighborhood'].isna().sum()\n",
    "        valid_joins = joined_clean[name_col].dropna().index\n",
    "        \n",
    "        df.loc[valid_joins, 'neighborhood'] = df.loc[valid_joins, 'neighborhood'].fillna(\n",
    "            joined_clean.loc[valid_joins, name_col].str.lower().values\n",
    "        )\n",
    "        \n",
    "        if 'neighborhood_label' in df.columns:\n",
    "            df.loc[valid_joins, 'neighborhood_label'] = df.loc[valid_joins, 'neighborhood_label'].fillna(\n",
    "                joined_clean.loc[valid_joins, name_col].str.upper().values\n",
    "            )\n",
    "        \n",
    "        filled = original_nans - df['neighborhood'].isna().sum()\n",
    "        console.print(f\" Â [green]âœ“ Filled {filled:,} Neighborhood values[/green]\")\n",
    "\n",
    "# --- Fill city_label ---\n",
    "if CITIES_SHP.exists() and \"city_label\" in df.columns:\n",
    "    console.print(\"\\n[cyan]â†’ Filling City Label via spatial join (intersects)...[/cyan]\")\n",
    "    gdf_cities = gpd.read_file(CITIES_SHP).to_crs(\"EPSG:4326\")\n",
    "    city_col = 'NAME'\n",
    "    \n",
    "    missing_mask = df['city_label'].isna()\n",
    "    \n",
    "    if missing_mask.sum() > 0:\n",
    "        gdf_missing = gdf[gdf.index.isin(df[missing_mask].index)].copy()\n",
    "        \n",
    "        joined = gpd.sjoin(gdf_missing, gdf_cities[[city_col, 'geometry']], \n",
    "                           how=\"left\", predicate=\"intersects\").drop(columns=['index_right'])\n",
    "        joined_clean = joined.groupby(joined.index).first()\n",
    "        \n",
    "        original_nans = df['city_label'].isna().sum()\n",
    "        valid_joins = joined_clean[city_col].dropna().index\n",
    "        \n",
    "        df.loc[valid_joins, 'city_label'] = df.loc[valid_joins, 'city_label'].fillna(\n",
    "            joined_clean.loc[valid_joins, city_col].str.upper().values\n",
    "        )\n",
    "        \n",
    "        filled = original_nans - df['city_label'].isna().sum()\n",
    "        console.print(f\" Â [green]âœ“ Filled {filled:,} City Label values[/green]\")\n",
    "\n",
    "\n",
    "console.print(\"\\n[green]âœ“ Spatial data repair complete.[/green]\")\n",
    "show_missing_comparison(df, spatial_cols_before, \"Step 11: Spatial Repair\")\n",
    "log_step(\"Step 12: Spatial data repaired via joins\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e7c9c0",
   "metadata": {},
   "source": [
    "#### Section 8: Campus Distance / Campus Code (Model-Friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a478a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(Panel(\"[bold cyan]STEP 13: Campus labels and distance features (Ensures campus_code is not missing)[/bold cyan]\", border_style=\"cyan\"))\n",
    "\n",
    "campus_before: Dict[str, Any] = {}\n",
    "for col in [\"campus_label\", \"campus_distance_m\", \"campus_code\"]:\n",
    "    if col in df.columns:\n",
    "        missing = df[col].isna().sum()\n",
    "        pct = (missing / len(df)) * 100\n",
    "        campus_before[col] = (missing, pct)\n",
    "\n",
    "# 1.5-mile threshold in meters\n",
    "DISTANCE_THRESHOLD_M = 2414.016\n",
    "console.print(f\"[cyan]Using 1.5-mile threshold ({DISTANCE_THRESHOLD_M:.0f} meters).[/cyan]\")\n",
    "\n",
    "# Haversine in meters (kept for reference, but not used in vectorized version)\n",
    "def haversine_meters(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "\n",
    "CAMPUS_ENCODING = {\n",
    "    \"none\": 0,\n",
    "    \"GSU\": 1,\n",
    "    \"GA_Tech\": 2,\n",
    "    \"Emory\": 3,\n",
    "    \"Clark\": 4,\n",
    "    \"Spelman\": 5,\n",
    "    \"Morehouse\": 6,\n",
    "    \"Morehouse_Med\": 7,\n",
    "    \"Atlanta_Metro\": 8,\n",
    "    \"Atlanta_Tech\": 9,\n",
    "    \"SCAD\": 10,\n",
    "    \"John_Marshall\": 11,\n",
    "}\n",
    "\n",
    "console.print(\"[cyan]Calculating distance to each campus and nearest campus (vectorized)...[/cyan]\")\n",
    "\n",
    "def calculate_nearest_campus_fully_vectorized(df):\n",
    "    \"\"\"Fully vectorized campus distance calculation using broadcasting.\"\"\"\n",
    "    # Filter valid coordinates\n",
    "    valid_mask = df['latitude'].notna() & df['longitude'].notna()\n",
    "    valid_df = df[valid_mask].copy()\n",
    "    \n",
    "    # Prepare results arrays\n",
    "    nearest_campus = pd.Series('none', index=df.index)\n",
    "    nearest_distance = pd.Series(np.nan, index=df.index)\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        return nearest_campus, nearest_distance\n",
    "    \n",
    "    # Convert campus centers to arrays\n",
    "    campus_names = list(SCHOOL_CENTERS.keys())\n",
    "    campus_lats = np.array([lat for lat, lon in SCHOOL_CENTERS.values()])\n",
    "    campus_lons = np.array([lon for lat, lon in SCHOOL_CENTERS.values()])\n",
    "    \n",
    "    # Calculate distances to all campuses at once (broadcasting)\n",
    "    crime_lats = valid_df['latitude'].values[:, np.newaxis]  # Shape: (n_crimes, 1)\n",
    "    crime_lons = valid_df['longitude'].values[:, np.newaxis]\n",
    "    \n",
    "    # Haversine formula vectorized\n",
    "    R = 6371000  # Earth radius in meters\n",
    "    lat1, lon1 = np.radians(crime_lats), np.radians(crime_lons)\n",
    "    lat2, lon2 = np.radians(campus_lats), np.radians(campus_lons)\n",
    "    \n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    distances = R * c  # Shape: (n_crimes, n_campuses)\n",
    "    \n",
    "    # Find nearest campus for each crime\n",
    "    min_dist_idx = distances.argmin(axis=1)\n",
    "    min_distances = distances[np.arange(len(distances)), min_dist_idx]\n",
    "    \n",
    "    # Apply threshold\n",
    "    within_threshold = min_distances <= DISTANCE_THRESHOLD_M\n",
    "    \n",
    "    nearest_campus.loc[valid_df.index[within_threshold]] = [\n",
    "        campus_names[idx] for idx in min_dist_idx[within_threshold]\n",
    "    ]\n",
    "    nearest_distance.loc[valid_df.index[within_threshold]] = min_distances[within_threshold]\n",
    "    \n",
    "    return nearest_campus, nearest_distance\n",
    "\n",
    "# Calculate campus labels and distances\n",
    "df['campus_label'], df['campus_distance_m'] = calculate_nearest_campus_fully_vectorized(df)\n",
    "df['campus_distance_m'] = df['campus_distance_m'].round(4)\n",
    "\n",
    "# âœ… ADD THESE MISSING LINES:\n",
    "# Numeric encoding (ensures no missing values for campus_code)\n",
    "df[\"campus_code\"] = df[\"campus_label\"].map(CAMPUS_ENCODING).fillna(0).astype(int)\n",
    "df[\"campus_distance_m\"] = df[\"campus_distance_m\"].fillna(0)\n",
    "\n",
    "# Binary flags for each campus\n",
    "binary_cols_created = []\n",
    "for campus in SCHOOL_CENTERS.keys():\n",
    "    col_name = f\"near_{campus.lower()}\"\n",
    "    df[col_name] = (df[\"campus_label\"] == campus).astype(int)\n",
    "    binary_cols_created.append(col_name)\n",
    "\n",
    "console.print(f\"[green]âœ“ Campus features created: {len(binary_cols_created)} near_* columns.[/green]\")\n",
    "console.print(f\"[green]âœ“ **campus_code** column is complete (non-proximal crimes = 0).[/green]\")\n",
    "\n",
    "# Distribution (including 'none')\n",
    "campus_table = Table(\n",
    "    title=\"Campus Distribution (including 'none')\",\n",
    "    show_header=True,\n",
    "    header_style=\"bold magenta\",\n",
    ")\n",
    "campus_table.add_column(\"Campus\", style=\"cyan\")\n",
    "campus_table.add_column(\"Count\", justify=\"right\", style=\"green\")\n",
    "campus_table.add_column(\"%\", justify=\"right\", style=\"yellow\")\n",
    "campus_table.add_column(\"Code\", justify=\"right\", style=\"white\")\n",
    "\n",
    "for campus, code in sorted(CAMPUS_ENCODING.items(), key=lambda x: x[1]):\n",
    "    count = (df[\"campus_label\"] == campus).sum()\n",
    "    pct = (count / len(df)) * 100\n",
    "    campus_table.add_row(campus, f\"{count:,}\", f\"{pct:.1f}%\", str(code))\n",
    "\n",
    "console.print(campus_table)\n",
    "\n",
    "show_missing_comparison(df, campus_before, \"Step 13\")\n",
    "log_step(\"Step 13: Campus distance and campus_code features\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba1831",
   "metadata": {},
   "source": [
    "#### Section 9: Full 2021â€“2025 Weather Refresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa5ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\n",
    "    Panel(\"[bold cyan]STEP 14: Fetch and merge full weather dataset (2021â€“2025) - Fixes missing 2025 data[/bold cyan]\", border_style=\"cyan\")\n",
    ")\n",
    "\n",
    "weather_before: Dict[str, Any] = {}\n",
    "weather_cols_check = [\n",
    "    \"temp_f\",\n",
    "    \"precip_in\",\n",
    "    \"rain_in\",\n",
    "    \"apparent_temp_f\",\n",
    "    \"weather_code_hourly\",\n",
    "    \"is_daylight\",\n",
    "    \"daylight_duration_sec\",\n",
    "    \"sunshine_duration_sec\",\n",
    "]\n",
    "for col in weather_cols_check:\n",
    "    # Ensure checking against the column names created in the prior merge\n",
    "    current_col = col if col in df.columns else (\n",
    "        'weather_code_hrly' if col == 'weather_code_hourly' else col\n",
    "    )\n",
    "    if current_col in df.columns:\n",
    "        missing = df[current_col].isna().sum()\n",
    "        pct = (missing / len(df)) * 100\n",
    "        weather_before[col] = (missing, pct)\n",
    "\n",
    "df[\"report_date\"] = pd.to_datetime(df[\"report_date\"])\n",
    "\n",
    "cache_session = requests_cache.CachedSession(\".cache\", expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "params = {\n",
    "    \"latitude\": 33.749,\n",
    "    \"longitude\": -84.388,\n",
    "    \"start_date\": \"2021-01-01\",\n",
    "    \"end_date\": \"2025-11-30\", # Extended to ensure all 2025 data is captured\n",
    "    \"hourly\": [\n",
    "        \"temperature_2m\",\n",
    "        \"precipitation\",\n",
    "        \"rain\",\n",
    "        \"apparent_temperature\",\n",
    "        \"weather_code\",\n",
    "        \"is_day\",\n",
    "    ],\n",
    "    \"daily\": [\n",
    "        \"sunrise\",\n",
    "        \"daylight_duration\",\n",
    "        \"sunshine_duration\",\n",
    "        \"precipitation_hours\",\n",
    "        \"rain_sum\",\n",
    "        \"temperature_2m_mean\",\n",
    "        \"weather_code\",\n",
    "    ],\n",
    "    \"timezone\": \"America/New_York\",\n",
    "    \"temperature_unit\": \"fahrenheit\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    console.print(\"[cyan]Fetching weather from Open-Meteo API...[/cyan]\")\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "    response = responses[0]\n",
    "\n",
    "    hourly = response.Hourly()\n",
    "    hourly_df = pd.DataFrame(\n",
    "        {\n",
    "            \"datetime\": pd.date_range(\n",
    "                start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "                inclusive=\"left\",\n",
    "            ),\n",
    "            \"temp_f\": hourly.Variables(0).ValuesAsNumpy(),\n",
    "            \"precip_in\": hourly.Variables(1).ValuesAsNumpy(),\n",
    "            \"rain_in\": hourly.Variables(2).ValuesAsNumpy(),\n",
    "            \"apparent_temp_f\": hourly.Variables(3).ValuesAsNumpy(),\n",
    "            \"weather_code_hourly\": hourly.Variables(4).ValuesAsNumpy(),\n",
    "            \"is_daylight\": hourly.Variables(5).ValuesAsNumpy().astype(int),\n",
    "        }\n",
    "    )\n",
    "    hourly_df[\"datetime\"] = (\n",
    "        hourly_df[\"datetime\"].dt.tz_convert(\"America/New_York\").dt.tz_localize(None)\n",
    "    )\n",
    "\n",
    "    daily = response.Daily()\n",
    "    daily_df = pd.DataFrame(\n",
    "        {\n",
    "            \"date\": pd.date_range(\n",
    "                start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=daily.Interval()),\n",
    "                inclusive=\"left\",\n",
    "            ),\n",
    "            \"sunrise\": daily.Variables(0).ValuesInt64AsNumpy(),\n",
    "            \"daylight_duration_sec\": daily.Variables(1).ValuesAsNumpy(),\n",
    "            \"sunshine_duration_sec\": daily.Variables(2).ValuesAsNumpy(),\n",
    "            \"precip_hours\": daily.Variables(3).ValuesAsNumpy(),\n",
    "            \"rain_sum_in\": daily.Variables(4).ValuesAsNumpy(),\n",
    "            \"temp_mean_f\": daily.Variables(5).ValuesAsNumpy(),\n",
    "            \"weather_code_daily\": daily.Variables(6).ValuesAsNumpy(),\n",
    "        }\n",
    "    )\n",
    "    daily_df[\"date\"] = (\n",
    "        daily_df[\"date\"]\n",
    "        .dt.tz_convert(\"America/New_York\")\n",
    "        .dt.tz_localize(None)\n",
    "        .dt.date\n",
    "    )\n",
    "\n",
    "    console.print(\n",
    "        f\"[green]âœ“ Fetched {len(hourly_df):,} hourly and {len(daily_df):,} daily weather records.[/green]\"\n",
    "    )\n",
    "    \n",
    "    # === FIX: Save the newly fetched data to the external folder ===\n",
    "    hourly_df.to_csv(HOURLY_WEATHER_PATH, index=False)\n",
    "    daily_df.to_csv(DAILY_WEATHER_PATH, index=False)\n",
    "    console.print(f\"[green]âœ“ Saved updated hourly data to: {HOURLY_WEATHER_PATH}[/green]\")\n",
    "    console.print(f\"[green]âœ“ Saved updated daily data to: {DAILY_WEATHER_PATH}[/green]\")\n",
    "    # ====================================================================\n",
    "\n",
    "    # Merge hourly weather\n",
    "    df[\"weather_datetime\"] = df[\"report_date\"].dt.floor(\"H\")\n",
    "\n",
    "    overlapping_hourly_cols = [\n",
    "        col for col in df.columns if col in hourly_df.columns and col != \"weather_datetime\"\n",
    "    ]\n",
    "    if overlapping_hourly_cols:\n",
    "        df = df.drop(columns=overlapping_hourly_cols)\n",
    "\n",
    "    df = df.merge(\n",
    "        hourly_df,\n",
    "        left_on=\"weather_datetime\",\n",
    "        right_on=\"datetime\",\n",
    "        how=\"left\",\n",
    "    ).drop(columns=[\"datetime\"])\n",
    "\n",
    "    # Merge daily weather\n",
    "    df[\"report_date_only\"] = df[\"report_date\"].dt.date\n",
    "    overlapping_daily_cols = [\n",
    "        col for col in df.columns if col in daily_df.columns and col != \"date\"\n",
    "    ]\n",
    "    if overlapping_daily_cols:\n",
    "        df = df.drop(columns=overlapping_daily_cols)\n",
    "\n",
    "    df = df.merge(\n",
    "        daily_df,\n",
    "        left_on=\"report_date_only\",\n",
    "        right_on=daily_df['date'].dt.date,\n",
    "        how=\"left\",\n",
    "    ).drop(columns=[\"date\", \"report_date_only\", \"key_0\"])\n",
    "\n",
    "    df = df.drop(columns=['weather_datetime'], errors='ignore')\n",
    "\n",
    "    console.print(\"[green]âœ“ Merged full 2021â€“2025 weather successfully.[/green]\")\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]Error fetching weather:[/bold red] {e}\")\n",
    "\n",
    "show_missing_comparison(df, weather_before, \"Step 14\")\n",
    "log_step(\"Step 14: Fetched and merged full weather dataset (2021â€“2025)\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d3fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(Panel(\"[bold cyan]STEP 15: Recalculate temperature-based flags[/bold cyan]\", border_style=\"cyan\"))\n",
    "\n",
    "if \"temp_f\" in df.columns:\n",
    "    p85 = df[\"temp_f\"].quantile(0.85)\n",
    "    p15 = df[\"temp_f\"].quantile(0.15)\n",
    "\n",
    "    console.print(\"[cyan]Temperature percentile thresholds:[/cyan]\")\n",
    "    console.print(f\"Â  85th percentile: [yellow]{p85:.1f}[/yellow] F\")\n",
    "    console.print(f\"Â  15th percentile: [yellow]{p15:.1f}[/yellow] F\")\n",
    "\n",
    "    df[\"is_hot\"] = (df[\"temp_f\"] >= p85).astype(int)\n",
    "    df[\"is_cold\"] = (df[\"temp_f\"] <= p15).astype(int)\n",
    "\n",
    "    hot_pct = (df[\"is_hot\"] == 1).sum() / len(df) * 100\n",
    "    cold_pct = (df[\"is_cold\"] == 1).sum() / len(df) * 100\n",
    "\n",
    "    console.print(\"[green]âœ“ Recalculated temperature flags:[/green]\")\n",
    "    console.print(f\"Â  is_hot (>= {p85:.1f} F): {hot_pct:.1f}%\")\n",
    "    console.print(f\"Â  is_cold (<= {p15:.1f} F): {cold_pct:.1f}%\")\n",
    "\n",
    "log_step(\"Step 15: Recalculated temperature-based flags\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747978f7",
   "metadata": {},
   "source": [
    "#### Section 10: Final Verification, Save, and Core EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e58ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(Panel(\"[bold cyan]STEP 16: Final verification and save[/bold cyan]\", border_style=\"cyan\"))\n",
    "\n",
    "# Final missing data summary\n",
    "all_cols_to_check = ['campus_distance_m', 'neighborhood', 'neighborhood_label', 'zone_int', 'npu', 'city_label', 'weather_code_hrly', 'is_daylight', 'temp_f', 'precip_in'] # Top 10 columns only\n",
    "final_missing: Dict[str, Any] = {}\n",
    "\n",
    "console.print(\"\\n[yellow]Final missing data summary (Top 10 columns only):[/yellow]\")\n",
    "final_table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "final_table.add_column(\"Column\", style=\"cyan\")\n",
    "final_table.add_column(\"Missing\", justify=\"right\", style=\"red\")\n",
    "final_table.add_column(\"%\", justify=\"right\", style=\"yellow\")\n",
    "\n",
    "for col in all_cols_to_check:\n",
    "    # Use fallback column names if needed\n",
    "    current_col = col if col in df.columns else (\n",
    "        'zone_int' if col == 'district' else (\n",
    "            'npu_label' if col == 'npu' else col\n",
    "        )\n",
    "    )\n",
    "    if current_col in df.columns:\n",
    "        missing = df[current_col].isna().sum()\n",
    "        pct = (missing / len(df)) * 100\n",
    "        final_table.add_row(col, f\"{missing:,}\", f\"{pct:.1f}%\")\n",
    "\n",
    "console.print(final_table)\n",
    "\n",
    "# Save final processed dataset\n",
    "OUTPUT_PATH = PROCESSED_DATA_FOLDER / \"target_crimes.csv\"\n",
    "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "log_step(\"Processed Data Export\", df)\n",
    "\n",
    "console.print(\n",
    "    Panel.fit(\n",
    "        \"[bold green]âœ“ CLEANING COMPLETE![/bold green]\\n\\n\"\n",
    "        f\"Saved to: [yellow]{OUTPUT_PATH}[/yellow]\\n\"\n",
    "        f\"Total records: [cyan]{len(df):,}[/cyan]\\n\"\n",
    "        f\"Total columns: [cyan]{len(df.columns)}[/cyan]\",\n",
    "        border_style=\"green\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beadef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(Panel(\"[bold magenta]STEP 17: Core EDA visualizations[/bold magenta]\", border_style=\"magenta\"))\n",
    "eda_df = df.copy()\n",
    "date_range_str = f\"{eda_df['year'].min()}â€“{eda_df['year'].max()}\"\n",
    "\n",
    "# Build GeoDataFrames for spatial EDA (needed for NPU map)\n",
    "gdf_full = gpd.GeoDataFrame(\n",
    "    eda_df.dropna(subset=['longitude', 'latitude']),\n",
    "    geometry=gpd.points_from_xy(eda_df.longitude, eda_df.latitude),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "\n",
    "# --- Quick Visuals -----------------------------------------------------------\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Crimes by hour\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x=\"incident_hour\", data=eda_df, color=\"steelblue\")\n",
    "plt.title(\"Crimes by Hour\")\n",
    "plt.xlabel(\"Hour of Day\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Crimes by day of week\n",
    "if \"day_of_week\" in eda_df.columns:\n",
    "    order = [\n",
    "        \"Monday\",\n",
    "        \"Tuesday\",\n",
    "        \"Wednesday\",\n",
    "        \"Thursday\",\n",
    "        \"Friday\",\n",
    "        \"Saturday\",\n",
    "        \"Sunday\",\n",
    "    ]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.countplot(x=\"day_of_week\", data=eda_df, order=order)\n",
    "    plt.title(\"Crimes by Day of Week\")\n",
    "    plt.xlabel(\"Day of Week\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Heatmap: day_of_week Ã— hour (KEEP)\n",
    "pivot = (\n",
    "    eda_df.pivot_table(\n",
    "        index=\"day_of_week\",\n",
    "        columns=\"incident_hour\",\n",
    "        values=\"incident_number\",\n",
    "        aggfunc=\"count\",\n",
    "    )\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "pivot = pivot.reindex(\n",
    "    [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.heatmap(pivot, cmap=\"mako\")\n",
    "plt.title(\"Heatmap: Day of Week Ã— Hour\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Day of Week\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Weather distribution (Only temp_f)\n",
    "if \"temp_f\" in eda_df.columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    eda_df[\"temp_f\"].hist(bins=30)\n",
    "    plt.title(\"Distribution of Temperature (temp_f)\")\n",
    "    plt.xlabel(r\"Temperature ($\\circ$F)\") # Added degrees symbol\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Plotly Interactive Monthly Comparison (KEEP) -----------------------------\n",
    "monthly_data = eda_df.groupby([eda_df['year'], eda_df['month']]).size().reset_index(name='count')\n",
    "monthly_data['year'] = monthly_data['year'].astype(str)\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "fig = px.line(monthly_data, x='month', y='count', color='year',\n",
    "              title='Monthly Crime Trends by Year (Interactive)',\n",
    "              labels={'month': 'Month', 'count': 'Number of Crimes', 'year': 'Year'},\n",
    "              markers=True)\n",
    "\n",
    "fig.update_xaxes(tickmode='linear', tick0=1, dtick=1,\n",
    "                 ticktext=month_names, tickvals=list(range(1, 13)))\n",
    "fig.update_layout(hovermode='x unified', height=600)\n",
    "fig.show()\n",
    "\n",
    "log_step(\"Step 17: Core EDA visualizations (including interactive Plotly)\", eda_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b6793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\n",
    "    Panel(\n",
    "        \"[bold cyan]STEP 18: Advanced temporal binning (6 x 4-hour bins)[/bold cyan]\",\n",
    "        border_style=\"cyan\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Overall time bin distribution (6-bin logic)\n",
    "time_bin_counts = (\n",
    "    eda_df[\"hour_block\"].value_counts()\n",
    "    .reindex([\"Early Night (0â€“4)\", \"Early Morning (5â€“8)\", \"Late Morning (9â€“12)\", \"Afternoon (13â€“16)\", \"Evening (17â€“20)\", \"Late Night (21â€“24)\"])\n",
    ")\n",
    "colors_bins = [\"#2C3E50\", \"#34495E\", \"#95A5A6\", \"#E67E22\", \"#E74C3C\", \"#8E44AD\"] # Consistent colors\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.bar(\n",
    "    range(len(time_bin_counts)),\n",
    "    time_bin_counts.values,\n",
    "    color=colors_bins,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "ax1.set_xticks(range(len(time_bin_counts)))\n",
    "ax1.set_xticklabels([l.split(' ')[0] for l in time_bin_counts.index], rotation=45, ha='right')\n",
    "ax1.set_title(\"Crime Distribution by Time Block (All Years)\")\n",
    "ax1.set_ylabel(\"Number of Crimes\")\n",
    "ax1.xlabel = 'Time Block'\n",
    "ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(time_bin_counts.values):\n",
    "    ax1.text(i, v + max(time_bin_counts.values) * 0.01, f\"{v:,}\", ha=\"center\", fontweight='bold')\n",
    "\n",
    "# Time bin by year\n",
    "time_bin_year = eda_df.groupby([\"year\", \"hour_block\"]).size().unstack(fill_value=0)\n",
    "time_bin_year = time_bin_year[\n",
    "    [\"Early Night (0â€“4)\", \"Early Morning (5â€“8)\", \"Late Morning (9â€“12)\", \"Afternoon (13â€“16)\", \"Evening (17â€“20)\", \"Late Night (21â€“24)\"]\n",
    "]\n",
    "\n",
    "x = np.arange(len(time_bin_year.index))\n",
    "width = 0.14\n",
    "\n",
    "for i, (bin_name, color) in enumerate(zip(time_bin_year.columns, colors_bins)):\n",
    "    ax2.bar(\n",
    "        x + i * width,\n",
    "        time_bin_year[bin_name].values,\n",
    "        width,\n",
    "        label=bin_name.split(' ')[0],\n",
    "        color=color,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "ax2.set_title(\"Crime by Time Block and Year\")\n",
    "ax2.set_xlabel(\"Year\")\n",
    "ax2.set_ylabel(\"Number of Crimes\")\n",
    "ax2.set_xticks(x + width * 2.5)\n",
    "ax2.set_xticklabels(time_bin_year.index)\n",
    "ax2.legend(title=\"Time Block\", loc=\"upper right\", fontsize=9, ncol=2)\n",
    "ax2.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Crimes per year\n",
    "crimes_per_year = eda_df[\"year\"].value_counts().sort_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "crimes_per_year.plot(kind=\"bar\", color=\"steelblue\", ax=ax, edgecolor=\"black\")\n",
    "ax.set_title(f\"Total Crimes Per Year ({date_range_str})\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Number of Crimes\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(crimes_per_year.values):\n",
    "    ax.text(i, v + max(crimes_per_year.values) * 0.01, f\"{v:,}\", ha=\"center\", fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Crimes by day of week (again, but from this EDA branch)\n",
    "day_order = [\n",
    "    \"Monday\",\n",
    "    \"Tuesday\",\n",
    "    \"Wednesday\",\n",
    "    \"Thursday\",\n",
    "    \"Friday\",\n",
    "    \"Saturday\",\n",
    "    \"Sunday\",\n",
    "]\n",
    "day_crimes = eda_df[\"day_of_week\"].value_counts().reindex(day_order)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.bar(\n",
    "    range(len(day_crimes)),\n",
    "    day_crimes.values,\n",
    "    color=\"lightblue\",\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "ax.set_title(\"Crimes by Day of Week\")\n",
    "ax.set_xlabel(\"Day of Week\")\n",
    "ax.set_ylabel(\"Number of Crimes\")\n",
    "ax.set_xticks(range(len(day_order)))\n",
    "ax.set_xticklabels(day_order, rotation=45, ha=\"right\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(day_crimes.values):\n",
    "    ax.text(i, v + max(day_crimes.values) * 0.01, f\"{v:,}\", ha=\"center\", fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "log_step(\"Step 18: Advanced temporal EDA (6-bin blocks and yearly trends)\", eda_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\n",
    "    Panel(\n",
    "        \"[bold cyan]STEP 19: Spatial EDA â€“ NPU choropleth, KDE, hexbin, GSU buffers[/bold cyan]\",\n",
    "        border_style=\"cyan\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Build GeoDataFrames for spatial EDA\n",
    "gdf_3857 = gdf_full.to_crs(epsg=3857)\n",
    "gdf_npu = gpd.read_file(NPU_SHP).to_crs(3857)\n",
    "gdf_cities = gpd.read_file(CITIES_SHP).to_crs(3857)\n",
    "atlanta_boundary = gdf_cities[gdf_cities[\"NAME\"] == \"Atlanta\"].iloc[0].geometry\n",
    "\n",
    "# Identify the shapefile column with NPU name/label\n",
    "candidate_npu_cols = [\"NPU\", \"NPU_ID\", \"NPU_NUM\", \"NPU_NAME\", \"NAME\"]\n",
    "npu_col_shp = next((c for c in candidate_npu_cols if c in gdf_npu.columns), gdf_npu.columns[0])\n",
    "npu_col = 'npu' # Use the standardized column name from the DataFrame\n",
    "\n",
    "# 19.1 NPU choropleth with bold black outlines and labels\n",
    "gdf_join_npu = gpd.sjoin(\n",
    "    gdf_3857, gdf_npu.rename(columns={npu_col_shp: 'npu_col_shp_merged'}, errors='ignore')[['npu_col_shp_merged', 'geometry']], \n",
    "    how=\"left\", predicate=\"intersects\"\n",
    ")\n",
    "# Merge crime counts based on the cleaned DataFrame column\n",
    "npu_counts = eda_df.groupby(npu_col).size().reset_index(name=\"crime_count\")\n",
    "gdf_npu_plot = gdf_npu.merge(\n",
    "    npu_counts, left_on=npu_col_shp, right_on=npu_col, how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "gdf_npu_plot.plot(\n",
    "    column=\"crime_count\",\n",
    "    cmap=\"Reds\",\n",
    "    legend=True,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1.5,\n",
    "    ax=ax,\n",
    "    legend_kwds={\"label\": \"Total crimes\", \"shrink\": 0.7},\n",
    ")\n",
    "\n",
    "for idx, row in gdf_npu_plot.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    label = row[npu_col_shp] if npu_col_shp in row else row[npu_col] # Use shapefile label first, then DataFrame label\n",
    "    count = int(row[\"crime_count\"])\n",
    "    ax.text(\n",
    "        centroid.x,\n",
    "        centroid.y,\n",
    "        f\"{label}\\n({count:,})\",\n",
    "        fontsize=9,\n",
    "        ha=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.7),\n",
    "    )\n",
    "\n",
    "ax.set_title(f\"Crime density by NPU ({date_range_str})\")\n",
    "ax.axis(\"off\")\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 19.2 KDE hotspot map\n",
    "coords = np.vstack([gdf_3857.geometry.x, gdf_3857.geometry.y]).T\n",
    "kde = KernelDensity(bandwidth=400, kernel=\"gaussian\").fit(coords)\n",
    "\n",
    "atl_bounds = atlanta_boundary.bounds\n",
    "x_margin = (atl_bounds[2] - atl_bounds[0]) * 0.1\n",
    "y_margin = (atl_bounds[3] - atl_bounds[1]) * 0.1\n",
    "\n",
    "xmin = atl_bounds[0] - x_margin\n",
    "xmax = atl_bounds[2] + x_margin\n",
    "ymin = atl_bounds[1] - y_margin\n",
    "ymax = atl_bounds[3] + y_margin\n",
    "\n",
    "xx, yy = np.mgrid[xmin:xmax:300j, ymin:ymax:300j]\n",
    "grid_points = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "z = np.exp(kde.score_samples(grid_points)).reshape(xx.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "im = ax.imshow(\n",
    "    z.T,\n",
    "    extent=[xmin, xmax, ymin, ymax],\n",
    "    origin=\"lower\",\n",
    "    cmap=\"hot\",\n",
    "    alpha=0.8,\n",
    "    vmin=z.max() * 0.3,\n",
    ")\n",
    "\n",
    "# NPU boundaries in bold black\n",
    "gdf_npu.boundary.plot(ax=ax, edgecolor=\"black\", linewidth=1.5, alpha=0.7)\n",
    "\n",
    "# Atlanta boundary in black, thicker\n",
    "gpd.GeoSeries([atlanta_boundary]).boundary.plot(\n",
    "    ax=ax, edgecolor=\"black\", linewidth=2\n",
    ")\n",
    "\n",
    "plt.colorbar(im, ax=ax, label=\"Crime density\", shrink=0.7)\n",
    "ax.set_title(\"Kernel density crime hotspot map\")\n",
    "ax.axis(\"off\")\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 19.3 Hexbin with NPU overlay (bold black)\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "hb = ax.hexbin(\n",
    "    gdf_3857.geometry.x,\n",
    "    gdf_3857.geometry.y,\n",
    "    gridsize=70,\n",
    "    cmap=\"Purples\",\n",
    "    mincnt=1,\n",
    "    extent=[xmin, xmax, ymin, ymax],\n",
    "    vmin=1,\n",
    ")\n",
    "\n",
    "gdf_npu.boundary.plot(ax=ax, edgecolor=\"black\", linewidth=1.5, alpha=0.7)\n",
    "\n",
    "gpd.GeoSeries([atlanta_boundary]).boundary.plot(\n",
    "    ax=ax, edgecolor=\"black\", linewidth=2\n",
    ")\n",
    "\n",
    "plt.colorbar(hb, ax=ax, label=\"Crime count per hexagon\", shrink=0.7)\n",
    "ax.set_title(\"Hexbin spatial density with NPU overlay\")\n",
    "ax.axis(\"off\")\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 19.4 GSU campus buffers with proper circles and crime points\n",
    "gsu_pt = Point(SCHOOL_CENTERS['GSU'][1], SCHOOL_CENTERS['GSU'][0])\n",
    "gsu_3857 = gpd.GeoSeries([gsu_pt], crs=4326).to_crs(3857).iloc[0]\n",
    "\n",
    "buffers = {\n",
    "    \"0.25 mile\": gsu_3857.buffer(402.336),\n",
    "    \"0.5 mile\": gsu_3857.buffer(804.672),\n",
    "    \"1 mile\": gsu_3857.buffer(1609.34),\n",
    "}\n",
    "\n",
    "max_buffer = buffers[\"1 mile\"]\n",
    "bounds = gpd.GeoSeries([max_buffer]).total_bounds\n",
    "buffer_margin = 800\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "\n",
    "# FIX: Buffer lines are distinct colors for visibility\n",
    "buffer_styles = [\n",
    "    (\"0.25 mile\", \"red\", 2.5),\n",
    "    (\"0.5 mile\", \"orange\", 2),\n",
    "    (\"1 mile\", \"yellow\", 1.5),\n",
    "]\n",
    "\n",
    "for name, color, lw in buffer_styles:\n",
    "    gpd.GeoSeries([buffers[name]]).plot(\n",
    "        ax=ax,\n",
    "        alpha=0,\n",
    "        edgecolor=color,\n",
    "        linewidth=lw,\n",
    "        label=f\"{name} radius\",\n",
    "        facecolor=\"none\",\n",
    "        linestyle=\"-\",\n",
    "    )\n",
    "\n",
    "crimes_in_area = gdf_3857.cx[bounds[0] : bounds[2], bounds[1] : bounds[3]]\n",
    "crimes_in_area.plot(\n",
    "    ax=ax,\n",
    "    color=\"red\",\n",
    "    markersize=2,\n",
    "    alpha=0.4,\n",
    "    label=f\"Crime incidents (n={len(crimes_in_area):,})\",\n",
    ")\n",
    "\n",
    "gpd.GeoSeries([gsu_3857]).plot(\n",
    "    ax=ax,\n",
    "    color=\"blue\",\n",
    "    markersize=60,\n",
    "    marker=\"*\",\n",
    "    zorder=5,\n",
    "    label=\"GSU campus center\",\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=1,\n",
    ")\n",
    "\n",
    "ax.set_xlim(bounds[0] - buffer_margin, bounds[2] + buffer_margin)\n",
    "ax.set_ylim(bounds[1] - buffer_margin, bounds[3] + buffer_margin)\n",
    "\n",
    "ax.set_title(\n",
    "    \"Crimes within GSU campus buffers\\nGSU center: (33.7538 N, 84.3880 W)\"\n",
    ")\n",
    "ax.legend(loc=\"upper left\", fontsize=10, framealpha=0.95)\n",
    "ax.axis(\"off\")\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "log_step(\"Step 19: Spatial EDA (NPU choropleth, KDE, hexbin, GSU buffers)\", eda_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc2d58d",
   "metadata": {},
   "source": [
    "#### Section 12: Comprehensive Crime Statistics Summary (Rich Formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17373356",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\n",
    "    Panel(\n",
    "        \"[bold cyan]STEP 20: Comprehensive crime statistics summary[/bold cyan]\",\n",
    "        border_style=\"cyan\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- Rich Summary Tables (6-BIN STANDARD) -------------------------------------\n",
    "\n",
    "# Offense breakdown (reusing eda_df from Step 17)\n",
    "if \"offense_category\" in eda_df.columns:\n",
    "    console.print(\"[cyan]Offense Category Breakdown[/cyan]\")\n",
    "    console.print(eda_df[\"offense_category\"].value_counts().to_string())\n",
    "\n",
    "summary_table = Table(\n",
    "    title=f\"COMPREHENSIVE CRIME STATISTICS SUMMARY ({date_range_str})\",\n",
    "    show_header=False,\n",
    "    header_style=\"bold magenta\",\n",
    "    show_lines=True,\n",
    ")\n",
    "summary_table.add_column(\"Metric\", style=\"cyan\")\n",
    "summary_table.add_column(\"Value\", style=\"green\")\n",
    "\n",
    "summary_table.add_row(\"[bold]Total Crimes[/bold]\", f\"{len(eda_df):,}\")\n",
    "summary_table.add_row(\n",
    "    \"[bold]Date Range[/bold]\",\n",
    "    f\"{eda_df['report_date'].min().date()} â†’ {eda_df['report_date'].max().date()}\",\n",
    ")\n",
    "\n",
    "# Temporal insights\n",
    "temporal_table = Table(\n",
    "    title=\"Temporal Insights\", show_header=True, header_style=\"bold yellow\", show_lines=True\n",
    ")\n",
    "temporal_table.add_column(\"Time Metric\", style=\"cyan\")\n",
    "temporal_table.add_column(\"Peak Value\", style=\"green\")\n",
    "temporal_table.add_column(\"Count\", justify=\"right\", style=\"magenta\")\n",
    "\n",
    "temporal_table.add_row(\n",
    "    \"Peak Hour\",\n",
    "    f\"{eda_df['incident_hour'].mode()[0]:02}:00\", # Fixed formatting to 2 digits\n",
    "    f\"{eda_df['incident_hour'].value_counts().max():,}\",\n",
    ")\n",
    "temporal_table.add_row(\n",
    "    \"Peak Day\",\n",
    "    f\"{eda_df['day_of_week'].mode()[0]}\",\n",
    "    f\"{eda_df['day_of_week'].value_counts().max():,}\",\n",
    ")\n",
    "month_names = {\n",
    "    i: name\n",
    "    for i, name in enumerate(\n",
    "        [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"],\n",
    "        1,\n",
    "    )\n",
    "}\n",
    "temporal_table.add_row(\n",
    "    \"Peak Month\",\n",
    "    f\"{month_names[eda_df['month'].mode()[0]]}\",\n",
    "    f\"{eda_df['month'].value_counts().max():,}\",\n",
    ")\n",
    "temporal_table.add_row(\n",
    "    \"Peak Year\",\n",
    "    f\"{eda_df['year'].mode()[0]}\",\n",
    "    f\"{eda_df['year'].value_counts().max():,}\",\n",
    ")\n",
    "\n",
    "# Time block analysis (6-BIN STANDARD)\n",
    "time_block_table = Table(\n",
    "    title=\"Time Block Analysis (6 x 4-Hour Bins)\",\n",
    "    show_header=True,\n",
    "    header_style=\"bold yellow\",\n",
    "    show_lines=True,\n",
    ")\n",
    "time_block_table.add_column(\"Time Block\", style=\"cyan\")\n",
    "time_block_table.add_column(\"Count\", justify=\"right\", style=\"green\")\n",
    "time_block_table.add_column(\"Percent\", justify=\"right\", style=\"magenta\")\n",
    "\n",
    "time_block_order = [\n",
    "    \"Early Night (0â€“4)\",\n",
    "    \"Early Morning (5â€“8)\",\n",
    "    \"Late Morning (9â€“12)\",\n",
    "    \"Afternoon (13â€“16)\",\n",
    "    \"Evening (17â€“20)\",\n",
    "    \"Late Night (21â€“24)\",\n",
    "]\n",
    "\n",
    "for block_name in time_block_order:\n",
    "    if \"hour_block\" in eda_df.columns:\n",
    "        count = (eda_df[\"hour_block\"] == block_name).sum()\n",
    "        pct = (count / len(eda_df)) * 100\n",
    "        time_block_table.add_row(block_name, f\"{count:,}\", f\"{pct:.1f}%\")\n",
    "\n",
    "# Day-of-week table\n",
    "day_of_week_table = Table(\n",
    "    title=\"Day of Week Analysis\",\n",
    "    show_header=True,\n",
    "    header_style=\"bold yellow\",\n",
    "    show_lines=True,\n",
    ")\n",
    "day_of_week_table.add_column(\"Day\", style=\"cyan\")\n",
    "day_of_week_table.add_column(\"Count\", justify=\"right\", style=\"green\")\n",
    "day_of_week_table.add_column(\"Percent\", justify=\"right\", style=\"magenta\")\n",
    "\n",
    "day_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "for day in day_order:\n",
    "    count = (eda_df[\"day_of_week\"] == day).sum()\n",
    "    pct = (count / len(eda_df)) * 100\n",
    "    day_of_week_table.add_row(day, f\"{count:,}\", f\"{pct:.1f}%\")\n",
    "\n",
    "rprint(summary_table)\n",
    "rprint(temporal_table)\n",
    "rprint(time_block_table)\n",
    "rprint(day_of_week_table)\n",
    "\n",
    "log_step(\"Step 20: Comprehensive crime statistics summary (Rich)\", eda_df)\n",
    "\n",
    "console.print(Panel(\"[bold green]âœ“ All pipeline steps complete![/bold green]\"))\n",
    "show_pipeline_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff484767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "dsci_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
