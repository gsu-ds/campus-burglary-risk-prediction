{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e4f687",
   "metadata": {},
   "source": [
    "### NOTEBOOK 1: APD DATA PIPELINE - STANDARDIZE, COMBINE, FEATURE ENGINEERING & EDA\n",
    "\n",
    "Save as: `01_wrangler.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "496568f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Import Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> Libraries imported successfully.                         <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>                                                          <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> Groups loaded:                                           <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> - Core Python: regex, math, typing, paths                <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> - Data Handling: pandas, numpy, geopandas                <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> - Visualization: matplotlib, seaborn, plotly, contextily <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> - Date &amp; Time: dateutil, holidays                        <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> - Geospatial: geopandas, shapely                         <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> - Weather / Networking: open-meteo, caching, retry       <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> - Rich Output: console, tables, panels                   <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mâ•­â”€\u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36m Import Summary \u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36mâ”€â•®\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m Libraries imported successfully.                         \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m                                                          \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m Groups loaded:                                           \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m - Core Python: regex, math, typing, paths                \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m - Data Handling: pandas, numpy, geopandas                \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m - Visualization: matplotlib, seaborn, plotly, contextily \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m - Date & Time: dateutil, holidays                        \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m - Geospatial: geopandas, shapely                         \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m - Weather / Networking: open-meteo, caching, retry       \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m - Rich Output: console, tables, panels                   \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Imports =================================================================\n",
    "# Required libraries:\n",
    "# pip install numpy pandas geopandas matplotlib seaborn contextily python-dateutil holidays\n",
    "# pip install shapely openmeteo-requests requests-cache retry-requests rich plotly\n",
    "\n",
    "# --- Core Python / Utilities ---\n",
    "import re\n",
    "from math import radians, sin, cos, sqrt, asin, atan2\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "# --- Data Handling ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd # Added import needed for notebook consistency\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import contextily as ctx\n",
    "\n",
    "# --- Date/Time Utilities ---\n",
    "from dateutil import parser\n",
    "import holidays\n",
    "\n",
    "# --- Geospatial Processing ---\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# --- Weather API / Networking ---\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "# --- Rich Console Debugging / Output ---\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.panel import Panel\n",
    "from rich import print as rprint\n",
    "\n",
    "# --- Spatial Density ---\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Initialize console\n",
    "console = Console()\n",
    "\n",
    "console.print(\n",
    "    Panel.fit(\n",
    "        \"Libraries imported successfully.\\n\\n\"\n",
    "        \"Groups loaded:\\n\"\n",
    "        \"- Core Python: regex, math, typing, paths\\n\"\n",
    "        \"- Data Handling: pandas, numpy, geopandas\\n\"\n",
    "        \"- Visualization: matplotlib, seaborn, plotly, contextily\\n\"\n",
    "        \"- Date & Time: dateutil, holidays\\n\"\n",
    "        \"- Geospatial: geopandas, shapely\\n\"\n",
    "        \"- Weather / Networking: open-meteo, caching, retry\\n\"\n",
    "        \"- Rich Output: console, tables, panels\",\n",
    "        title=\"Import Summary\",\n",
    "        border_style=\"cyan\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba527f13",
   "metadata": {},
   "source": [
    "#### APD Crime Data Processing Pipeline\n",
    "\n",
    "1. Configuration and logger\n",
    "2. Ingest and standardize raw CSVs\n",
    "3. Combine and deduplicate\n",
    "4. Clean and basic feature engineering\n",
    "5. Geospatial enrichment\n",
    "6. Date/context features\n",
    "7. Weather enrichment\n",
    "8. Campus distance/label features\n",
    "9. Final cleaning, export, and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe268224",
   "metadata": {},
   "source": [
    "#### Section 1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b6207bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Pipeline logger configured.</span>                                                                                     <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m \u001b[1;32mPipeline logger configured.\u001b[0m                                                                                     \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pipeline Logging Setup\n",
    "\n",
    "pipeline_log: List[Dict[str, Any]] = []\n",
    "\n",
    "# FIX: Stabilized log_step for ValueError: Cannot specify ',' with 's'\n",
    "def log_step(step_name: str, df: pd.DataFrame) -> None:\n",
    "    \"\"\"Record a pipeline step with shape info, safely handling N/A.\"\"\"\n",
    "    \n",
    "    # 1. Determine values and set format type\n",
    "    if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "        rows_val = \"N/A\"\n",
    "        cols_val = \"N/A\"\n",
    "        rows_str = rows_val\n",
    "        cols_str = cols_val\n",
    "    else:\n",
    "        rows_val = int(df.shape[0])\n",
    "        cols_val = int(df.shape[1])\n",
    "        # Apply comma formatting only when values are integers\n",
    "        rows_str = f\"{rows_val:,}\"\n",
    "        cols_str = str(cols_val)\n",
    "\n",
    "    # 2. Append the original raw values to the log (int or string 'N/A')\n",
    "    pipeline_log.append({\"step\": step_name, \"rows\": rows_val, \"cols\": cols_val})\n",
    "    \n",
    "    # 3. Print the result using the safely formatted strings\n",
    "    console.print(f\"[green]âœ“ {step_name}[/green] [cyan]â†’ shape: {rows_str} x {cols_str}[/cyan]\")\n",
    "\n",
    "\n",
    "def show_pipeline_table() -> None:\n",
    "    \"\"\"Display a Rich table summarizing all pipeline steps, safely formatting N/A values.\"\"\"\n",
    "    if not pipeline_log:\n",
    "        console.print(\"[red]No pipeline steps logged yet.[/red]\")\n",
    "        return\n",
    "\n",
    "    table = Table(title=\"ğŸ“Š Data Pipeline Summary\", show_lines=True)\n",
    "    table.add_column(\"Step\", style=\"cyan\", no_wrap=True)\n",
    "    table.add_column(\"Rows\", style=\"green\")\n",
    "    table.add_column(\"Cols\", style=\"yellow\")\n",
    "\n",
    "    for entry in pipeline_log:\n",
    "        # Determine values for printing\n",
    "        rows_val = entry['rows']\n",
    "        cols_val = entry['cols']\n",
    "        \n",
    "        # Safely format: use comma only for integers, use string otherwise\n",
    "        rows_str = f\"{rows_val:,}\" if isinstance(rows_val, int) else str(rows_val)\n",
    "        cols_str = str(cols_val) if isinstance(cols_val, int) else str(cols_val)\n",
    "        \n",
    "        table.add_row(entry[\"step\"], rows_str, cols_str)\n",
    "    \n",
    "    console.print(table)\n",
    "\n",
    "\n",
    "console.print(Panel(\"[bold green]Pipeline logger configured.[/bold green]\", border_style=\"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6e07a49",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Data Paths</span><span style=\"color: #008080; text-decoration-color: #008080\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Data paths configured.</span>                <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>                                       <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> Raw Data: <span style=\"color: #808000; text-decoration-color: #808000\">../data/raw/apd</span>             <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> Interim Data: <span style=\"color: #808000; text-decoration-color: #808000\">../data/interim/apd</span>     <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> External Data: <span style=\"color: #808000; text-decoration-color: #808000\">../data/external</span>       <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> Processed Data: <span style=\"color: #808000; text-decoration-color: #808000\">../data/processed/apd</span> <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> Shapefiles: <span style=\"color: #808000; text-decoration-color: #808000\">../data/raw/shapefiles</span>    <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mâ•­â”€\u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36m \u001b[0m\u001b[1;36mData Paths\u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36mâ”€â•®\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m \u001b[1;36mData paths configured.\u001b[0m                \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m                                       \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m Raw Data: \u001b[33m../data/raw/apd\u001b[0m             \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m Interim Data: \u001b[33m../data/interim/apd\u001b[0m     \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m External Data: \u001b[33m../data/external\u001b[0m       \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m Processed Data: \u001b[33m../data/processed/apd\u001b[0m \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m Shapefiles: \u001b[33m../data/raw/shapefiles\u001b[0m    \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Step </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000\">: Logger, paths, settings, and constants configured</span> <span style=\"color: #008080; text-decoration-color: #008080\">â†’ shape: N/A x N/A</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Step \u001b[0m\u001b[1;32m1\u001b[0m\u001b[32m: Logger, paths, settings, and constants configured\u001b[0m \u001b[36mâ†’ shape: N/A x N/A\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configure Data Paths\n",
    "\n",
    "DATA_DIR = Path(\"../data\")\n",
    "RAW_DATA_FOLDER = DATA_DIR / \"raw\" / \"apd\"\n",
    "INTERIM_DATA_FOLDER = DATA_DIR / \"interim\" / \"apd\"\n",
    "PROCESSED_DATA_FOLDER = DATA_DIR / \"processed\" / \"apd\"\n",
    "EXTERNAL_DATA_FOLDER = DATA_DIR / \"external\"\n",
    "SHAPEFILES_DIR = DATA_DIR / \"raw\" / \"shapefiles\"\n",
    "\n",
    "# Create necessary folders\n",
    "EXTERNAL_DATA_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "INTERIM_DATA_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DATA_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Weather CSVs (initial run; later we fetch full 2021â€“2025)\n",
    "HOURLY_WEATHER_PATH = EXTERNAL_DATA_FOLDER / \"atlanta_hourly_weather_2024.csv\"\n",
    "DAILY_WEATHER_PATH = EXTERNAL_DATA_FOLDER / \"atlanta_daily_weather_2024.csv\"\n",
    "\n",
    "# Shapefiles\n",
    "CITIES_SHP = SHAPEFILES_DIR / \"census_boundary_2024_sf\" / \"ga_census_places_2024.shp\"\n",
    "CAMPUS_SHP = SHAPEFILES_DIR / \"area_landmark_2024_sf\" / \"ga_census_landmarks_2023.shp\"\n",
    "NEIGHBORHOOD_SHP = SHAPEFILES_DIR / \"atl_neighborhood_sf\" / \"atl_neighborhoods.shp\"\n",
    "NPU_SHP = SHAPEFILES_DIR / \"atl_npu_sf\" / \"atl_npu_boundaries.shp\"\n",
    "APD_ZONE_SHP = SHAPEFILES_DIR / \"apd_zone_2019_sf\" / \"apd_police_zones_2019.shp\"\n",
    "\n",
    "# School center coordinates for distance-based enrichment\n",
    "SCHOOL_CENTERS = {\n",
    "    \"GSU\": (33.7530, -84.3863),\n",
    "    \"GA_Tech\": (33.7756, -84.3963),\n",
    "    \"Emory\": (33.7925, -84.3239),\n",
    "    \"Clark\": (33.7533, -84.4124),\n",
    "    \"Spelman\": (33.7460, -84.4129),\n",
    "    \"Morehouse\": (33.7483, -84.4126),\n",
    "    \"Morehouse_Med\": (33.7505, -84.4131),\n",
    "    \"Atlanta_Metro\": (33.7145, -84.4020),\n",
    "    \"Atlanta_Tech\": (33.7126, -84.4034),\n",
    "    \"SCAD\": (33.7997, -84.3920),\n",
    "    \"John_Marshall\": (33.7621, -84.3896),\n",
    "}\n",
    "\n",
    "console.print(\n",
    "    Panel.fit(\n",
    "        \"[bold cyan]Data paths configured.[/bold cyan]\\n\\n\"\n",
    "        f\"Raw Data: [yellow]{RAW_DATA_FOLDER}[/yellow]\\n\"\n",
    "        f\"Interim Data: [yellow]{INTERIM_DATA_FOLDER}[/yellow]\\n\"\n",
    "        f\"External Data: [yellow]{EXTERNAL_DATA_FOLDER}[/yellow]\\n\"\n",
    "        f\"Processed Data: [yellow]{PROCESSED_DATA_FOLDER}[/yellow]\\n\"\n",
    "        f\"Shapefiles: [yellow]{SHAPEFILES_DIR}[/yellow]\",\n",
    "        title=\"[bold cyan]Data Paths[/bold cyan]\",\n",
    "        border_style=\"cyan\",\n",
    "    )\n",
    ")\n",
    "\n",
    "log_step(\"Step 1: Logger, paths, settings, and constants configured\", pd.DataFrame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea5fbd5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Fetching initial weather data</span>                                                                                   <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> Location: (33.749 N, -84.388 W)                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> Date Range: 2021-01-01 to 2024-12-31                                                                            <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m \u001b[1;36mFetching initial weather data\u001b[0m                                                                                   \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m Location: (33.749 N, -84.388 W)                                                                                 \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m Date Range: 2021-01-01 to 2024-12-31                                                                            \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Saved hourly:</span> ..<span style=\"color: #800080; text-decoration-color: #800080\">/data/external/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">atlanta_hourly_weather_2024.csv</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Saved hourly:\u001b[0m ..\u001b[35m/data/external/\u001b[0m\u001b[95matlanta_hourly_weather_2024.csv\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Saved daily:</span> ..<span style=\"color: #800080; text-decoration-color: #800080\">/data/external/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">atlanta_daily_weather_2024.csv</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Saved daily:\u001b[0m ..\u001b[35m/data/external/\u001b[0m\u001b[95matlanta_daily_weather_2024.csv\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Total rows: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">35</span><span style=\"color: #008000; text-decoration-color: #008000\">,</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">064</span><span style=\"color: #008000; text-decoration-color: #008000\"> hourly, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000\">,</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">461</span><span style=\"color: #008000; text-decoration-color: #008000\"> daily</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Total rows: \u001b[0m\u001b[1;32m35\u001b[0m\u001b[32m,\u001b[0m\u001b[1;32m064\u001b[0m\u001b[32m hourly, \u001b[0m\u001b[1;32m1\u001b[0m\u001b[32m,\u001b[0m\u001b[1;32m461\u001b[0m\u001b[32m daily\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Step </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2</span><span style=\"color: #008000; text-decoration-color: #008000\">: Initial Atlanta weather </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2021</span><span style=\"color: #008000; text-decoration-color: #008000\">â€“</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2024</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span><span style=\"color: #008000; text-decoration-color: #008000\"> saved to data/external</span> <span style=\"color: #008080; text-decoration-color: #008080\">â†’ shape: N/A x N/A</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Step \u001b[0m\u001b[1;32m2\u001b[0m\u001b[32m: Initial Atlanta weather \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32m2021\u001b[0m\u001b[32mâ€“\u001b[0m\u001b[1;32m2024\u001b[0m\u001b[1;32m)\u001b[0m\u001b[32m saved to data/external\u001b[0m \u001b[36mâ†’ shape: N/A x N/A\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initial weather fetch (pre-fetch for initial merge)\n",
    "def fetch_atlanta_weather(\n",
    "    start_date=\"2021-01-01\", end_date=\"2024-12-31\", lat=33.749, lon=-84.388\n",
    "):\n",
    "    \"\"\"Fetch weather data from Open-Meteo and save hourly/daily CSVs for reference.\"\"\"\n",
    "    console.print(\n",
    "        Panel(\n",
    "            \"[bold cyan]Fetching initial weather data[/bold cyan]\\n\"\n",
    "            f\"Location: ({lat:.3f} N, {lon:.3f} W)\\n\"\n",
    "            f\"Date Range: {start_date} to {end_date}\",\n",
    "            border_style=\"cyan\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cache_session = requests_cache.CachedSession(\".cache\", expire_after=-1)\n",
    "    retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "    openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"hourly\": [\n",
    "            \"temperature_2m\",\n",
    "            \"precipitation\",\n",
    "            \"rain\",\n",
    "            \"apparent_temperature\",\n",
    "            \"weather_code\",\n",
    "            \"is_day\",\n",
    "        ],\n",
    "        \"daily\": [\n",
    "            \"sunrise\",\n",
    "            \"daylight_duration\",\n",
    "            \"sunshine_duration\",\n",
    "            \"precipitation_hours\",\n",
    "            \"rain_sum\",\n",
    "            \"temperature_2m_mean\",\n",
    "            \"weather_code\",\n",
    "        ],\n",
    "        \"timezone\": \"America/New_York\",\n",
    "        \"temperature_unit\": \"fahrenheit\",\n",
    "    }\n",
    "\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "    response = responses[0]\n",
    "\n",
    "    # Hourly\n",
    "    hourly = response.Hourly()\n",
    "    hourly_df = pd.DataFrame(\n",
    "        {\n",
    "            \"datetime\": pd.date_range(\n",
    "                start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "                inclusive=\"left\",\n",
    "            ),\n",
    "            \"temp_f\": hourly.Variables(0).ValuesAsNumpy(),\n",
    "            \"precip_in\": hourly.Variables(1).ValuesAsNumpy(),\n",
    "            \"rain_in\": hourly.Variables(2).ValuesAsNumpy(),\n",
    "            \"apparent_temp_f\": hourly.Variables(3).ValuesAsNumpy(),\n",
    "            \"weather_code\": hourly.Variables(4).ValuesAsNumpy(),\n",
    "            \"is_daylight\": hourly.Variables(5).ValuesAsNumpy().astype(int),\n",
    "        }\n",
    "    )\n",
    "    hourly_df[\"datetime\"] = (\n",
    "        hourly_df[\"datetime\"].dt.tz_convert(\"America/New_York\").dt.tz_localize(None)\n",
    "    )\n",
    "\n",
    "    # Daily\n",
    "    daily = response.Daily()\n",
    "    daily_df = pd.DataFrame(\n",
    "        {\n",
    "            \"date\": pd.date_range(\n",
    "                start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=daily.Interval()),\n",
    "                inclusive=\"left\",\n",
    "            ),\n",
    "            \"sunrise\": daily.Variables(0).ValuesInt64AsNumpy(),\n",
    "            \"daylight_duration_sec\": daily.Variables(1).ValuesAsNumpy(),\n",
    "            \"sunshine_duration_sec\": daily.Variables(2).ValuesAsNumpy(),\n",
    "            \"precip_hours\": daily.Variables(3).ValuesAsNumpy(),\n",
    "            \"rain_sum_in\": daily.Variables(4).ValuesAsNumpy(),\n",
    "            \"temp_mean_f\": daily.Variables(5).ValuesAsNumpy(),\n",
    "            \"weather_code\": daily.Variables(6).ValuesAsNumpy(),\n",
    "        }\n",
    "    )\n",
    "    daily_df[\"date\"] = (\n",
    "        daily_df[\"date\"]\n",
    "        .dt.tz_convert(\"America/New_York\")\n",
    "        .dt.tz_localize(None)\n",
    "        .dt.date\n",
    "    )\n",
    "\n",
    "    hourly_df.to_csv(HOURLY_WEATHER_PATH, index=False)\n",
    "    daily_df.to_csv(DAILY_WEATHER_PATH, index=False)\n",
    "\n",
    "    console.print(f\"[green]âœ“ Saved hourly:[/green] {HOURLY_WEATHER_PATH}\")\n",
    "    console.print(f\"[green]âœ“ Saved daily:[/green] {DAILY_WEATHER_PATH}\")\n",
    "    console.print(\n",
    "        f\"[green]âœ“ Total rows: {len(hourly_df):,} hourly, {len(daily_df):,} daily[/green]\"\n",
    "    )\n",
    "\n",
    "    return hourly_df, daily_df\n",
    "\n",
    "\n",
    "fetch_atlanta_weather()\n",
    "log_step(\"Step 2: Initial Atlanta weather (2021â€“2024) saved to data/external\", pd.DataFrame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6261a3",
   "metadata": {},
   "source": [
    "#### Section 2: Standardize & Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39c7a888",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def standardize_column_name(col: str) -> str:\n",
    "    \"\"\"Convert column name to snake_case format.\"\"\"\n",
    "    col = re.sub(r\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", col)\n",
    "    col = re.sub(r\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", col)\n",
    "    col = col.lower()\n",
    "    col = re.sub(r\"[\\s\\-\\.\\,\\(\\)\\[\\]\\{\\}]+\", \"_\", col)\n",
    "    col = re.sub(r\"[^\\w]\", \"\", col)\n",
    "    col = re.sub(r\"_+\", \"_\", col).strip(\"_\")\n",
    "    return col\n",
    "\n",
    "\n",
    "def combine_and_deduplicate(files: List[Path], dedupe_key: str) -> pd.DataFrame:\n",
    "    \"\"\"Combine multiple CSVs, standardize columns, and drop duplicates.\"\"\"\n",
    "    console.print(\"\\n[bold cyan]â•â•â• Combining data files â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "    dfs = []\n",
    "    for filepath in files:\n",
    "        console.print(f\"[cyan]Reading and standardizing:[/cyan] {filepath.name}\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        df.columns = [standardize_column_name(c) for c in df.columns]\n",
    "        dfs.append(df)\n",
    "\n",
    "    df_combined = pd.concat(dfs, ignore_index=True)\n",
    "    total_rows = len(df_combined)\n",
    "\n",
    "    # New logger step: Ingest Raw Data\n",
    "    log_step(\"Ingest Raw Data (standardized columns, pre-dedup)\", df_combined)\n",
    "\n",
    "    if dedupe_key not in df_combined.columns:\n",
    "        raise KeyError(\n",
    "            f\"dedupe_key='{dedupe_key}' not found in columns: {df_combined.columns.tolist()}\"\n",
    "        )\n",
    "\n",
    "    df_dedup = df_combined.drop_duplicates(subset=[dedupe_key])\n",
    "    duplicates = total_rows - len(df_dedup)\n",
    "\n",
    "    console.print(f\"[yellow]Combined:[/yellow] {total_rows:,} rows\")\n",
    "    console.print(f\"[red]Removed:[/red] {duplicates:,} duplicate rows\")\n",
    "\n",
    "    log_step(\"Step 3: Standardize & Combine, deduplicated by incident_number\", df_dedup)\n",
    "    return df_dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d344a93a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Found </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> CSV file(s).</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mFound \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;36m CSV \u001b[0m\u001b[1;36mfile\u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36ms\u001b[0m\u001b[1;36m)\u001b[0m\u001b[1;36m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">â•â•â• Combining data files â•â•â•</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mâ•â•â• Combining data files â•â•â•\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Reading and standardizing:</span> apd_2020_2024.csv\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mReading and standardizing:\u001b[0m apd_2020_2024.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Reading and standardizing:</span> apd_2023_2025.csv\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mReading and standardizing:\u001b[0m apd_2023_2025.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Reading and standardizing:</span> apd_2021_2024.csv\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mReading and standardizing:\u001b[0m apd_2021_2024.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Ingest Raw Data </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">standardized columns, pre-dedup</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span> <span style=\"color: #008080; text-decoration-color: #008080\">â†’ shape: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">372</span><span style=\"color: #008080; text-decoration-color: #008080\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">864</span><span style=\"color: #008080; text-decoration-color: #008080\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Ingest Raw Data \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mstandardized columns, pre-dedup\u001b[0m\u001b[1;32m)\u001b[0m \u001b[36mâ†’ shape: \u001b[0m\u001b[1;36m372\u001b[0m\u001b[36m,\u001b[0m\u001b[1;36m864\u001b[0m\u001b[36m x \u001b[0m\u001b[1;36m28\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Combined:</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">372</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">864</span> rows\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mCombined:\u001b[0m \u001b[1;36m372\u001b[0m,\u001b[1;36m864\u001b[0m rows\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">Removed:</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">106</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">903</span> duplicate rows\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31mRemoved:\u001b[0m \u001b[1;36m106\u001b[0m,\u001b[1;36m903\u001b[0m duplicate rows\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Step </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000\">: Standardize &amp; Combine, deduplicated by incident_number</span> <span style=\"color: #008080; text-decoration-color: #008080\">â†’ shape: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">265</span><span style=\"color: #008080; text-decoration-color: #008080\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">961</span><span style=\"color: #008080; text-decoration-color: #008080\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Step \u001b[0m\u001b[1;32m3\u001b[0m\u001b[32m: Standardize & Combine, deduplicated by incident_number\u001b[0m \u001b[36mâ†’ shape: \u001b[0m\u001b[1;36m265\u001b[0m\u001b[36m,\u001b[0m\u001b[1;36m961\u001b[0m\u001b[36m x \u001b[0m\u001b[1;36m28\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function Execution: Combine Raw Data CSVs\n",
    "\n",
    "input_files = list(RAW_DATA_FOLDER.glob(\"*.csv\"))\n",
    "\n",
    "if not input_files:\n",
    "    console.print(f\"[bold red]Warning:[/bold red] No CSV files found in '{RAW_DATA_FOLDER}'\")\n",
    "    df_combined = pd.DataFrame()\n",
    "else:\n",
    "    console.print(f\"[bold cyan]Found {len(input_files)} CSV file(s).[/bold cyan]\")\n",
    "    df_combined = combine_and_deduplicate(\n",
    "        files=input_files, dedupe_key=\"incident_number\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b78ff",
   "metadata": {},
   "source": [
    "#### Section 3: Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5fdc1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">â•â•â• Cleaning APD data â•â•â•</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mâ•â•â• Cleaning APD data â•â•â•\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Dropped </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">11</span><span style=\"color: #808000; text-decoration-color: #808000\"> columns:</span> report_number, zone, fire_arm_involved, occurred_from_date, occurred_to_date, part, vic_count, \n",
       "is_bias_motivation_involved, x, y, beat_text\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mDropped \u001b[0m\u001b[1;33m11\u001b[0m\u001b[33m columns:\u001b[0m report_number, zone, fire_arm_involved, occurred_from_date, occurred_to_date, part, vic_count, \n",
       "is_bias_motivation_involved, x, y, beat_text\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">One-hot encoding:</span> <span style=\"color: #008000; text-decoration-color: #008000\">'event_watch'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mOne-hot encoding:\u001b[0m \u001b[32m'event_watch'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Created </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000\"> standardized one-hot columns</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Created \u001b[0m\u001b[1;32m3\u001b[0m\u001b[32m standardized one-hot columns\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Step </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000\">: Clean and drop selected columns</span> <span style=\"color: #008080; text-decoration-color: #008080\">â†’ shape: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">265</span><span style=\"color: #008080; text-decoration-color: #008080\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">961</span><span style=\"color: #008080; text-decoration-color: #008080\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Step \u001b[0m\u001b[1;32m4\u001b[0m\u001b[32m: Clean and drop selected columns\u001b[0m \u001b[36mâ†’ shape: \u001b[0m\u001b[1;36m265\u001b[0m\u001b[36m,\u001b[0m\u001b[1;36m961\u001b[0m\u001b[36m x \u001b[0m\u001b[1;36m19\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_apd_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Column drops, normalization, and one-hot encoding.\"\"\"\n",
    "    console.print(\"\\n[bold cyan]â•â•â• Cleaning APD data â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Define and drop unnecessary columns\n",
    "    columns_to_drop = [\n",
    "        \"report_number\",\n",
    "        \"zone\",\n",
    "        \"fire_arm_involved\",\n",
    "        \"object_id\",\n",
    "        \"occurred_from_date\",\n",
    "        \"occurred_to_date\",\n",
    "        \"part\",\n",
    "        \"vic_count\",\n",
    "        \"is_bias_motivation_involved\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        \"beat_text\",\n",
    "    ]\n",
    "\n",
    "    existing_drops = [col for col in columns_to_drop if col in df.columns]\n",
    "    if existing_drops:\n",
    "        df = df.drop(columns=existing_drops)\n",
    "        console.print(f\"[yellow]Dropped {len(existing_drops)} columns:[/yellow] {', '.join(existing_drops)}\")\n",
    "\n",
    "    # 2. Rename columns\n",
    "    if \"objectid\" in df.columns:\n",
    "        df = df.rename(columns={\"objectid\": \"object_id\"})\n",
    "    \n",
    "    # 3. One-hot encode 'event_watch'\n",
    "    if \"event_watch\" in df.columns:\n",
    "        console.print(\"[cyan]One-hot encoding:[/cyan] 'event_watch'\")\n",
    "        one_hot = pd.get_dummies(df[\"event_watch\"], prefix=\"event_watch\", dummy_na=False)\n",
    "        one_hot.columns = [standardize_column_name(col) for col in one_hot.columns]\n",
    "        df = pd.concat([df.drop(columns=[\"event_watch\"]), one_hot], axis=1)\n",
    "        console.print(f\"[green]âœ“ Created {len(one_hot.columns)} standardized one-hot columns[/green]\")\n",
    "\n",
    "    # 4. Convert text to lowercase\n",
    "    text_columns = [\"location_type\", \"street_address\", \"nibrs_offense\"]\n",
    "    for col in text_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.lower()\n",
    "\n",
    "    log_step(\"Step 4: Clean and drop selected columns\", df)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_clean = clean_apd_data(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c9af498",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">â•â•â• Standardizing </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'report_date'</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> column with robust parser â•â•â•</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mâ•â•â• Standardizing \u001b[0m\u001b[1;36m'report_date'\u001b[0m\u001b[1;36m column with robust parser â•â•â•\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Total rows:</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">265</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">961</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mTotal rows:\u001b[0m \u001b[1;36m265\u001b[0m,\u001b[1;36m961\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">Successfully standardized:</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">265</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">961</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mSuccessfully standardized:\u001b[0m \u001b[1;36m265\u001b[0m,\u001b[1;36m961\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Unrecoverable dates dropped:</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mUnrecoverable dates dropped:\u001b[0m \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Step </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span><span style=\"color: #008000; text-decoration-color: #008000\">: Robust date standardization</span> <span style=\"color: #008080; text-decoration-color: #008080\">â†’ shape: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">265</span><span style=\"color: #008080; text-decoration-color: #008080\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">961</span><span style=\"color: #008080; text-decoration-color: #008080\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Step \u001b[0m\u001b[1;32m5\u001b[0m\u001b[32m: Robust date standardization\u001b[0m \u001b[36mâ†’ shape: \u001b[0m\u001b[1;36m265\u001b[0m\u001b[36m,\u001b[0m\u001b[1;36m961\u001b[0m\u001b[36m x \u001b[0m\u001b[1;36m20\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Examples of corrected formats:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;34mExamples of corrected formats:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">           _raw_report_date         report_date\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">322728</span>      <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:02</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:02:00</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">343266</span>      <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:33</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">08</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:33:00</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3487</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2021</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">5:24:13</span> PM <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2021</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">04</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">17:24:13</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">333162</span>       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:10</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:10:00</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342851</span>      <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:28</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">08</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:28:00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "           _raw_report_date         report_date\n",
       "\u001b[1;36m322728\u001b[0m      \u001b[1;36m3\u001b[0m/\u001b[1;36m26\u001b[0m/\u001b[1;36m2025\u001b[0m \u001b[1;92m21:02\u001b[0m \u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m21:02:00\u001b[0m\n",
       "\u001b[1;36m343266\u001b[0m      \u001b[1;36m8\u001b[0m/\u001b[1;36m17\u001b[0m/\u001b[1;36m2025\u001b[0m \u001b[1;92m21:33\u001b[0m \u001b[1;36m2025\u001b[0m-\u001b[1;36m08\u001b[0m-\u001b[1;36m17\u001b[0m \u001b[1;92m21:33:00\u001b[0m\n",
       "\u001b[1;36m3487\u001b[0m    \u001b[1;36m9\u001b[0m/\u001b[1;36m4\u001b[0m/\u001b[1;36m2021\u001b[0m \u001b[1;92m5:24:13\u001b[0m PM \u001b[1;36m2021\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m04\u001b[0m \u001b[1;92m17:24:13\u001b[0m\n",
       "\u001b[1;36m333162\u001b[0m       \u001b[1;36m6\u001b[0m/\u001b[1;36m6\u001b[0m/\u001b[1;36m2025\u001b[0m \u001b[1;92m11:10\u001b[0m \u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m06\u001b[0m \u001b[1;92m11:10:00\u001b[0m\n",
       "\u001b[1;36m342851\u001b[0m      \u001b[1;36m8\u001b[0m/\u001b[1;36m14\u001b[0m/\u001b[1;36m2025\u001b[0m \u001b[1;92m21:28\u001b[0m \u001b[1;36m2025\u001b[0m-\u001b[1;36m08\u001b[0m-\u001b[1;36m14\u001b[0m \u001b[1;92m21:28:00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Date Standardization\n",
    "\n",
    "console.print(\"\\n[bold cyan]â•â•â• Standardizing 'report_date' column with robust parser â•â•â•[/bold cyan]\\n\")\n",
    "total_rows = len(df_clean)\n",
    "\n",
    "# Preserve raw values for comparison\n",
    "df_clean[\"_raw_report_date\"] = df_clean[\"report_date\"].astype(str).str.strip()\n",
    "\n",
    "\n",
    "def parse_report_date(x):\n",
    "    \"\"\"Safely parse mixed APD/NIBRS date formats.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return pd.NaT\n",
    "\n",
    "    x = str(x).strip()\n",
    "\n",
    "    # Explicit patterns: MM/DD/YYYY HH:MM:SS AM/PM, etc.\n",
    "    if re.match(r\"^\\d{1,2}/\\d{1,2}/\\d{4} \\d{1,2}:\\d{2}:\\d{2} [APMapm]{2}$\", x):\n",
    "        return pd.to_datetime(x, format=\"%m/%d/%Y %I:%M:%S %p\", errors=\"coerce\")\n",
    "    if re.match(r\"^\\d{1,2}/\\d{1,2}/\\d{4} \\d{1,2}:\\d{2} [APMapm]{2}$\", x):\n",
    "        return pd.to_datetime(x, format=\"%m/%d/%Y %I:%M %p\", errors=\"coerce\")\n",
    "    if re.match(r\"^\\d{1,2}/\\d{1,2}/\\d{4} \\d{1,2}:\\d{2}$\", x):\n",
    "        return pd.to_datetime(x, format=\"%m/%d/%Y %H:%M\", errors=\"coerce\")\n",
    "\n",
    "    # Fallback\n",
    "    try:\n",
    "        return parser.parse(x, fuzzy=True)\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "\n",
    "df_clean[\"report_date\"] = df_clean[\"_raw_report_date\"].apply(parse_report_date)\n",
    "invalid = df_clean[\"report_date\"].isna().sum()\n",
    "parsed = total_rows - invalid\n",
    "\n",
    "console.print(f\"[cyan]Total rows:[/cyan] {total_rows:,}\")\n",
    "console.print(f\"[green]Successfully standardized:[/green] {parsed:,}\")\n",
    "console.print(f\"[yellow]Unrecoverable dates dropped:[/yellow] {invalid:,}\")\n",
    "\n",
    "df_clean = df_clean.dropna(subset=[\"report_date\"]).copy()\n",
    "df_clean[\"report_date\"] = pd.to_datetime(df_clean[\"report_date\"])\n",
    "\n",
    "log_step(\"Step 5: Robust date standardization\", df_clean)\n",
    "\n",
    "console.print(\"\\n[bold blue]Examples of corrected formats:[/bold blue]\")\n",
    "changed = df_clean[\n",
    "    df_clean[\"_raw_report_date\"] != df_clean[\"report_date\"].astype(str)\n",
    "]\n",
    "if len(changed) > 0:\n",
    "    console.print(\n",
    "        changed.sample(min(5, len(changed)))[[\"_raw_report_date\", \"report_date\"]]\n",
    "    )\n",
    "else:\n",
    "    console.print(\"[yellow]All dates already standardized.[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d3d86f",
   "metadata": {},
   "source": [
    "#### Section 4: Geospatial Enrichment (NPU, Zone, Campus Footprints, City, Neighborhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89871434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">â•â•â• Spatial enrichment â•â•â•</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mâ•â•â• Spatial enrichment â•â•â•\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Data projected to EPSG:</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3857</span><span style=\"color: #008000; text-decoration-color: #008000\"> for joins.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Data projected to EPSG:\u001b[0m\u001b[1;32m3857\u001b[0m\u001b[32m for joins.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Attaching NPU labels </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">output column: </span><span style=\"color: #008080; text-decoration-color: #008080\">'npu'</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mAttaching NPU labels \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36moutput column: \u001b[0m\u001b[36m'npu'\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Attaching APD zones...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mAttaching APD zones\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Attaching campus footprint labels...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mAttaching campus footprint labels\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Attaching neighborhood labels...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mAttaching neighborhood labels\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Attaching city labels...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mAttaching city labels\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Step </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">6</span><span style=\"color: #008000; text-decoration-color: #008000\">: Spatial enrichment </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">NPU, zone, campus footprints, city, neighborhood</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span> <span style=\"color: #008080; text-decoration-color: #008080\">â†’ shape: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">265</span><span style=\"color: #008080; text-decoration-color: #008080\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">965</span><span style=\"color: #008080; text-decoration-color: #008080\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Step \u001b[0m\u001b[1;32m6\u001b[0m\u001b[32m: Spatial enrichment \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mNPU, zone, campus footprints, city, neighborhood\u001b[0m\u001b[1;32m)\u001b[0m \u001b[36mâ†’ shape: \u001b[0m\u001b[1;36m265\u001b[0m\u001b[36m,\u001b[0m\u001b[1;36m965\u001b[0m\u001b[36m x \u001b[0m\u001b[1;36m25\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Geospatial Helper Functions (NPU STANDARDIZED TO 'npu')\n",
    "\n",
    "def haversine_distance(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"Great-circle distance between two points in miles (R=3956).\"\"\"\n",
    "    R = 3956\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "\n",
    "    return c * R\n",
    "\n",
    "\n",
    "def to_gdf(df: pd.DataFrame, lon_col: str = \"longitude\", lat_col: str = \"latitude\"):\n",
    "    \"\"\"Convert a DataFrame with lon/lat into a GeoDataFrame in EPSG:4326.\"\"\"\n",
    "    for c in (lon_col, lat_col):\n",
    "        if c not in df.columns:\n",
    "            raise KeyError(f\"Expected coordinate column '{c}' not found.\")\n",
    "    return gpd.GeoDataFrame(\n",
    "        df,\n",
    "        geometry=gpd.points_from_xy(df[lon_col], df[lat_col]),\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "\n",
    "\n",
    "def load_shapefile(path: Path, target_crs) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Load shapefile and ensure CRS alignment.\"\"\"\n",
    "    try:\n",
    "        gdf = gpd.read_file(path)\n",
    "    except Exception:\n",
    "        console.print(f\"[bold red]FATAL: Could not load shapefile:[/bold red] {path}\")\n",
    "        raise\n",
    "\n",
    "    if gdf.crs is None:\n",
    "        gdf = gdf.set_crs(\"EPSG:4326\")\n",
    "    return gdf.to_crs(target_crs)\n",
    "\n",
    "\n",
    "def attach_campus(gdf: gpd.GeoDataFrame, campus_shp: Path) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Attach closest campus footprint label where available (MTFCC S1400).\"\"\"\n",
    "    console.print(\"[cyan]Attaching campus footprint labels...[/cyan]\")\n",
    "    campus = load_shapefile(campus_shp, gdf.crs)\n",
    "    if \"MTFCC\" in campus.columns:\n",
    "        campus = campus[campus[\"MTFCC\"] == \"S1400\"]\n",
    "    name_col = \"FULLNAME\" if \"FULLNAME\" in campus.columns else campus.columns[0]\n",
    "    campus = campus[[name_col, \"geometry\"]].rename(columns={name_col: \"campus_label\"})\n",
    "\n",
    "    gdf = gpd.sjoin_nearest(gdf, campus, how=\"left\")\n",
    "    gdf = gdf.drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def attach_neighborhood(\n",
    "    gdf: gpd.GeoDataFrame, nhood_shp: Path\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"Attach neighborhood labels (within polygon).\"\"\"\n",
    "    console.print(\"[cyan]Attaching neighborhood labels...[/cyan]\")\n",
    "    nhoods = load_shapefile(nhood_shp, gdf.crs)\n",
    "    name_col = \"NAME\" if \"NAME\" in nhoods.columns else nhoods.columns[0]\n",
    "    nhoods = nhoods[[name_col, \"geometry\"]].rename(\n",
    "        columns={name_col: \"neighborhood_label\"}\n",
    "    )\n",
    "\n",
    "    gdf = gpd.sjoin(gdf, nhoods, how=\"left\", predicate=\"within\")\n",
    "    gdf = gdf.drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def attach_city(gdf: gpd.GeoDataFrame, cities_shp: Path) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Attach city labels.\"\"\"\n",
    "    console.print(\"[cyan]Attaching city labels...[/cyan]\")\n",
    "    cities = load_shapefile(cities_shp, gdf.crs)\n",
    "    name_col = \"NAME\" if \"NAME\" in cities.columns else cities.columns[0]\n",
    "    cities = cities[[name_col, \"geometry\"]].rename(columns={name_col: \"city_label\"})\n",
    "\n",
    "    gdf = gpd.sjoin(gdf, cities, how=\"left\", predicate=\"within\")\n",
    "    gdf = gdf.drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# FIX: Standardize NPU column output name to 'npu'\n",
    "def attach_npu(gdf: gpd.GeoDataFrame, npu_shp: Path) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Attach NPU labels using nearest polygon, naming the output column 'npu'.\"\"\"\n",
    "    console.print(\"[cyan]Attaching NPU labels (output column: 'npu')...[/cyan]\")\n",
    "    npu_gdf = load_shapefile(npu_shp, gdf.crs)\n",
    "\n",
    "    candidate_cols = [\"NPU\", \"npu\", \"NPU_ID\", \"NPU_NUM\", \"NPU_NAME\", \"NAME\"]\n",
    "    name_col = None\n",
    "    for c in candidate_cols:\n",
    "        if c in npu_gdf.columns:\n",
    "            name_col = c\n",
    "            break\n",
    "    if name_col is None:\n",
    "        name_col = npu_gdf.columns[0]\n",
    "\n",
    "    # Rename the source NPU column to the desired final column name ('npu')\n",
    "    npu_gdf = npu_gdf[[name_col, \"geometry\"]].rename(columns={name_col: \"npu\"})\n",
    "\n",
    "    gdf = gpd.sjoin_nearest(gdf, npu_gdf, how=\"left\")\n",
    "    gdf = gdf.drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def attach_apd_zone(gdf: gpd.GeoDataFrame, zone_shp: Path) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Attach APD Zone numbers by predicate 'within'.\"\"\"\n",
    "    console.print(\"[cyan]Attaching APD zones...[/cyan]\")\n",
    "    zones = load_shapefile(zone_shp, gdf.crs)\n",
    "\n",
    "    zone_col = None\n",
    "    for c in [\"ZONE\", \"zone\", \"Zone\"]:\n",
    "        if c in zones.columns:\n",
    "            zone_col = c\n",
    "            break\n",
    "    if zone_col is None:\n",
    "        zone_col = zones.columns[0]\n",
    "\n",
    "    zones = zones[[zone_col, \"geometry\"]].rename(columns={zone_col: \"zone_raw\"})\n",
    "\n",
    "    gdf = gpd.sjoin(gdf, zones, how=\"left\", predicate=\"within\")\n",
    "    gdf = gdf.drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "\n",
    "    gdf[\"zone_int\"] = pd.to_numeric(\n",
    "        gdf[\"zone_raw\"].astype(str).str.extract(r\"(\\d+)\", expand=False),\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# FIX: Updated to use 'npu' as the grouping column\n",
    "def impute_missing_zone_from_npu(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Impute missing zone_int using the modal zone_int within its NPU.\"\"\"\n",
    "    if \"zone_int\" not in df.columns or \"npu\" not in df.columns:\n",
    "        return df\n",
    "\n",
    "    missing_before = df[\"zone_int\"].isna().sum()\n",
    "    if missing_before == 0:\n",
    "        return df\n",
    "\n",
    "    mapping = (\n",
    "        df.dropna(subset=[\"zone_int\"])\n",
    "        .groupby(\"npu\")[\"zone_int\"]\n",
    "        .apply(lambda s: s.mode().iloc[0] if not s.mode().empty else np.nan)\n",
    "    )\n",
    "\n",
    "    df[\"zone_int_cleaned\"] = df[\"zone_int\"].fillna(df[\"npu\"].map(mapping))\n",
    "    missing_after = df[\"zone_int_cleaned\"].isna().sum()\n",
    "\n",
    "    df = df.drop(columns=[\"zone_int\"]).rename(columns={\"zone_int_cleaned\": \"zone_int\"})\n",
    "    console.print(\n",
    "        f\"[green]âœ“ Imputed {missing_before - missing_after} zone_int values via NPU mode.[/green]\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_location_label(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create location_label with priority campus > neighborhood > city.\"\"\"\n",
    "    conditions = [\n",
    "        df.get(\"campus_label\").notna() if \"campus_label\" in df.columns else False,\n",
    "        df.get(\"neighborhood_label\").notna()\n",
    "        if \"neighborhood_label\" in df.columns\n",
    "        else False,\n",
    "        df.get(\"city_label\").notna() if \"city_label\" in df.columns else False,\n",
    "    ]\n",
    "    choices = [\n",
    "        df.get(\"campus_label\"),\n",
    "        df.get(\"neighborhood_label\"),\n",
    "        df.get(\"city_label\"),\n",
    "    ]\n",
    "    df[\"location_label\"] = np.select(conditions, choices, default=\"Other\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def enrich_spatial(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Spatial enrichment pipeline.\"\"\"\n",
    "    console.print(\"\\n[bold cyan]â•â•â• Spatial enrichment â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "    initial_count = len(df)\n",
    "    df_temp = df.dropna(subset=[\"longitude\", \"latitude\"]).copy()\n",
    "    dropped_count = initial_count - len(df_temp)\n",
    "    if dropped_count > 0:\n",
    "        console.print(f\"[yellow]Dropped {dropped_count} rows missing coordinates.[/yellow]\")\n",
    "\n",
    "    gdf = to_gdf(df_temp, lon_col=\"longitude\", lat_col=\"latitude\")\n",
    "\n",
    "    TARGET_PCS = \"EPSG:3857\"\n",
    "    gdf = gdf.to_crs(TARGET_PCS)\n",
    "    console.print(f\"[green]âœ“ Data projected to {TARGET_PCS} for joins.[/green]\")\n",
    "\n",
    "    # NPU is attached as 'npu'\n",
    "    gdf = attach_npu(gdf, NPU_SHP)\n",
    "    gdf = attach_apd_zone(gdf, APD_ZONE_SHP)\n",
    "    gdf = attach_campus(gdf, CAMPUS_SHP)\n",
    "    gdf = attach_neighborhood(gdf, NEIGHBORHOOD_SHP)\n",
    "    gdf = attach_city(gdf, CITIES_SHP)\n",
    "\n",
    "    df_enriched = pd.DataFrame(gdf.drop(columns=[\"geometry\", \"zone_raw\"], errors=\"ignore\"))\n",
    "    df_enriched = impute_missing_zone_from_npu(df_enriched)\n",
    "    df_enriched = build_location_label(df_enriched)\n",
    "\n",
    "    log_step(\"Step 6: Spatial enrichment (NPU, zone, campus footprints, city, neighborhood)\", df_enriched)\n",
    "    return df_enriched\n",
    "\n",
    "\n",
    "df_spatial = enrich_spatial(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96d60963",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Step </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">7</span><span style=\"color: #008000; text-decoration-color: #008000\">: Rename raw date column</span> <span style=\"color: #008080; text-decoration-color: #008080\">â†’ shape: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">265</span><span style=\"color: #008080; text-decoration-color: #008080\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">965</span><span style=\"color: #008080; text-decoration-color: #008080\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Step \u001b[0m\u001b[1;32m7\u001b[0m\u001b[32m: Rename raw date column\u001b[0m \u001b[36mâ†’ shape: \u001b[0m\u001b[1;36m265\u001b[0m\u001b[36m,\u001b[0m\u001b[1;36m965\u001b[0m\u001b[36m x \u001b[0m\u001b[1;36m25\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Renamed </span><span style=\"color: #008000; text-decoration-color: #008000\">'_raw_report_date'</span><span style=\"color: #008000; text-decoration-color: #008000\"> to </span><span style=\"color: #008000; text-decoration-color: #008000\">'raw_report_date'</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Renamed \u001b[0m\u001b[32m'_raw_report_date'\u001b[0m\u001b[32m to \u001b[0m\u001b[32m'raw_report_date'\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fix raw date column name if present\n",
    "if \"_raw_report_date\" in df_spatial.columns:\n",
    "    df_spatial = df_spatial.rename(columns={\"_raw_report_date\": \"raw_report_date\"})\n",
    "    log_step(\"Step 7: Rename raw date column\", df_spatial)\n",
    "    console.print(\"[green]âœ“ Renamed '_raw_report_date' to 'raw_report_date'.[/green]\")\n",
    "else:\n",
    "    console.print(\"[yellow]No raw date column to rename.[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c905d777",
   "metadata": {},
   "source": [
    "#### Section 5: Weather + Date/Context Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78fcde9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_holiday_flag(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str = \"report_date\",\n",
    "    country: str = \"US\",\n",
    "    subdiv: str = \"GA\",\n",
    "    years: Optional[List[int]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Add 'is_holiday' boolean column.\"\"\"\n",
    "    df = df.copy()\n",
    "    if years is None:\n",
    "        years = sorted(df[date_col].dt.year.unique().tolist())\n",
    "\n",
    "    console.print(f\"[cyan]Generating holidays for {country}-{subdiv}, years={years}[/cyan]\")\n",
    "    holiday_dates = holidays.country_holidays(country=country, subdiv=subdiv, years=years)\n",
    "    holiday_set = set(holiday_dates.keys())\n",
    "\n",
    "    df[\"_date_only\"] = df[date_col].dt.date\n",
    "    df[\"is_holiday\"] = df[\"_date_only\"].isin(holiday_set)\n",
    "    df = df.drop(columns=[\"_date_only\"])\n",
    "    console.print(\"[green]âœ“ 'is_holiday' column created.[/green]\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1be12411",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">â•â•â• Engineering date and contextual features (Standardizing to </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Bins) â•â•â•</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mâ•â•â• Engineering date and contextual features \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36mStandardizing to \u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;36m Bins\u001b[0m\u001b[1;36m)\u001b[0m\u001b[1;36m â•â•â•\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Generating holidays for US-GA, </span><span style=\"color: #008080; text-decoration-color: #008080\">years</span><span style=\"color: #008080; text-decoration-color: #008080\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2021</span><span style=\"color: #008080; text-decoration-color: #008080\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"color: #008080; text-decoration-color: #008080\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"color: #008080; text-decoration-color: #008080\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span><span style=\"color: #008080; text-decoration-color: #008080\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mGenerating holidays for US-GA, \u001b[0m\u001b[36myears\u001b[0m\u001b[36m=\u001b[0m\u001b[1;36m[\u001b[0m\u001b[1;36m2021\u001b[0m\u001b[36m, \u001b[0m\u001b[1;36m2022\u001b[0m\u001b[36m, \u001b[0m\u001b[1;36m2023\u001b[0m\u001b[36m, \u001b[0m\u001b[1;36m2024\u001b[0m\u001b[36m, \u001b[0m\u001b[1;36m2025\u001b[0m\u001b[1;36m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ </span><span style=\"color: #008000; text-decoration-color: #008000\">'is_holiday'</span><span style=\"color: #008000; text-decoration-color: #008000\"> column created.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ \u001b[0m\u001b[32m'is_holiday'\u001b[0m\u001b[32m column created.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Step </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">8</span><span style=\"color: #008000; text-decoration-color: #008000\">: Date and context features </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">Standardized to </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">6</span><span style=\"color: #008000; text-decoration-color: #008000\">-bin hour_block</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span> <span style=\"color: #008080; text-decoration-color: #008080\">â†’ shape: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">265</span><span style=\"color: #008080; text-decoration-color: #008080\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">965</span><span style=\"color: #008080; text-decoration-color: #008080\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Step \u001b[0m\u001b[1;32m8\u001b[0m\u001b[32m: Date and context features \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mStandardized to \u001b[0m\u001b[1;32m6\u001b[0m\u001b[32m-bin hour_block\u001b[0m\u001b[1;32m)\u001b[0m \u001b[36mâ†’ shape: \u001b[0m\u001b[1;36m265\u001b[0m\u001b[36m,\u001b[0m\u001b[1;36m965\u001b[0m\u001b[36m x \u001b[0m\u001b[1;36m42\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature Engineering: Date & Contextual Features\n",
    "def engineer_date_context_features(\n",
    "    df: pd.DataFrame, date_col: str = \"report_date\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Create core temporal + contextual features. Standardized to 6 x 4-hour bins.\"\"\"\n",
    "    df = df.copy()\n",
    "    console.print(\"\\n[bold cyan]â•â•â• Engineering date and contextual features (Standardizing to 6 Bins) â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    dt = df[date_col].dt\n",
    "\n",
    "    # Core temporal features\n",
    "    df[\"incident_datetime\"] = df[date_col]\n",
    "    df[\"incident_date\"] = dt.date\n",
    "    df[\"incident_hour\"] = dt.hour\n",
    "    df[\"incident_datetime_hour\"] = dt.to_period(\"h\").astype(str)\n",
    "    df[\"day_of_week\"] = dt.day_name()\n",
    "    df[\"day_number\"] = dt.weekday + 1  # Monday=1\n",
    "    df[\"year\"] = dt.year\n",
    "    df[\"day_of_year\"] = dt.dayofyear\n",
    "    df[\"month\"] = dt.month\n",
    "    df[\"week_number\"] = dt.isocalendar().week.astype(int)\n",
    "\n",
    "    df = add_holiday_flag(df, date_col=date_col)\n",
    "\n",
    "    if \"nibrs_ucr_code\" in df.columns:\n",
    "        df[\"nibrs_code\"] = df[\"nibrs_ucr_code\"]\n",
    "\n",
    "    df[\"offense_category\"] = np.select(\n",
    "        [\n",
    "            df[\"nibrs_offense\"].str.contains(\"burglary|robbery\", case=False, na=False),\n",
    "            df[\"nibrs_offense\"].str.contains(\n",
    "                \"motor vehicle theft\", case=False, na=False\n",
    "            ),\n",
    "            df[\"nibrs_offense\"].str.contains(\n",
    "                \"theft|larceny|shoplift|fraud|swindle|embezzelment|stolen property|false pretenses\",\n",
    "                case=False,\n",
    "                na=False,\n",
    "            ),\n",
    "            df[\"nibrs_offense\"].str.contains(\n",
    "                \"assault|murder|rape|battery|intimidation|extortion|kidnapping\",\n",
    "                case=False,\n",
    "                na=False,\n",
    "            ),\n",
    "        ],\n",
    "        [\n",
    "            \"Burglary/Robbery\",\n",
    "            \"Motor Vehicle Theft\",\n",
    "            \"Theft/Larceny/Fraud\",\n",
    "            \"Violent Crime\",\n",
    "        ],\n",
    "        default=\"Other/Misc.\",\n",
    "    )\n",
    "\n",
    "    month = dt.month\n",
    "    df[\"semester\"] = np.select(\n",
    "        [month.isin([8, 9, 10, 11, 12]), month.isin([1, 2, 3, 4, 5])],\n",
    "        [\"Fall\", \"Spring\"],\n",
    "        default=\"Summer\",\n",
    "    )\n",
    "\n",
    "    # STANDARDIZED TO SIX 4-HOUR BINS (0-4, 5-8, ..., 21-24)\n",
    "    df['hour'] = df['incident_datetime'].dt.hour\n",
    "    bins = [0, 5, 9, 13, 17, 21, 25]\n",
    "    labels = [\n",
    "        \"Early Night (0â€“4)\",\n",
    "        \"Early Morning (5â€“8)\",\n",
    "        \"Late Morning (9â€“12)\",\n",
    "        \"Afternoon (13â€“16)\",\n",
    "        \"Evening (17â€“20)\",\n",
    "        \"Late Night (21â€“24)\",\n",
    "    ]\n",
    "    # This is the single, consistent binning column\n",
    "    df[\"hour_block\"] = pd.cut(df[\"hour\"], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "    \n",
    "    df[\"is_weekend\"] = dt.weekday >= 5\n",
    "\n",
    "    df[\"loc_acc\"] = np.where(\n",
    "        df[\"latitude\"].isna() | df[\"longitude\"].isna(), 1, 0\n",
    "    )\n",
    "\n",
    "    log_step(\"Step 8: Date and context features (Standardized to 6-bin hour_block)\", df)\n",
    "    return df\n",
    "\n",
    "df_features = engineer_date_context_features(df_spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "571a64c3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">â•â•â• Merging initial weather data (external CSVs) â•â•â•</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mâ•â•â• Merging initial weather data \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36mexternal CSVs\u001b[0m\u001b[1;36m)\u001b[0m\u001b[1;36m â•â•â•\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62127/517359540.py:25: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  left_on=df[\"report_date_dt\"].dt.floor(\"H\"),\n",
      "/tmp/ipykernel_62127/517359540.py:26: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  right_on=hourly_df[\"datetime\"].dt.floor(\"H\"),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Initial hourly + daily weather merged </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2021</span><span style=\"color: #008000; text-decoration-color: #008000\">-</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2024</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Initial hourly + daily weather merged \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32m2021\u001b[0m\u001b[32m-\u001b[0m\u001b[1;32m2024\u001b[0m\u001b[1;32m)\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def merge_weather_data_basic(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge hourly and daily weather into APD data using the initial external CSVs.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    console.print(\"\\n[bold cyan]â•â•â• Merging initial weather data (external CSVs) â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "    try:\n",
    "        hourly_df = pd.read_csv(HOURLY_WEATHER_PATH)\n",
    "        daily_df = pd.read_csv(DAILY_WEATHER_PATH)\n",
    "    except FileNotFoundError:\n",
    "        console.print(\"[bold red]Warning:[/bold red] Initial weather CSVs not found. Skipping this merge.\")\n",
    "        return df\n",
    "\n",
    "    df[\"report_date_dt\"] = pd.to_datetime(df[\"report_date\"])\n",
    "    hourly_df[\"datetime\"] = pd.to_datetime(hourly_df[\"datetime\"])\n",
    "    daily_df[\"date\"] = pd.to_datetime(daily_df[\"date\"])\n",
    "\n",
    "    # Drop potential existing weather columns before merge\n",
    "    weather_cols_to_drop = [col for col in df.columns if any(c in col for c in hourly_df.columns if c != 'datetime') or any(c in col for c in daily_df.columns if c != 'date')]\n",
    "    df = df.drop(columns=weather_cols_to_drop, errors='ignore')\n",
    "\n",
    "    df = df.merge(\n",
    "        hourly_df,\n",
    "        left_on=df[\"report_date_dt\"].dt.floor(\"H\"),\n",
    "        right_on=hourly_df[\"datetime\"].dt.floor(\"H\"),\n",
    "        how=\"left\",\n",
    "    )\n",
    "    df = df.drop(columns=[\"key_0\"], errors=\"ignore\")\n",
    "\n",
    "    df[\"date_only\"] = df[\"report_date_dt\"].dt.date\n",
    "    daily_df[\"date_only\"] = daily_df[\"date\"].dt.date\n",
    "\n",
    "    df = df.merge(\n",
    "        daily_df,\n",
    "        on=\"date_only\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_hourly\", \"_daily\"),\n",
    "    )\n",
    "\n",
    "    df = df.drop(columns=[\"report_date_dt\", \"date_only\", \"datetime\", \"date\"], errors=\"ignore\")\n",
    "    console.print(\"[green]âœ“ Initial hourly + daily weather merged (2021-2024).[/green]\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df_features = merge_weather_data_basic(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "879c3b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">â•â•â• Deriving basic weather flags â•â•â•</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mâ•â•â• Deriving basic weather flags â•â•â•\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Step </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">9</span><span style=\"color: #008000; text-decoration-color: #008000\">: Basic weather flags</span> <span style=\"color: #008080; text-decoration-color: #008080\">â†’ shape: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">265</span><span style=\"color: #008080; text-decoration-color: #008080\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">988</span><span style=\"color: #008080; text-decoration-color: #008080\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">58</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Step \u001b[0m\u001b[1;32m9\u001b[0m\u001b[32m: Basic weather flags\u001b[0m \u001b[36mâ†’ shape: \u001b[0m\u001b[1;36m265\u001b[0m\u001b[36m,\u001b[0m\u001b[1;36m988\u001b[0m\u001b[36m x \u001b[0m\u001b[1;36m58\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Weather flags </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">hot/cold/raining</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span><span style=\"color: #008000; text-decoration-color: #008000\"> created.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Weather flags \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mhot/cold/raining\u001b[0m\u001b[1;32m)\u001b[0m\u001b[32m created.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def derive_weather_flags(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create boolean flags is_raining, is_hot, is_cold based on merged columns.\"\"\"\n",
    "    df = df.copy()\n",
    "    console.print(\"\\n[bold cyan]â•â•â• Deriving basic weather flags â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "    temp_col = \"apparent_temp_f\"\n",
    "    precip_col = \"precip_in\"\n",
    "\n",
    "    if temp_col not in df.columns or precip_col not in df.columns:\n",
    "        console.print(\n",
    "            \"[yellow]Weather columns missing or incomplete; skipping flag derivation.[/yellow]\"\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    df[\"is_raining\"] = (df[precip_col].fillna(0) > 0.01).astype(int)\n",
    "    df[\"is_hot\"] = (df[temp_col].fillna(-999) >= 90).astype(int)\n",
    "    df[\"is_cold\"] = (df[temp_col].fillna(999) <= 40).astype(int)\n",
    "\n",
    "    log_step(\"Step 9: Basic weather flags\", df)\n",
    "    console.print(\"[green]âœ“ Weather flags (hot/cold/raining) created.[/green]\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df_features = derive_weather_flags(df_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7b989",
   "metadata": {},
   "source": [
    "#### Section 6: Filter Target Offenses & Interim Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc39e1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">â•â•â• Final filtering for target offenses and interim export â•â•â•</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mâ•â•â• Final filtering for target offenses and interim export â•â•â•\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Dropped debug column </span><span style=\"color: #808000; text-decoration-color: #808000\">'raw_report_date'</span><span style=\"color: #808000; text-decoration-color: #808000\"> prior to export.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mDropped debug column \u001b[0m\u001b[33m'raw_report_date'\u001b[0m\u001b[33m prior to export.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Step </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">10</span><span style=\"color: #008000; text-decoration-color: #008000\">: Filter for target crimes</span> <span style=\"color: #008080; text-decoration-color: #008080\">â†’ shape: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">117</span><span style=\"color: #008080; text-decoration-color: #008080\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">756</span><span style=\"color: #008080; text-decoration-color: #008080\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Step \u001b[0m\u001b[1;32m10\u001b[0m\u001b[32m: Filter for target crimes\u001b[0m \u001b[36mâ†’ shape: \u001b[0m\u001b[1;36m117\u001b[0m\u001b[36m,\u001b[0m\u001b[1;36m756\u001b[0m\u001b[36m x \u001b[0m\u001b[1;36m57\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Filtered for modeling:</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">117</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">756</span> rows match target offenses.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mFiltered for modeling:\u001b[0m \u001b[1;36m117\u001b[0m,\u001b[1;36m756\u001b[0m rows match target offenses.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Interim Data Export</span> <span style=\"color: #008080; text-decoration-color: #008080\">â†’ shape: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">117</span><span style=\"color: #008080; text-decoration-color: #008080\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">756</span><span style=\"color: #008080; text-decoration-color: #008080\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Interim Data Export\u001b[0m \u001b[36mâ†’ shape: \u001b[0m\u001b[1;36m117\u001b[0m\u001b[36m,\u001b[0m\u001b[1;36m756\u001b[0m\u001b[36m x \u001b[0m\u001b[1;36m57\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">âœ“ Interim dataset exported.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mâœ“ Interim dataset exported.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Final interim dataset:</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">117</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">756</span> rows x <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span> columns\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mFinal interim dataset:\u001b[0m \u001b[1;36m117\u001b[0m,\u001b[1;36m756\u001b[0m rows x \u001b[1;36m57\u001b[0m columns\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Saved to:</span> ..<span style=\"color: #800080; text-decoration-color: #800080\">/data/interim/apd/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">apd_model_data_target_crimes.csv</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSaved to:\u001b[0m ..\u001b[35m/data/interim/apd/\u001b[0m\u001b[95mapd_model_data_target_crimes.csv\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(\"\\n[bold cyan]â•â•â• Final filtering for target offenses and interim export â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "TARGET_OFFENSES = [\n",
    "    \"larceny\",\n",
    "    \"theft\",\n",
    "    \"robbery\",\n",
    "    \"burglary\",\n",
    "    \"prowling\",\n",
    "    \"shoplifting\",\n",
    "    \"fraud\",\n",
    "    \"swindle\",\n",
    "    \"embezzelment\",\n",
    "    \"credit card\",\n",
    "    \"wire fraud\",\n",
    "    \"impersonation\",\n",
    "]\n",
    "\n",
    "if \"nibrs_offense\" not in df_features.columns:\n",
    "    raise KeyError(\"'nibrs_offense' column not found; cannot filter TARGET_OFFENSES.\")\n",
    "\n",
    "mask = df_features[\"nibrs_offense\"].str.contains(\n",
    "    \"|\".join(TARGET_OFFENSES), case=False, na=False\n",
    ")\n",
    "df_model = df_features[mask].copy()\n",
    "\n",
    "if \"raw_report_date\" in df_model.columns:\n",
    "    df_model = df_model.drop(columns=[\"raw_report_date\"])\n",
    "    console.print(\"[yellow]Dropped debug column 'raw_report_date' prior to export.[/yellow]\")\n",
    "\n",
    "log_step(\"Step 10: Filter for target crimes\", df_model)\n",
    "console.print(\n",
    "    f\"[bold yellow]Filtered for modeling:[/bold yellow] {len(df_model):,} rows match target offenses.\"\n",
    ")\n",
    "\n",
    "df_final = df_model.sort_values(\"report_date\", ascending=True, ignore_index=True)\n",
    "\n",
    "final_output = INTERIM_DATA_FOLDER / \"apd_model_data_target_crimes.csv\"\n",
    "df_final.to_csv(final_output, index=False)\n",
    "\n",
    "log_step(\"Interim Data Export\", df_final)\n",
    "console.print(\"[bold green]âœ“ Interim dataset exported.[/bold green]\")\n",
    "console.print(\n",
    "    f\"[bold]Final interim dataset:[/bold] {len(df_final):,} rows x {df_final.shape[1]} columns\"\n",
    ")\n",
    "console.print(f\"[bold]Saved to:[/bold] {final_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258df835",
   "metadata": {},
   "source": [
    "#### Section 7: Additional Spatial/Weather Repair (Load Interim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b788456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">STEP 11: Load interim data for final spatial and weather repair</span> <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> Tasks:                                                          <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> â€¢ Load interim CSV                                              <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> â€¢ Fill missing spatial data (NPU, district, neighborhood, city) <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> â€¢ Refetch and merge complete 2021â€“2025 weather                  <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> â€¢ Add campus-based features                                     <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> â€¢ Save to processed folder                                      <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m \u001b[1;36mSTEP 11: Load interim data for final spatial and weather repair\u001b[0m \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m                                                                 \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m Tasks:                                                          \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m â€¢ Load interim CSV                                              \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m â€¢ Fill missing spatial data (NPU, district, neighborhood, city) \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m â€¢ Refetch and merge complete 2021â€“2025 weather                  \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m â€¢ Add campus-based features                                     \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m â€¢ Save to processed folder                                      \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Step </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">11</span><span style=\"color: #008000; text-decoration-color: #008000\">: Loaded interim data</span> <span style=\"color: #008080; text-decoration-color: #008080\">â†’ shape: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">117</span><span style=\"color: #008080; text-decoration-color: #008080\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">756</span><span style=\"color: #008080; text-decoration-color: #008080\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Step \u001b[0m\u001b[1;32m11\u001b[0m\u001b[32m: Loaded interim data\u001b[0m \u001b[36mâ†’ shape: \u001b[0m\u001b[1;36m117\u001b[0m\u001b[36m,\u001b[0m\u001b[1;36m756\u001b[0m\u001b[36m x \u001b[0m\u001b[1;36m57\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">Missing data before spatial cleaning:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[33mMissing data before spatial cleaning:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Column             </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Missing </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">      % </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> zone_int           </span>â”‚<span style=\"color: #800000; text-decoration-color: #800000\">   1,421 </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\">   1.2% </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> neighborhood_label </span>â”‚<span style=\"color: #800000; text-decoration-color: #800000\">   7,964 </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\">   6.8% </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> campus_label       </span>â”‚<span style=\"color: #800000; text-decoration-color: #800000\"> 117,756 </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> 100.0% </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> city_label         </span>â”‚<span style=\"color: #800000; text-decoration-color: #800000\">     962 </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\">   0.8% </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mColumn            \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mMissing\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m     %\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mzone_int          \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[31m \u001b[0m\u001b[31m  1,421\u001b[0m\u001b[31m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33m  1.2%\u001b[0m\u001b[33m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mneighborhood_label\u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[31m \u001b[0m\u001b[31m  7,964\u001b[0m\u001b[31m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33m  6.8%\u001b[0m\u001b[33m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mcampus_label      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[31m \u001b[0m\u001b[31m117,756\u001b[0m\u001b[31m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33m100.0%\u001b[0m\u001b[33m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mcity_label        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[31m \u001b[0m\u001b[31m    962\u001b[0m\u001b[31m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33m  0.8%\u001b[0m\u001b[33m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(\n",
    "    Panel.fit(\n",
    "        \"[bold cyan]STEP 11: Load interim data for final spatial and weather repair[/bold cyan]\\n\\n\"\n",
    "        \"Tasks:\\n\"\n",
    "        \"â€¢ Load interim CSV\\n\"\n",
    "        \"â€¢ Fill missing spatial data (NPU, district, neighborhood, city)\\n\"\n",
    "        \"â€¢ Refetch and merge complete 2021â€“2025 weather\\n\"\n",
    "        \"â€¢ Add campus-based features\\n\"\n",
    "        \"â€¢ Save to processed folder\",\n",
    "        border_style=\"cyan\",\n",
    "    )\n",
    ")\n",
    "\n",
    "INTERIM_PATH = INTERIM_DATA_FOLDER / \"apd_model_data_target_crimes.csv\"\n",
    "df = pd.read_csv(INTERIM_PATH)\n",
    "\n",
    "log_step(\"Step 11: Loaded interim data\", df)\n",
    "\n",
    "console.print(\"\\n[yellow]Missing data before spatial cleaning:[/yellow]\")\n",
    "missing_cols_initial = [\n",
    "    \"zone_int\", \n",
    "    \"npu\",      # Standardized NPU column\n",
    "    \"neighborhood\",\n",
    "    \"neighborhood_label\",\n",
    "    \"campus_label\",\n",
    "    \"city_label\",\n",
    "]\n",
    "missing_table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "missing_table.add_column(\"Column\", style=\"cyan\")\n",
    "missing_table.add_column(\"Missing\", justify=\"right\", style=\"red\")\n",
    "missing_table.add_column(\"%\", justify=\"right\", style=\"yellow\")\n",
    "\n",
    "for col in missing_cols_initial:\n",
    "    if col in df.columns:\n",
    "        missing = df[col].isna().sum()\n",
    "        pct = (missing / len(df)) * 100\n",
    "        missing_table.add_row(col, f\"{missing:,}\", f\"{pct:.1f}%\")\n",
    "\n",
    "console.print(missing_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6af39bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">STEP 12: Fill missing spatial data via spatial joins (Stabilized)</span>                                               <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m \u001b[1;36mSTEP 12: Fill missing spatial data via spatial joins (Stabilized)\u001b[0m                                               \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â†’ Filling Zone Integer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">District</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\"> via spatial join </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">intersects</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mâ†’ Filling Zone Integer \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mDistrict\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m via spatial join \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mintersects\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Â <span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Filled </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0</span><span style=\"color: #008000; text-decoration-color: #008000\"> Zone Int values</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       " Â \u001b[32mâœ“ Filled \u001b[0m\u001b[1;32m0\u001b[0m\u001b[32m Zone Int values\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â†’ Filling City Label via spatial join </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">intersects</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mâ†’ Filling City Label via spatial join \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mintersects\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "\"value\" parameter must be a scalar, dict or Series, but you passed a \"ndarray\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_62127/3686300538.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    155\u001b[39m \n\u001b[32m    156\u001b[39m         original_nans = df[\u001b[33m'city_label'\u001b[39m].isna().sum()\n\u001b[32m    157\u001b[39m         valid_joins = joined_clean[city_col].dropna().index\n\u001b[32m    158\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m         df.loc[valid_joins, 'city_label'] = df.loc[valid_joins, 'city_label'].fillna(\n\u001b[32m    160\u001b[39m             joined_clean.loc[valid_joins, city_col].str.upper().values\n\u001b[32m    161\u001b[39m         )\n\u001b[32m    162\u001b[39m \n",
      "\u001b[32m/workspaces/campus-burglary-risk-prediction/dsci_env/lib/python3.12/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, value, method, axis, inplace, limit, downcast)\u001b[39m\n\u001b[32m   7362\u001b[39m                     value = value._values\n\u001b[32m   7363\u001b[39m                 \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m is_list_like(value):\n\u001b[32m   7364\u001b[39m                     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   7365\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m7366\u001b[39m                     raise TypeError(\n\u001b[32m   7367\u001b[39m                         \u001b[33m'\"value\" parameter must be a scalar, dict '\u001b[39m\n\u001b[32m   7368\u001b[39m                         \u001b[33m\"or Series, but you passed a \"\u001b[39m\n\u001b[32m   7369\u001b[39m                         f'\"{type(value).__name__}\"'\n",
      "\u001b[31mTypeError\u001b[39m: \"value\" parameter must be a scalar, dict or Series, but you passed a \"ndarray\""
     ]
    }
   ],
   "source": [
    "console.print(Panel(\"[bold cyan]STEP 12: Fill missing spatial data via spatial joins (Stabilized)[/bold cyan]\", border_style=\"cyan\"))\n",
    "\n",
    "\n",
    "def show_missing_comparison(df_local, cols_snapshot, step_name):\n",
    "    table = Table(\n",
    "        title=f\"{step_name} - Missing Data Comparison\",\n",
    "        show_header=True,\n",
    "        header_style=\"bold magenta\",\n",
    "    )\n",
    "    table.add_column(\"Column\", style=\"cyan\")\n",
    "    table.add_column(\"Before\", justify=\"right\", style=\"red\")\n",
    "    table.add_column(\"Before %\", justify=\"right\", style=\"yellow\")\n",
    "    table.add_column(\"After\", justify=\"right\", style=\"green\")\n",
    "    table.add_column(\"After %\", justify=\"right\", style=\"blue\")\n",
    "    table.add_column(\"Filled\", justify=\"right\", style=\"white\")\n",
    "\n",
    "    for col, (before, before_pct) in cols_snapshot.items():\n",
    "        if col in df_local.columns:\n",
    "            after = df_local[col].isna().sum()\n",
    "            after_pct = (after / len(df_local)) * 100\n",
    "            filled = before - after\n",
    "            table.add_row(\n",
    "                col,\n",
    "                f\"{before:,}\",\n",
    "                f\"{before_pct:.1f}%\",\n",
    "                f\"{after:,}\",\n",
    "                f\"{after_pct:.1f}%\",\n",
    "                f\"{filled:,}\",\n",
    "            )\n",
    "\n",
    "    console.print(table)\n",
    "\n",
    "\n",
    "spatial_cols_before: Dict[str, Any] = {}\n",
    "# FIX: Use standardized column names here\n",
    "for col in [\"zone_int\", \"npu\", \"neighborhood\", \"neighborhood_label\", \"city_label\"]:\n",
    "    if col in df.columns:\n",
    "        missing = df[col].isna().sum()\n",
    "        pct = (missing / len(df)) * 100\n",
    "        spatial_cols_before[col] = (missing, pct)\n",
    "\n",
    "# Create GeoDataFrame for ALL rows with valid coordinates\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df.dropna(subset=['longitude', 'latitude']).copy(),\n",
    "    geometry=gpd.points_from_xy(df.longitude, df.latitude), \n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "# Keep track of the indices we are working on (all non-null coordinate rows)\n",
    "gdf_indices = gdf.index.intersection(df.index)\n",
    "\n",
    "# --- Fill NPU ---\n",
    "if NPU_SHP.exists() and \"npu\" in df.columns: # Targeted column is 'npu'\n",
    "    console.print(\"\\n[cyan]â†’ Filling NPU via spatial join (intersects)...[/cyan]\")\n",
    "    gdf_npu = gpd.read_file(NPU_SHP).to_crs(\"EPSG:4326\")\n",
    "    npu_col_shp = 'NAME' if 'NAME' in gdf_npu.columns else 'NPU' # Source column in shapefile\n",
    "    npu_col = 'npu' # Target column in DataFrame\n",
    "    \n",
    "    # Identify rows that need NPU filling and have valid coordinates\n",
    "    missing_mask = df[npu_col].isna()\n",
    "    \n",
    "    if missing_mask.sum() > 0:\n",
    "        gdf_missing = gdf[gdf.index.isin(df[missing_mask].index)].copy()\n",
    "        \n",
    "        joined = gpd.sjoin(gdf_missing, gdf_npu[[npu_col_shp, 'geometry']], \n",
    "                           how=\"left\", predicate=\"intersects\").drop(columns=['index_right'])\n",
    "        \n",
    "        # Take the first match for each original index\n",
    "        joined_clean = joined.groupby(joined.index).first()\n",
    "        \n",
    "        # Safely fill the missing values in the main DataFrame 'df'\n",
    "        original_nans = df[npu_col].isna().sum()\n",
    "        \n",
    "        # Only fill if the join was successful (npu_col_shp value is not NaN)\n",
    "        valid_joins = joined_clean[npu_col_shp].dropna().index\n",
    "        \n",
    "        df.loc[valid_joins, npu_col] = df.loc[valid_joins, npu_col].fillna(\n",
    "            joined_clean.loc[valid_joins, npu_col_shp].values\n",
    "        )\n",
    "        \n",
    "        filled = original_nans - df[npu_col].isna().sum()\n",
    "        console.print(f\" Â [green]âœ“ Filled {filled:,} NPU values[/green]\")\n",
    "\n",
    "\n",
    "# --- Fill zone_int (District) ---\n",
    "if APD_ZONE_SHP.exists() and \"zone_int\" in df.columns:\n",
    "    console.print(\"\\n[cyan]â†’ Filling Zone Integer (District) via spatial join (intersects)...[/cyan]\")\n",
    "    gdf_zones = gpd.read_file(APD_ZONE_SHP).to_crs(\"EPSG:4326\")\n",
    "    zone_col = 'ZONE' \n",
    "    \n",
    "    missing_mask = df['zone_int'].isna()\n",
    "    \n",
    "    if missing_mask.sum() > 0:\n",
    "        gdf_missing = gdf[gdf.index.isin(df[missing_mask].index)].copy()\n",
    "        \n",
    "        joined = gpd.sjoin(gdf_missing, gdf_zones[[zone_col, 'geometry']], \n",
    "                           how=\"left\", predicate=\"intersects\").drop(columns=['index_right'])\n",
    "        joined_clean = joined.groupby(joined.index).first()\n",
    "        \n",
    "        original_nans = df['zone_int'].isna().sum()\n",
    "        \n",
    "        zone_int_filled = pd.to_numeric(\n",
    "            joined_clean[zone_col].astype(str).str.extract(r\"(\\d+)\", expand=False), \n",
    "            errors=\"coerce\"\n",
    "        )\n",
    "        \n",
    "        df.loc[zone_int_filled.index, 'zone_int'] = df.loc[zone_int_filled.index, 'zone_int'].fillna(zone_int_filled)\n",
    "        \n",
    "        filled = original_nans - df['zone_int'].isna().sum()\n",
    "        console.print(f\" Â [green]âœ“ Filled {filled:,} Zone Int values[/green]\")\n",
    "\n",
    "# --- Fill neighborhood ---\n",
    "if NEIGHBORHOOD_SHP.exists() and \"neighborhood\" in df.columns:\n",
    "    console.print(\"\\n[cyan]â†’ Filling Neighborhood via spatial join (intersects)...[/cyan]\")\n",
    "    gdf_neighborhoods = gpd.read_file(NEIGHBORHOOD_SHP).to_crs(\"EPSG:4326\")\n",
    "    name_col = 'NAME' \n",
    "    \n",
    "    missing_mask = df['neighborhood'].isna()\n",
    "    \n",
    "    if missing_mask.sum() > 0:\n",
    "        gdf_missing = gdf[gdf.index.isin(df[missing_mask].index)].copy()\n",
    "        \n",
    "        joined = gpd.sjoin(gdf_missing, gdf_neighborhoods[[name_col, 'geometry']], \n",
    "                           how=\"left\", predicate=\"intersects\").drop(columns=['index_right'])\n",
    "        joined_clean = joined.groupby(joined.index).first()\n",
    "        \n",
    "        original_nans = df['neighborhood'].isna().sum()\n",
    "        valid_joins = joined_clean[name_col].dropna().index\n",
    "        \n",
    "        df.loc[valid_joins, 'neighborhood'] = df.loc[valid_joins, 'neighborhood'].fillna(\n",
    "            joined_clean.loc[valid_joins, name_col].str.lower().values\n",
    "        )\n",
    "        \n",
    "        if 'neighborhood_label' in df.columns:\n",
    "            df.loc[valid_joins, 'neighborhood_label'] = df.loc[valid_joins, 'neighborhood_label'].fillna(\n",
    "                joined_clean.loc[valid_joins, name_col].str.upper().values\n",
    "            )\n",
    "        \n",
    "        filled = original_nans - df['neighborhood'].isna().sum()\n",
    "        console.print(f\" Â [green]âœ“ Filled {filled:,} Neighborhood values[/green]\")\n",
    "\n",
    "# --- Fill city_label ---\n",
    "if CITIES_SHP.exists() and \"city_label\" in df.columns:\n",
    "    console.print(\"\\n[cyan]â†’ Filling City Label via spatial join (intersects)...[/cyan]\")\n",
    "    gdf_cities = gpd.read_file(CITIES_SHP).to_crs(\"EPSG:4326\")\n",
    "    city_col = 'NAME'\n",
    "    \n",
    "    missing_mask = df['city_label'].isna()\n",
    "    \n",
    "    if missing_mask.sum() > 0:\n",
    "        gdf_missing = gdf[gdf.index.isin(df[missing_mask].index)].copy()\n",
    "        \n",
    "        joined = gpd.sjoin(gdf_missing, gdf_cities[[city_col, 'geometry']], \n",
    "                           how=\"left\", predicate=\"intersects\").drop(columns=['index_right'])\n",
    "        joined_clean = joined.groupby(joined.index).first()\n",
    "        \n",
    "        original_nans = df['city_label'].isna().sum()\n",
    "        valid_joins = joined_clean[city_col].dropna().index\n",
    "        \n",
    "        df.loc[valid_joins, 'city_label'] = df.loc[valid_joins, 'city_label'].fillna(\n",
    "            joined_clean.loc[valid_joins, city_col].str.upper().values\n",
    "        )\n",
    "        \n",
    "        filled = original_nans - df['city_label'].isna().sum()\n",
    "        console.print(f\" Â [green]âœ“ Filled {filled:,} City Label values[/green]\")\n",
    "\n",
    "\n",
    "console.print(\"\\n[green]âœ“ Spatial data filling complete.[/green]\")\n",
    "show_missing_comparison(df, spatial_cols_before, \"Step 12\")\n",
    "log_step(\"Step 12: Filled missing spatial data via spatial joins\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e7c9c0",
   "metadata": {},
   "source": [
    "#### Section 8: Campus Distance / Campus Code (Model-Friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a478a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(Panel(\"[bold cyan]STEP 13: Campus labels and distance features (Ensures campus_code is not missing)[/bold cyan]\", border_style=\"cyan\"))\n",
    "\n",
    "campus_before: Dict[str, Any] = {}\n",
    "for col in [\"campus_label\", \"campus_distance_m\", \"campus_code\"]:\n",
    "    if col in df.columns:\n",
    "        missing = df[col].isna().sum()\n",
    "        pct = (missing / len(df)) * 100\n",
    "        campus_before[col] = (missing, pct)\n",
    "\n",
    "# 1.5-mile threshold in meters\n",
    "DISTANCE_THRESHOLD_M = 2414.016\n",
    "console.print(f\"[cyan]Using 1.5-mile threshold ({DISTANCE_THRESHOLD_M:.0f} meters).[/cyan]\")\n",
    "\n",
    "# Haversine in meters\n",
    "def haversine_meters(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "\n",
    "CAMPUS_ENCODING = {\n",
    "    \"none\": 0,\n",
    "    \"GSU\": 1,\n",
    "    \"GA_Tech\": 2,\n",
    "    \"Emory\": 3,\n",
    "    \"Clark\": 4,\n",
    "    \"Spelman\": 5,\n",
    "    \"Morehouse\": 6,\n",
    "    \"Morehouse_Med\": 7,\n",
    "    \"Atlanta_Metro\": 8,\n",
    "    \"Atlanta_Tech\": 9,\n",
    "    \"SCAD\": 10,\n",
    "    \"John_Marshall\": 11,\n",
    "}\n",
    "\n",
    "console.print(\"[cyan]Calculating distance to each campus and nearest campus...[/cyan]\")\n",
    "\n",
    "# Calculate distance to nearest campus for every crime\n",
    "nearest_campuses = []\n",
    "nearest_distances = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    lat, lon = row[\"latitude\"], row[\"longitude\"]\n",
    "    \n",
    "    if pd.isna(lat) or pd.isna(lon):\n",
    "        nearest_campuses.append(\"none\")\n",
    "        nearest_distances.append(np.nan)\n",
    "        continue\n",
    "        \n",
    "    min_dist = float('inf')\n",
    "    nearest = 'none'\n",
    "    \n",
    "    for campus, (c_lat, c_lon) in SCHOOL_CENTERS.items():\n",
    "        dist = haversine_meters(lat, lon, c_lat, c_lon)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            nearest = campus\n",
    "            \n",
    "    if min_dist <= DISTISTANCE_THRESHOLD_M:\n",
    "        nearest_campuses.append(nearest)\n",
    "        nearest_distances.append(min_dist)\n",
    "    else:\n",
    "        # Distance is NaN if outside the proximity radius (expected/desired missingness)\n",
    "        nearest_campuses.append(\"none\")\n",
    "        nearest_distances.append(np.nan) \n",
    "\n",
    "df[\"campus_label\"] = nearest_campuses\n",
    "\n",
    "# campus_distance_m is NaN if outside 1.5 miles (correct for a proximity metric)\n",
    "df[\"campus_distance_m\"] = nearest_distances\n",
    "df[\"campus_distance_m\"] = df[\"campus_distance_m\"].round(4) # Round for consistency\n",
    "\n",
    "# Numeric encoding (Fix: Ensures no missing values for campus_code)\n",
    "df[\"campus_code\"] = df[\"campus_label\"].map(CAMPUS_ENCODING).fillna(0).astype(int)\n",
    "df[\"campus_distance_m\"] = df[\"campus_distance_m\"].fillna(0)\n",
    "\n",
    "# Binary flags for each campus\n",
    "binary_cols_created = []\n",
    "for campus in SCHOOL_CENTERS.keys():\n",
    "    col_name = f\"near_{campus.lower()}\"\n",
    "    df[col_name] = (df[\"campus_label\"] == campus).astype(int)\n",
    "    binary_cols_created.append(col_name)\n",
    "\n",
    "console.print(\n",
    "    f\"[green]âœ“ Campus features created: {len(binary_cols_created)} near_* columns.[/green]\"\n",
    ")\n",
    "console.print(f\"[green]âœ“ **campus_code** column is complete (non-proximal crimes = 0).[/green]\")\n",
    "\n",
    "# Distribution (including 'none')\n",
    "campus_table = Table(\n",
    "    title=\"Campus Distribution (including 'none')\",\n",
    "    show_header=True,\n",
    "    header_style=\"bold magenta\",\n",
    ")\n",
    "campus_table.add_column(\"Campus\", style=\"cyan\")\n",
    "campus_table.add_column(\"Count\", justify=\"right\", style=\"green\")\n",
    "campus_table.add_column(\"%\", justify=\"right\", style=\"yellow\")\n",
    "campus_table.add_column(\"Code\", justify=\"right\", style=\"white\")\n",
    "\n",
    "for campus, code in sorted(CAMPUS_ENCODING.items(), key=lambda x: x[1]):\n",
    "    count = (df[\"campus_label\"] == campus).sum()\n",
    "    pct = (count / len(df)) * 100\n",
    "    campus_table.add_row(campus, f\"{count:,}\", f\"{pct:.1f}%\", str(code))\n",
    "\n",
    "console.print(campus_table)\n",
    "\n",
    "show_missing_comparison(df, campus_before, \"Step 13\")\n",
    "log_step(\"Step 13: Campus distance and campus_code features\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba1831",
   "metadata": {},
   "source": [
    "#### Section 9: Full 2021â€“2025 Weather Refresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa5ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\n",
    "    Panel(\"[bold cyan]STEP 14: Fetch and merge full weather dataset (2021â€“2025) - Fixes missing 2025 data[/bold cyan]\", border_style=\"cyan\")\n",
    ")\n",
    "\n",
    "weather_before: Dict[str, Any] = {}\n",
    "weather_cols_check = [\n",
    "    \"temp_f\",\n",
    "    \"precip_in\",\n",
    "    \"rain_in\",\n",
    "    \"apparent_temp_f\",\n",
    "    \"weather_code_hourly\",\n",
    "    \"is_daylight\",\n",
    "    \"daylight_duration_sec\",\n",
    "    \"sunshine_duration_sec\",\n",
    "]\n",
    "for col in weather_cols_check:\n",
    "    # Ensure checking against the column names created in the prior merge\n",
    "    current_col = col if col in df.columns else (\n",
    "        'weather_code_hrly' if col == 'weather_code_hourly' else col\n",
    "    )\n",
    "    if current_col in df.columns:\n",
    "        missing = df[current_col].isna().sum()\n",
    "        pct = (missing / len(df)) * 100\n",
    "        weather_before[col] = (missing, pct)\n",
    "\n",
    "df[\"report_date\"] = pd.to_datetime(df[\"report_date\"])\n",
    "\n",
    "cache_session = requests_cache.CachedSession(\".cache\", expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "params = {\n",
    "    \"latitude\": 33.749,\n",
    "    \"longitude\": -84.388,\n",
    "    \"start_date\": \"2021-01-01\",\n",
    "    \"end_date\": \"2025-11-30\", # Extended to ensure all 2025 data is captured\n",
    "    \"hourly\": [\n",
    "        \"temperature_2m\",\n",
    "        \"precipitation\",\n",
    "        \"rain\",\n",
    "        \"apparent_temperature\",\n",
    "        \"weather_code\",\n",
    "        \"is_day\",\n",
    "    ],\n",
    "    \"daily\": [\n",
    "        \"sunrise\",\n",
    "        \"daylight_duration\",\n",
    "        \"sunshine_duration\",\n",
    "        \"precipitation_hours\",\n",
    "        \"rain_sum\",\n",
    "        \"temperature_2m_mean\",\n",
    "        \"weather_code\",\n",
    "    ],\n",
    "    \"timezone\": \"America/New_York\",\n",
    "    \"temperature_unit\": \"fahrenheit\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    console.print(\"[cyan]Fetching weather from Open-Meteo API...[/cyan]\")\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "    response = responses[0]\n",
    "\n",
    "    hourly = response.Hourly()\n",
    "    hourly_df = pd.DataFrame(\n",
    "        {\n",
    "            \"datetime\": pd.date_range(\n",
    "                start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "                inclusive=\"left\",\n",
    "            ),\n",
    "            \"temp_f\": hourly.Variables(0).ValuesAsNumpy(),\n",
    "            \"precip_in\": hourly.Variables(1).ValuesAsNumpy(),\n",
    "            \"rain_in\": hourly.Variables(2).ValuesAsNumpy(),\n",
    "            \"apparent_temp_f\": hourly.Variables(3).ValuesAsNumpy(),\n",
    "            \"weather_code_hourly\": hourly.Variables(4).ValuesAsNumpy(),\n",
    "            \"is_daylight\": hourly.Variables(5).ValuesAsNumpy().astype(int),\n",
    "        }\n",
    "    )\n",
    "    hourly_df[\"datetime\"] = (\n",
    "        hourly_df[\"datetime\"].dt.tz_convert(\"America/New_York\").dt.tz_localize(None)\n",
    "    )\n",
    "\n",
    "    daily = response.Daily()\n",
    "    daily_df = pd.DataFrame(\n",
    "        {\n",
    "            \"date\": pd.date_range(\n",
    "                start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=daily.Interval()),\n",
    "                inclusive=\"left\",\n",
    "            ),\n",
    "            \"sunrise\": daily.Variables(0).ValuesInt64AsNumpy(),\n",
    "            \"daylight_duration_sec\": daily.Variables(1).ValuesAsNumpy(),\n",
    "            \"sunshine_duration_sec\": daily.Variables(2).ValuesAsNumpy(),\n",
    "            \"precip_hours\": daily.Variables(3).ValuesAsNumpy(),\n",
    "            \"rain_sum_in\": daily.Variables(4).ValuesAsNumpy(),\n",
    "            \"temp_mean_f\": daily.Variables(5).ValuesAsNumpy(),\n",
    "            \"weather_code_daily\": daily.Variables(6).ValuesAsNumpy(),\n",
    "        }\n",
    "    )\n",
    "    daily_df[\"date\"] = (\n",
    "        daily_df[\"date\"]\n",
    "        .dt.tz_convert(\"America/New_York\")\n",
    "        .dt.tz_localize(None)\n",
    "        .dt.date\n",
    "    )\n",
    "\n",
    "    console.print(\n",
    "        f\"[green]âœ“ Fetched {len(hourly_df):,} hourly and {len(daily_df):,} daily weather records.[/green]\"\n",
    "    )\n",
    "    \n",
    "    # === FIX: Save the newly fetched data to the external folder ===\n",
    "    hourly_df.to_csv(HOURLY_WEATHER_PATH, index=False)\n",
    "    daily_df.to_csv(DAILY_WEATHER_PATH, index=False)\n",
    "    console.print(f\"[green]âœ“ Saved updated hourly data to: {HOURLY_WEATHER_PATH}[/green]\")\n",
    "    console.print(f\"[green]âœ“ Saved updated daily data to: {DAILY_WEATHER_PATH}[/green]\")\n",
    "    # ====================================================================\n",
    "\n",
    "    # Merge hourly weather\n",
    "    df[\"weather_datetime\"] = df[\"report_date\"].dt.floor(\"H\")\n",
    "\n",
    "    overlapping_hourly_cols = [\n",
    "        col for col in df.columns if col in hourly_df.columns and col != \"weather_datetime\"\n",
    "    ]\n",
    "    if overlapping_hourly_cols:\n",
    "        df = df.drop(columns=overlapping_hourly_cols)\n",
    "\n",
    "    df = df.merge(\n",
    "        hourly_df,\n",
    "        left_on=\"weather_datetime\",\n",
    "        right_on=\"datetime\",\n",
    "        how=\"left\",\n",
    "    ).drop(columns=[\"datetime\"])\n",
    "\n",
    "    # Merge daily weather\n",
    "    df[\"report_date_only\"] = df[\"report_date\"].dt.date\n",
    "    overlapping_daily_cols = [\n",
    "        col for col in df.columns if col in daily_df.columns and col != \"date\"\n",
    "    ]\n",
    "    if overlapping_daily_cols:\n",
    "        df = df.drop(columns=overlapping_daily_cols)\n",
    "\n",
    "    df = df.merge(\n",
    "        daily_df,\n",
    "        left_on=\"report_date_only\",\n",
    "        right_on=daily_df['date'].dt.date,\n",
    "        how=\"left\",\n",
    "    ).drop(columns=[\"date\", \"report_date_only\", \"key_0\"])\n",
    "\n",
    "    df = df.drop(columns=['weather_datetime'], errors='ignore')\n",
    "\n",
    "    console.print(\"[green]âœ“ Merged full 2021â€“2025 weather successfully.[/green]\")\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]Error fetching weather:[/bold red] {e}\")\n",
    "\n",
    "show_missing_comparison(df, weather_before, \"Step 14\")\n",
    "log_step(\"Step 14: Fetched and merged full weather dataset (2021â€“2025)\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d3fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(Panel(\"[bold cyan]STEP 15: Recalculate temperature-based flags[/bold cyan]\", border_style=\"cyan\"))\n",
    "\n",
    "if \"temp_f\" in df.columns:\n",
    "    p85 = df[\"temp_f\"].quantile(0.85)\n",
    "    p15 = df[\"temp_f\"].quantile(0.15)\n",
    "\n",
    "    console.print(\"[cyan]Temperature percentile thresholds:[/cyan]\")\n",
    "    console.print(f\"Â  85th percentile: [yellow]{p85:.1f}[/yellow] F\")\n",
    "    console.print(f\"Â  15th percentile: [yellow]{p15:.1f}[/yellow] F\")\n",
    "\n",
    "    df[\"is_hot\"] = (df[\"temp_f\"] >= p85).astype(int)\n",
    "    df[\"is_cold\"] = (df[\"temp_f\"] <= p15).astype(int)\n",
    "\n",
    "    hot_pct = (df[\"is_hot\"] == 1).sum() / len(df) * 100\n",
    "    cold_pct = (df[\"is_cold\"] == 1).sum() / len(df) * 100\n",
    "\n",
    "    console.print(\"[green]âœ“ Recalculated temperature flags:[/green]\")\n",
    "    console.print(f\"Â  is_hot (>= {p85:.1f} F): {hot_pct:.1f}%\")\n",
    "    console.print(f\"Â  is_cold (<= {p15:.1f} F): {cold_pct:.1f}%\")\n",
    "\n",
    "log_step(\"Step 15: Recalculated temperature-based flags\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747978f7",
   "metadata": {},
   "source": [
    "#### Section 10: Final Verification, Save, and Core EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e58ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(Panel(\"[bold cyan]STEP 16: Final verification and save[/bold cyan]\", border_style=\"cyan\"))\n",
    "\n",
    "# Final missing data summary\n",
    "all_cols_to_check = ['campus_distance_m', 'neighborhood', 'neighborhood_label', 'zone_int', 'npu', 'city_label', 'weather_code_hrly', 'is_daylight', 'temp_f', 'precip_in'] # Top 10 columns only\n",
    "final_missing: Dict[str, Any] = {}\n",
    "\n",
    "console.print(\"\\n[yellow]Final missing data summary (Top 10 columns only):[/yellow]\")\n",
    "final_table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "final_table.add_column(\"Column\", style=\"cyan\")\n",
    "final_table.add_column(\"Missing\", justify=\"right\", style=\"red\")\n",
    "final_table.add_column(\"%\", justify=\"right\", style=\"yellow\")\n",
    "\n",
    "for col in all_cols_to_check:\n",
    "    # Use fallback column names if needed\n",
    "    current_col = col if col in df.columns else (\n",
    "        'zone_int' if col == 'district' else (\n",
    "            'npu_label' if col == 'npu' else col\n",
    "        )\n",
    "    )\n",
    "    if current_col in df.columns:\n",
    "        missing = df[current_col].isna().sum()\n",
    "        pct = (missing / len(df)) * 100\n",
    "        final_table.add_row(col, f\"{missing:,}\", f\"{pct:.1f}%\")\n",
    "\n",
    "console.print(final_table)\n",
    "\n",
    "# Save final processed dataset\n",
    "OUTPUT_PATH = PROCESSED_DATA_FOLDER / \"target_crimes.csv\"\n",
    "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "log_step(\"Processed Data Export\", df)\n",
    "\n",
    "console.print(\n",
    "    Panel.fit(\n",
    "        \"[bold green]âœ“ CLEANING COMPLETE![/bold green]\\n\\n\"\n",
    "        f\"Saved to: [yellow]{OUTPUT_PATH}[/yellow]\\n\"\n",
    "        f\"Total records: [cyan]{len(df):,}[/cyan]\\n\"\n",
    "        f\"Total columns: [cyan]{len(df.columns)}[/cyan]\",\n",
    "        border_style=\"green\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beadef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(Panel(\"[bold magenta]STEP 17: Core EDA visualizations[/bold magenta]\", border_style=\"magenta\"))\n",
    "eda_df = df.copy()\n",
    "date_range_str = f\"{eda_df['year'].min()}â€“{eda_df['year'].max()}\"\n",
    "\n",
    "# Build GeoDataFrames for spatial EDA (needed for NPU map)\n",
    "gdf_full = gpd.GeoDataFrame(\n",
    "    eda_df.dropna(subset=['longitude', 'latitude']),\n",
    "    geometry=gpd.points_from_xy(eda_df.longitude, eda_df.latitude),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "\n",
    "# --- Quick Visuals -----------------------------------------------------------\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Crimes by hour\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x=\"incident_hour\", data=eda_df, color=\"steelblue\")\n",
    "plt.title(\"Crimes by Hour\")\n",
    "plt.xlabel(\"Hour of Day\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Crimes by day of week\n",
    "if \"day_of_week\" in eda_df.columns:\n",
    "    order = [\n",
    "        \"Monday\",\n",
    "        \"Tuesday\",\n",
    "        \"Wednesday\",\n",
    "        \"Thursday\",\n",
    "        \"Friday\",\n",
    "        \"Saturday\",\n",
    "        \"Sunday\",\n",
    "    ]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.countplot(x=\"day_of_week\", data=eda_df, order=order)\n",
    "    plt.title(\"Crimes by Day of Week\")\n",
    "    plt.xlabel(\"Day of Week\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Heatmap: day_of_week Ã— hour (KEEP)\n",
    "pivot = (\n",
    "    eda_df.pivot_table(\n",
    "        index=\"day_of_week\",\n",
    "        columns=\"incident_hour\",\n",
    "        values=\"incident_number\",\n",
    "        aggfunc=\"count\",\n",
    "    )\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "pivot = pivot.reindex(\n",
    "    [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.heatmap(pivot, cmap=\"mako\")\n",
    "plt.title(\"Heatmap: Day of Week Ã— Hour\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Day of Week\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Weather distribution (Only temp_f)\n",
    "if \"temp_f\" in eda_df.columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    eda_df[\"temp_f\"].hist(bins=30)\n",
    "    plt.title(\"Distribution of Temperature (temp_f)\")\n",
    "    plt.xlabel(r\"Temperature ($\\circ$F)\") # Added degrees symbol\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Plotly Interactive Monthly Comparison (KEEP) -----------------------------\n",
    "monthly_data = eda_df.groupby([eda_df['year'], eda_df['month']]).size().reset_index(name='count')\n",
    "monthly_data['year'] = monthly_data['year'].astype(str)\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "fig = px.line(monthly_data, x='month', y='count', color='year',\n",
    "              title='Monthly Crime Trends by Year (Interactive)',\n",
    "              labels={'month': 'Month', 'count': 'Number of Crimes', 'year': 'Year'},\n",
    "              markers=True)\n",
    "\n",
    "fig.update_xaxes(tickmode='linear', tick0=1, dtick=1,\n",
    "                 ticktext=month_names, tickvals=list(range(1, 13)))\n",
    "fig.update_layout(hovermode='x unified', height=600)\n",
    "fig.show()\n",
    "\n",
    "log_step(\"Step 17: Core EDA visualizations (including interactive Plotly)\", eda_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b6793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\n",
    "    Panel(\n",
    "        \"[bold cyan]STEP 18: Advanced temporal binning (6 x 4-hour bins)[/bold cyan]\",\n",
    "        border_style=\"cyan\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Overall time bin distribution (6-bin logic)\n",
    "time_bin_counts = (\n",
    "    eda_df[\"hour_block\"].value_counts()\n",
    "    .reindex([\"Early Night (0â€“4)\", \"Early Morning (5â€“8)\", \"Late Morning (9â€“12)\", \"Afternoon (13â€“16)\", \"Evening (17â€“20)\", \"Late Night (21â€“24)\"])\n",
    ")\n",
    "colors_bins = [\"#2C3E50\", \"#34495E\", \"#95A5A6\", \"#E67E22\", \"#E74C3C\", \"#8E44AD\"] # Consistent colors\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.bar(\n",
    "    range(len(time_bin_counts)),\n",
    "    time_bin_counts.values,\n",
    "    color=colors_bins,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "ax1.set_xticks(range(len(time_bin_counts)))\n",
    "ax1.set_xticklabels([l.split(' ')[0] for l in time_bin_counts.index], rotation=45, ha='right')\n",
    "ax1.set_title(\"Crime Distribution by Time Block (All Years)\")\n",
    "ax1.set_ylabel(\"Number of Crimes\")\n",
    "ax1.xlabel = 'Time Block'\n",
    "ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(time_bin_counts.values):\n",
    "    ax1.text(i, v + max(time_bin_counts.values) * 0.01, f\"{v:,}\", ha=\"center\", fontweight='bold')\n",
    "\n",
    "# Time bin by year\n",
    "time_bin_year = eda_df.groupby([\"year\", \"hour_block\"]).size().unstack(fill_value=0)\n",
    "time_bin_year = time_bin_year[\n",
    "    [\"Early Night (0â€“4)\", \"Early Morning (5â€“8)\", \"Late Morning (9â€“12)\", \"Afternoon (13â€“16)\", \"Evening (17â€“20)\", \"Late Night (21â€“24)\"]\n",
    "]\n",
    "\n",
    "x = np.arange(len(time_bin_year.index))\n",
    "width = 0.14\n",
    "\n",
    "for i, (bin_name, color) in enumerate(zip(time_bin_year.columns, colors_bins)):\n",
    "    ax2.bar(\n",
    "        x + i * width,\n",
    "        time_bin_year[bin_name].values,\n",
    "        width,\n",
    "        label=bin_name.split(' ')[0],\n",
    "        color=color,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "ax2.set_title(\"Crime by Time Block and Year\")\n",
    "ax2.set_xlabel(\"Year\")\n",
    "ax2.set_ylabel(\"Number of Crimes\")\n",
    "ax2.set_xticks(x + width * 2.5)\n",
    "ax2.set_xticklabels(time_bin_year.index)\n",
    "ax2.legend(title=\"Time Block\", loc=\"upper right\", fontsize=9, ncol=2)\n",
    "ax2.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Crimes per year\n",
    "crimes_per_year = eda_df[\"year\"].value_counts().sort_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "crimes_per_year.plot(kind=\"bar\", color=\"steelblue\", ax=ax, edgecolor=\"black\")\n",
    "ax.set_title(f\"Total Crimes Per Year ({date_range_str})\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Number of Crimes\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(crimes_per_year.values):\n",
    "    ax.text(i, v + max(crimes_per_year.values) * 0.01, f\"{v:,}\", ha=\"center\", fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Crimes by day of week (again, but from this EDA branch)\n",
    "day_order = [\n",
    "    \"Monday\",\n",
    "    \"Tuesday\",\n",
    "    \"Wednesday\",\n",
    "    \"Thursday\",\n",
    "    \"Friday\",\n",
    "    \"Saturday\",\n",
    "    \"Sunday\",\n",
    "]\n",
    "day_crimes = eda_df[\"day_of_week\"].value_counts().reindex(day_order)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.bar(\n",
    "    range(len(day_crimes)),\n",
    "    day_crimes.values,\n",
    "    color=\"lightblue\",\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "ax.set_title(\"Crimes by Day of Week\")\n",
    "ax.set_xlabel(\"Day of Week\")\n",
    "ax.set_ylabel(\"Number of Crimes\")\n",
    "ax.set_xticks(range(len(day_order)))\n",
    "ax.set_xticklabels(day_order, rotation=45, ha=\"right\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(day_crimes.values):\n",
    "    ax.text(i, v + max(day_crimes.values) * 0.01, f\"{v:,}\", ha=\"center\", fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "log_step(\"Step 18: Advanced temporal EDA (6-bin blocks and yearly trends)\", eda_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\n",
    "    Panel(\n",
    "        \"[bold cyan]STEP 19: Spatial EDA â€“ NPU choropleth, KDE, hexbin, GSU buffers[/bold cyan]\",\n",
    "        border_style=\"cyan\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Build GeoDataFrames for spatial EDA\n",
    "gdf_3857 = gdf_full.to_crs(epsg=3857)\n",
    "gdf_npu = gpd.read_file(NPU_SHP).to_crs(3857)\n",
    "gdf_cities = gpd.read_file(CITIES_SHP).to_crs(3857)\n",
    "atlanta_boundary = gdf_cities[gdf_cities[\"NAME\"] == \"Atlanta\"].iloc[0].geometry\n",
    "\n",
    "# Identify the shapefile column with NPU name/label\n",
    "candidate_npu_cols = [\"NPU\", \"NPU_ID\", \"NPU_NUM\", \"NPU_NAME\", \"NAME\"]\n",
    "npu_col_shp = next((c for c in candidate_npu_cols if c in gdf_npu.columns), gdf_npu.columns[0])\n",
    "npu_col = 'npu' # Use the standardized column name from the DataFrame\n",
    "\n",
    "# 19.1 NPU choropleth with bold black outlines and labels\n",
    "gdf_join_npu = gpd.sjoin(\n",
    "    gdf_3857, gdf_npu.rename(columns={npu_col_shp: 'npu_col_shp_merged'}, errors='ignore')[['npu_col_shp_merged', 'geometry']], \n",
    "    how=\"left\", predicate=\"intersects\"\n",
    ")\n",
    "# Merge crime counts based on the cleaned DataFrame column\n",
    "npu_counts = eda_df.groupby(npu_col).size().reset_index(name=\"crime_count\")\n",
    "gdf_npu_plot = gdf_npu.merge(\n",
    "    npu_counts, left_on=npu_col_shp, right_on=npu_col, how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "gdf_npu_plot.plot(\n",
    "    column=\"crime_count\",\n",
    "    cmap=\"Reds\",\n",
    "    legend=True,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1.5,\n",
    "    ax=ax,\n",
    "    legend_kwds={\"label\": \"Total crimes\", \"shrink\": 0.7},\n",
    ")\n",
    "\n",
    "for idx, row in gdf_npu_plot.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    label = row[npu_col_shp] if npu_col_shp in row else row[npu_col] # Use shapefile label first, then DataFrame label\n",
    "    count = int(row[\"crime_count\"])\n",
    "    ax.text(\n",
    "        centroid.x,\n",
    "        centroid.y,\n",
    "        f\"{label}\\n({count:,})\",\n",
    "        fontsize=9,\n",
    "        ha=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.7),\n",
    "    )\n",
    "\n",
    "ax.set_title(f\"Crime density by NPU ({date_range_str})\")\n",
    "ax.axis(\"off\")\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 19.2 KDE hotspot map\n",
    "coords = np.vstack([gdf_3857.geometry.x, gdf_3857.geometry.y]).T\n",
    "kde = KernelDensity(bandwidth=400, kernel=\"gaussian\").fit(coords)\n",
    "\n",
    "atl_bounds = atlanta_boundary.bounds\n",
    "x_margin = (atl_bounds[2] - atl_bounds[0]) * 0.1\n",
    "y_margin = (atl_bounds[3] - atl_bounds[1]) * 0.1\n",
    "\n",
    "xmin = atl_bounds[0] - x_margin\n",
    "xmax = atl_bounds[2] + x_margin\n",
    "ymin = atl_bounds[1] - y_margin\n",
    "ymax = atl_bounds[3] + y_margin\n",
    "\n",
    "xx, yy = np.mgrid[xmin:xmax:300j, ymin:ymax:300j]\n",
    "grid_points = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "z = np.exp(kde.score_samples(grid_points)).reshape(xx.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "im = ax.imshow(\n",
    "    z.T,\n",
    "    extent=[xmin, xmax, ymin, ymax],\n",
    "    origin=\"lower\",\n",
    "    cmap=\"hot\",\n",
    "    alpha=0.8,\n",
    "    vmin=z.max() * 0.3,\n",
    ")\n",
    "\n",
    "# NPU boundaries in bold black\n",
    "gdf_npu.boundary.plot(ax=ax, edgecolor=\"black\", linewidth=1.5, alpha=0.7)\n",
    "\n",
    "# Atlanta boundary in black, thicker\n",
    "gpd.GeoSeries([atlanta_boundary]).boundary.plot(\n",
    "    ax=ax, edgecolor=\"black\", linewidth=2\n",
    ")\n",
    "\n",
    "plt.colorbar(im, ax=ax, label=\"Crime density\", shrink=0.7)\n",
    "ax.set_title(\"Kernel density crime hotspot map\")\n",
    "ax.axis(\"off\")\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 19.3 Hexbin with NPU overlay (bold black)\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "hb = ax.hexbin(\n",
    "    gdf_3857.geometry.x,\n",
    "    gdf_3857.geometry.y,\n",
    "    gridsize=70,\n",
    "    cmap=\"Purples\",\n",
    "    mincnt=1,\n",
    "    extent=[xmin, xmax, ymin, ymax],\n",
    "    vmin=1,\n",
    ")\n",
    "\n",
    "gdf_npu.boundary.plot(ax=ax, edgecolor=\"black\", linewidth=1.5, alpha=0.7)\n",
    "\n",
    "gpd.GeoSeries([atlanta_boundary]).boundary.plot(\n",
    "    ax=ax, edgecolor=\"black\", linewidth=2\n",
    ")\n",
    "\n",
    "plt.colorbar(hb, ax=ax, label=\"Crime count per hexagon\", shrink=0.7)\n",
    "ax.set_title(\"Hexbin spatial density with NPU overlay\")\n",
    "ax.axis(\"off\")\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 19.4 GSU campus buffers with proper circles and crime points\n",
    "gsu_pt = Point(SCHOOL_CENTERS['GSU'][1], SCHOOL_CENTERS['GSU'][0])\n",
    "gsu_3857 = gpd.GeoSeries([gsu_pt], crs=4326).to_crs(3857).iloc[0]\n",
    "\n",
    "buffers = {\n",
    "    \"0.25 mile\": gsu_3857.buffer(402.336),\n",
    "    \"0.5 mile\": gsu_3857.buffer(804.672),\n",
    "    \"1 mile\": gsu_3857.buffer(1609.34),\n",
    "}\n",
    "\n",
    "max_buffer = buffers[\"1 mile\"]\n",
    "bounds = gpd.GeoSeries([max_buffer]).total_bounds\n",
    "buffer_margin = 800\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "\n",
    "# FIX: Buffer lines are distinct colors for visibility\n",
    "buffer_styles = [\n",
    "    (\"0.25 mile\", \"red\", 2.5),\n",
    "    (\"0.5 mile\", \"orange\", 2),\n",
    "    (\"1 mile\", \"yellow\", 1.5),\n",
    "]\n",
    "\n",
    "for name, color, lw in buffer_styles:\n",
    "    gpd.GeoSeries([buffers[name]]).plot(\n",
    "        ax=ax,\n",
    "        alpha=0,\n",
    "        edgecolor=color,\n",
    "        linewidth=lw,\n",
    "        label=f\"{name} radius\",\n",
    "        facecolor=\"none\",\n",
    "        linestyle=\"-\",\n",
    "    )\n",
    "\n",
    "crimes_in_area = gdf_3857.cx[bounds[0] : bounds[2], bounds[1] : bounds[3]]\n",
    "crimes_in_area.plot(\n",
    "    ax=ax,\n",
    "    color=\"red\",\n",
    "    markersize=2,\n",
    "    alpha=0.4,\n",
    "    label=f\"Crime incidents (n={len(crimes_in_area):,})\",\n",
    ")\n",
    "\n",
    "gpd.GeoSeries([gsu_3857]).plot(\n",
    "    ax=ax,\n",
    "    color=\"blue\",\n",
    "    markersize=60,\n",
    "    marker=\"*\",\n",
    "    zorder=5,\n",
    "    label=\"GSU campus center\",\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=1,\n",
    ")\n",
    "\n",
    "ax.set_xlim(bounds[0] - buffer_margin, bounds[2] + buffer_margin)\n",
    "ax.set_ylim(bounds[1] - buffer_margin, bounds[3] + buffer_margin)\n",
    "\n",
    "ax.set_title(\n",
    "    \"Crimes within GSU campus buffers\\nGSU center: (33.7538 N, 84.3880 W)\"\n",
    ")\n",
    "ax.legend(loc=\"upper left\", fontsize=10, framealpha=0.95)\n",
    "ax.axis(\"off\")\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "log_step(\"Step 19: Spatial EDA (NPU choropleth, KDE, hexbin, GSU buffers)\", eda_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc2d58d",
   "metadata": {},
   "source": [
    "#### Section 12: Comprehensive Crime Statistics Summary (Rich Formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17373356",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\n",
    "    Panel(\n",
    "        \"[bold cyan]STEP 20: Comprehensive crime statistics summary[/bold cyan]\",\n",
    "        border_style=\"cyan\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- Rich Summary Tables (6-BIN STANDARD) -------------------------------------\n",
    "\n",
    "# Offense breakdown (reusing eda_df from Step 17)\n",
    "if \"offense_category\" in eda_df.columns:\n",
    "    console.print(\"[cyan]Offense Category Breakdown[/cyan]\")\n",
    "    console.print(eda_df[\"offense_category\"].value_counts().to_string())\n",
    "\n",
    "summary_table = Table(\n",
    "    title=f\"COMPREHENSIVE CRIME STATISTICS SUMMARY ({date_range_str})\",\n",
    "    show_header=False,\n",
    "    header_style=\"bold magenta\",\n",
    "    show_lines=True,\n",
    ")\n",
    "summary_table.add_column(\"Metric\", style=\"cyan\")\n",
    "summary_table.add_column(\"Value\", style=\"green\")\n",
    "\n",
    "summary_table.add_row(\"[bold]Total Crimes[/bold]\", f\"{len(eda_df):,}\")\n",
    "summary_table.add_row(\n",
    "    \"[bold]Date Range[/bold]\",\n",
    "    f\"{eda_df['report_date'].min().date()} â†’ {eda_df['report_date'].max().date()}\",\n",
    ")\n",
    "\n",
    "# Temporal insights\n",
    "temporal_table = Table(\n",
    "    title=\"Temporal Insights\", show_header=True, header_style=\"bold yellow\", show_lines=True\n",
    ")\n",
    "temporal_table.add_column(\"Time Metric\", style=\"cyan\")\n",
    "temporal_table.add_column(\"Peak Value\", style=\"green\")\n",
    "temporal_table.add_column(\"Count\", justify=\"right\", style=\"magenta\")\n",
    "\n",
    "temporal_table.add_row(\n",
    "    \"Peak Hour\",\n",
    "    f\"{eda_df['incident_hour'].mode()[0]:02}:00\", # Fixed formatting to 2 digits\n",
    "    f\"{eda_df['incident_hour'].value_counts().max():,}\",\n",
    ")\n",
    "temporal_table.add_row(\n",
    "    \"Peak Day\",\n",
    "    f\"{eda_df['day_of_week'].mode()[0]}\",\n",
    "    f\"{eda_df['day_of_week'].value_counts().max():,}\",\n",
    ")\n",
    "month_names = {\n",
    "    i: name\n",
    "    for i, name in enumerate(\n",
    "        [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"],\n",
    "        1,\n",
    "    )\n",
    "}\n",
    "temporal_table.add_row(\n",
    "    \"Peak Month\",\n",
    "    f\"{month_names[eda_df['month'].mode()[0]]}\",\n",
    "    f\"{eda_df['month'].value_counts().max():,}\",\n",
    ")\n",
    "temporal_table.add_row(\n",
    "    \"Peak Year\",\n",
    "    f\"{eda_df['year'].mode()[0]}\",\n",
    "    f\"{eda_df['year'].value_counts().max():,}\",\n",
    ")\n",
    "\n",
    "# Time block analysis (6-BIN STANDARD)\n",
    "time_block_table = Table(\n",
    "    title=\"Time Block Analysis (6 x 4-Hour Bins)\",\n",
    "    show_header=True,\n",
    "    header_style=\"bold yellow\",\n",
    "    show_lines=True,\n",
    ")\n",
    "time_block_table.add_column(\"Time Block\", style=\"cyan\")\n",
    "time_block_table.add_column(\"Count\", justify=\"right\", style=\"green\")\n",
    "time_block_table.add_column(\"Percent\", justify=\"right\", style=\"magenta\")\n",
    "\n",
    "time_block_order = [\n",
    "    \"Early Night (0â€“4)\",\n",
    "    \"Early Morning (5â€“8)\",\n",
    "    \"Late Morning (9â€“12)\",\n",
    "    \"Afternoon (13â€“16)\",\n",
    "    \"Evening (17â€“20)\",\n",
    "    \"Late Night (21â€“24)\",\n",
    "]\n",
    "\n",
    "for block_name in time_block_order:\n",
    "    if \"hour_block\" in eda_df.columns:\n",
    "        count = (eda_df[\"hour_block\"] == block_name).sum()\n",
    "        pct = (count / len(eda_df)) * 100\n",
    "        time_block_table.add_row(block_name, f\"{count:,}\", f\"{pct:.1f}%\")\n",
    "\n",
    "# Day-of-week table\n",
    "day_of_week_table = Table(\n",
    "    title=\"Day of Week Analysis\",\n",
    "    show_header=True,\n",
    "    header_style=\"bold yellow\",\n",
    "    show_lines=True,\n",
    ")\n",
    "day_of_week_table.add_column(\"Day\", style=\"cyan\")\n",
    "day_of_week_table.add_column(\"Count\", justify=\"right\", style=\"green\")\n",
    "day_of_week_table.add_column(\"Percent\", justify=\"right\", style=\"magenta\")\n",
    "\n",
    "day_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "for day in day_order:\n",
    "    count = (eda_df[\"day_of_week\"] == day).sum()\n",
    "    pct = (count / len(eda_df)) * 100\n",
    "    day_of_week_table.add_row(day, f\"{count:,}\", f\"{pct:.1f}%\")\n",
    "\n",
    "rprint(summary_table)\n",
    "rprint(temporal_table)\n",
    "rprint(time_block_table)\n",
    "rprint(day_of_week_table)\n",
    "\n",
    "log_step(\"Step 20: Comprehensive crime statistics summary (Rich)\", eda_df)\n",
    "\n",
    "console.print(Panel(\"[bold green]âœ“ All pipeline steps complete![/bold green]\"))\n",
    "show_pipeline_table()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
