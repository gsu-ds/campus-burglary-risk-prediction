{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b48bb450",
   "metadata": {},
   "source": [
    "# 01 - APD Data Wrangler\n",
    "\n",
    "**Purpose:** Clean, standardize, and enrich raw APD crime data\n",
    "\n",
    "**Input:** \n",
    "- `data/raw/apd/*.csv` (raw crime reports)\n",
    "- Shapefiles (NPU, zones, campuses, neighborhoods)\n",
    "- Weather API (Open-Meteo)\n",
    "\n",
    "**Output:** \n",
    "- `data/processed/apd/target_crimes.csv` (analysis-ready dataset)\n",
    "\n",
    "--- \n",
    "**Pipeline:** <br>\n",
    "\n",
    "A. **Extract**:\n",
    "1. Ingest & standardize raw CSVs\n",
    "2. Combine & deduplicate<br>\n",
    "\n",
    "B. **Transform**:<br>\n",
    "\n",
    "3. Clean columns & parse dates<br>\n",
    "4. Spatial enrichment (NPU, zones, campuses, neighborhoods)<br>\n",
    "5. Temporal feature engineering<br>\n",
    "6. Weather enrichment<br>\n",
    "7. Campus distance calculations<br>\n",
    "\n",
    "C. **Load**<br>\n",
    "\n",
    "8. Final validation & export<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Team:** Run this notebook whenever raw data is updated\n",
    "\n",
    "**Runtime:** ~5-10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9566ac40",
   "metadata": {},
   "source": [
    "### NOTEBOOK 1: APD DATA PIPELINE - STANDARDIZE, COMBINE, FEATURE ENGINEERING & EDA\n",
    "\n",
    "Save as: `01_wrangler.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a1ff4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Import Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> Libraries imported successfully.                         <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>                                                          <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> Groups loaded:                                           <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> - Core Python: regex, math, typing, paths                <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> - Data Handling: pandas, numpy, geopandas                <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> - Visualization: matplotlib, seaborn, plotly, contextily <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> - Date &amp; Time: dateutil, holidays                        <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> - Geospatial: geopandas, shapely                         <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> - Weather / Networking: open-meteo, caching, retry       <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> - Rich Output: console, tables, panels                   <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mâ•­â”€\u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36m Import Summary \u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36mâ”€â•®\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m Libraries imported successfully.                         \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m                                                          \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m Groups loaded:                                           \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m - Core Python: regex, math, typing, paths                \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m - Data Handling: pandas, numpy, geopandas                \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m - Visualization: matplotlib, seaborn, plotly, contextily \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m - Date & Time: dateutil, holidays                        \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m - Geospatial: geopandas, shapely                         \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m - Weather / Networking: open-meteo, caching, retry       \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m - Rich Output: console, tables, panels                   \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Imports =================================================================\n",
    "# Required libraries:\n",
    "# pip install numpy pandas geopandas matplotlib seaborn contextily python-dateutil holidays scikit-learn\n",
    "# pip install shapely openmeteo-requests requests-cache retry-requests rich plotly\n",
    "\n",
    "# --- Core Python / Utilities ---\n",
    "import re\n",
    "from math import radians, sin, cos, sqrt, asin, atan2\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Any\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# --- Data Handling ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd # Added import needed for notebook consistency\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import contextily as ctx\n",
    "\n",
    "# --- Date/Time Utilities ---\n",
    "from dateutil import parser\n",
    "import holidays\n",
    "\n",
    "# --- Geospatial Processing ---\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# --- Weather API / Networking ---\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "# --- Rich Console Debugging / Output ---\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.panel import Panel\n",
    "from rich import print as rprint\n",
    "\n",
    "# --- Spatial Density ---\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Initialize console\n",
    "console = Console()\n",
    "\n",
    "console.print(\n",
    "    Panel.fit(\n",
    "        \"Libraries imported successfully.\\n\\n\"\n",
    "        \"Groups loaded:\\n\"\n",
    "        \"- Core Python: regex, math, typing, paths\\n\"\n",
    "        \"- Data Handling: pandas, numpy, geopandas\\n\"\n",
    "        \"- Visualization: matplotlib, seaborn, plotly, contextily\\n\"\n",
    "        \"- Date & Time: dateutil, holidays\\n\"\n",
    "        \"- Geospatial: geopandas, shapely\\n\"\n",
    "        \"- Weather / Networking: open-meteo, caching, retry\\n\"\n",
    "        \"- Rich Output: console, tables, panels\",\n",
    "        title=\"Import Summary\",\n",
    "        border_style=\"cyan\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d876715a",
   "metadata": {},
   "source": [
    "#### APD Crime Data Processing Pipeline\n",
    "\n",
    "1. Configuration and logger\n",
    "2. Ingest and standardize raw CSVs\n",
    "3. Combine and deduplicate\n",
    "4. Clean and basic feature engineering\n",
    "5. Geospatial enrichment\n",
    "6. Date/context features\n",
    "7. Weather enrichment\n",
    "8. Campus distance/label features\n",
    "9. Final cleaning, export, and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497c2ca8",
   "metadata": {},
   "source": [
    "#### Section 1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bdf6436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Pipeline logger configured.</span>                                                                                     <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m \u001b[1;32mPipeline logger configured.\u001b[0m                                                                                     \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pipeline Logging Setup\n",
    "\n",
    "pipeline_log: List[Dict[str, Any]] = []\n",
    "\n",
    "# FIX: Stabilized log_step for ValueError: Cannot specify ',' with 's'\n",
    "def log_step(step_name: str, df: pd.DataFrame) -> None:\n",
    "    \"\"\"Record a pipeline step with shape info, safely handling N/A.\"\"\"\n",
    "    \n",
    "    # 1. Determine values and set format type\n",
    "    if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "        rows_val = \"N/A\"\n",
    "        cols_val = \"N/A\"\n",
    "        rows_str = rows_val\n",
    "        cols_str = cols_val\n",
    "    else:\n",
    "        rows_val = int(df.shape[0])\n",
    "        cols_val = int(df.shape[1])\n",
    "        # Apply comma formatting only when values are integers\n",
    "        rows_str = f\"{rows_val:,}\"\n",
    "        cols_str = str(cols_val)\n",
    "\n",
    "    # 2. Append the original raw values to the log (int or string 'N/A')\n",
    "    pipeline_log.append({\"step\": step_name, \"rows\": rows_val, \"cols\": cols_val})\n",
    "    \n",
    "    # 3. Print the result using the safely formatted strings\n",
    "    console.print(f\"[green]âœ“ {step_name}[/green] [cyan]â†’ shape: {rows_str} x {cols_str}[/cyan]\")\n",
    "\n",
    "\n",
    "def show_pipeline_table() -> None:\n",
    "    \"\"\"Display a Rich table summarizing all pipeline steps, safely formatting N/A values.\"\"\"\n",
    "    if not pipeline_log:\n",
    "        console.print(\"[red]No pipeline steps logged yet.[/red]\")\n",
    "        return\n",
    "\n",
    "    table = Table(title=\"ğŸ“Š Data Pipeline Summary\", show_lines=True)\n",
    "    table.add_column(\"Step\", style=\"cyan\", no_wrap=True)\n",
    "    table.add_column(\"Rows\", style=\"green\")\n",
    "    table.add_column(\"Cols\", style=\"yellow\")\n",
    "\n",
    "    for entry in pipeline_log:\n",
    "        # Determine values for printing\n",
    "        rows_val = entry['rows']\n",
    "        cols_val = entry['cols']\n",
    "        \n",
    "        # Safely format: use comma only for integers, use string otherwise\n",
    "        rows_str = f\"{rows_val:,}\" if isinstance(rows_val, int) else str(rows_val)\n",
    "        cols_str = str(cols_val) if isinstance(cols_val, int) else str(cols_val)\n",
    "        \n",
    "        table.add_row(entry[\"step\"], rows_str, cols_str)\n",
    "    \n",
    "    console.print(table)\n",
    "\n",
    "\n",
    "console.print(Panel(\"[bold green]Pipeline logger configured.[/bold green]\", border_style=\"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9940f302",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Data Paths</span><span style=\"color: #008080; text-decoration-color: #008080\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Data paths configured.</span>                                                         <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>                                                                                <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> Raw Data: <span style=\"color: #808000; text-decoration-color: #808000\">../data/raw/apd</span>                                                      <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> Interim Data: <span style=\"color: #808000; text-decoration-color: #808000\">../data/interim/apd</span>                                              <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> External Data: <span style=\"color: #808000; text-decoration-color: #808000\">../data/external</span>                                                <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> Processed Data: <span style=\"color: #808000; text-decoration-color: #808000\">../data/processed/apd</span>                                          <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> Shapefiles: <span style=\"color: #808000; text-decoration-color: #808000\">../data/raw/shapefiles</span>CV Results: <span style=\"color: #808000; text-decoration-color: #808000\">../data/processed/apd/cv_results</span> <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mâ•­â”€\u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36m \u001b[0m\u001b[1;36mData Paths\u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36mâ”€â•®\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m \u001b[1;36mData paths configured.\u001b[0m                                                         \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m                                                                                \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m Raw Data: \u001b[33m../data/raw/apd\u001b[0m                                                      \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m Interim Data: \u001b[33m../data/interim/apd\u001b[0m                                              \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m External Data: \u001b[33m../data/external\u001b[0m                                                \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m Processed Data: \u001b[33m../data/processed/apd\u001b[0m                                          \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m Shapefiles: \u001b[33m../data/raw/shapefiles\u001b[0mCV Results: \u001b[33m../data/processed/apd/cv_results\u001b[0m \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Step </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000\">: Logger, paths, settings, and constants configured</span> <span style=\"color: #008080; text-decoration-color: #008080\">â†’ shape: N/A x N/A</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Step \u001b[0m\u001b[1;32m1\u001b[0m\u001b[32m: Logger, paths, settings, and constants configured\u001b[0m \u001b[36mâ†’ shape: N/A x N/A\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configure Data Paths\n",
    "\n",
    "DATA_DIR = Path(\"../data\")\n",
    "RAW_DATA_FOLDER = DATA_DIR / \"raw\" / \"apd\"\n",
    "INTERIM_DATA_FOLDER = DATA_DIR / \"interim\" / \"apd\"\n",
    "PROCESSED_DATA_FOLDER = DATA_DIR / \"processed\" / \"apd\"\n",
    "EXTERNAL_DATA_FOLDER = DATA_DIR / \"external\"\n",
    "SHAPEFILES_DIR = DATA_DIR / \"raw\" / \"shapefiles\"\n",
    "FIGURES_DIR = Path(\"../reports/figures\")\n",
    "\n",
    "\n",
    "# Create necessary folders\n",
    "EXTERNAL_DATA_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "INTERIM_DATA_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DATA_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Weather CSVs (initial run; later we fetch full 2021â€“2025)\n",
    "HOURLY_WEATHER_PATH = EXTERNAL_DATA_FOLDER / \"atlanta_hourly_weather_2024.csv\"\n",
    "DAILY_WEATHER_PATH = EXTERNAL_DATA_FOLDER / \"atlanta_daily_weather_2024.csv\"\n",
    "\n",
    "# Shapefiles\n",
    "CITIES_SHP = SHAPEFILES_DIR / \"census_boundary_2024_sf\" / \"ga_census_places_2024.shp\"\n",
    "CAMPUS_SHP = SHAPEFILES_DIR / \"area_landmark_2024_sf\" / \"ga_census_landmarks_2023.shp\"\n",
    "NEIGHBORHOOD_SHP = SHAPEFILES_DIR / \"atl_neighborhood_sf\" / \"atl_neighborhoods.shp\"\n",
    "NPU_SHP = SHAPEFILES_DIR / \"atl_npu_sf\" / \"atl_npu_boundaries.shp\"\n",
    "APD_ZONE_SHP = SHAPEFILES_DIR / \"apd_zone_2019_sf\" / \"apd_police_zones_2019.shp\"\n",
    "\n",
    "# Folders for Rolling Cross-Validation\n",
    "CV_RESULTS_DIR = PROCESSED_DATA_FOLDER / \"cv_results\"\n",
    "CV_FOLDS_DIR = CV_RESULTS_DIR / \"folds\"\n",
    "CV_PREDICTIONS_DIR = CV_RESULTS_DIR / \"predictions\"\n",
    "\n",
    "# School center coordinates for distance-based enrichment\n",
    "SCHOOL_CENTERS = {\n",
    "    \"GSU\": (33.7530, -84.3863),\n",
    "    \"GA_Tech\": (33.7756, -84.3963),\n",
    "    \"Emory\": (33.7925, -84.3239),\n",
    "    \"Clark\": (33.7533, -84.4124),\n",
    "    \"Spelman\": (33.7460, -84.4129),\n",
    "    \"Morehouse\": (33.7483, -84.4126),\n",
    "    \"Morehouse_Med\": (33.7505, -84.4131),\n",
    "    \"Atlanta_Metro\": (33.7145, -84.4020),\n",
    "    \"Atlanta_Tech\": (33.7126, -84.4034),\n",
    "    \"SCAD\": (33.7997, -84.3920),\n",
    "    \"John_Marshall\": (33.7621, -84.3896),\n",
    "}\n",
    "\n",
    "console.print(\n",
    "    Panel.fit(\n",
    "        \"[bold cyan]Data paths configured.[/bold cyan]\\n\\n\"\n",
    "        f\"Raw Data: [yellow]{RAW_DATA_FOLDER}[/yellow]\\n\"\n",
    "        f\"Interim Data: [yellow]{INTERIM_DATA_FOLDER}[/yellow]\\n\"\n",
    "        f\"External Data: [yellow]{EXTERNAL_DATA_FOLDER}[/yellow]\\n\"\n",
    "        f\"Processed Data: [yellow]{PROCESSED_DATA_FOLDER}[/yellow]\\n\"\n",
    "        f\"Shapefiles: [yellow]{SHAPEFILES_DIR}[/yellow]\"\n",
    "        f\"CV Results: [yellow]{CV_RESULTS_DIR}[/yellow]\",\n",
    "        title=\"[bold cyan]Data Paths[/bold cyan]\",\n",
    "        border_style=\"cyan\",\n",
    "    )\n",
    ")\n",
    "\n",
    "log_step(\"Step 1: Logger, paths, settings, and constants configured\", pd.DataFrame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d188774a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Fetching initial weather data</span>                                                                                   <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> Location: (33.749 N, -84.388 W)                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> Date Range: 2021-01-01 to 2024-12-31                                                                            <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m \u001b[1;36mFetching initial weather data\u001b[0m                                                                                   \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m Location: (33.749 N, -84.388 W)                                                                                 \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m Date Range: 2021-01-01 to 2024-12-31                                                                            \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Saved hourly:</span> ..<span style=\"color: #800080; text-decoration-color: #800080\">/data/external/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">atlanta_hourly_weather_2024.csv</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Saved hourly:\u001b[0m ..\u001b[35m/data/external/\u001b[0m\u001b[95matlanta_hourly_weather_2024.csv\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Saved daily:</span> ..<span style=\"color: #800080; text-decoration-color: #800080\">/data/external/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">atlanta_daily_weather_2024.csv</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Saved daily:\u001b[0m ..\u001b[35m/data/external/\u001b[0m\u001b[95matlanta_daily_weather_2024.csv\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Total rows: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">35</span><span style=\"color: #008000; text-decoration-color: #008000\">,</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">064</span><span style=\"color: #008000; text-decoration-color: #008000\"> hourly, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000\">,</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">461</span><span style=\"color: #008000; text-decoration-color: #008000\"> daily</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Total rows: \u001b[0m\u001b[1;32m35\u001b[0m\u001b[32m,\u001b[0m\u001b[1;32m064\u001b[0m\u001b[32m hourly, \u001b[0m\u001b[1;32m1\u001b[0m\u001b[32m,\u001b[0m\u001b[1;32m461\u001b[0m\u001b[32m daily\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Step </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2</span><span style=\"color: #008000; text-decoration-color: #008000\">: Initial Atlanta weather </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2021</span><span style=\"color: #008000; text-decoration-color: #008000\">â€“</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2024</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span><span style=\"color: #008000; text-decoration-color: #008000\"> saved to data/external</span> <span style=\"color: #008080; text-decoration-color: #008080\">â†’ shape: N/A x N/A</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Step \u001b[0m\u001b[1;32m2\u001b[0m\u001b[32m: Initial Atlanta weather \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32m2021\u001b[0m\u001b[32mâ€“\u001b[0m\u001b[1;32m2024\u001b[0m\u001b[1;32m)\u001b[0m\u001b[32m saved to data/external\u001b[0m \u001b[36mâ†’ shape: N/A x N/A\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initial weather fetch (pre-fetch for initial merge)\n",
    "def fetch_atlanta_weather(\n",
    "    start_date=\"2021-01-01\", end_date=\"2024-12-31\", lat=33.749, lon=-84.388\n",
    "):\n",
    "    \"\"\"Fetch weather data from Open-Meteo and save hourly/daily CSVs for reference.\"\"\"\n",
    "    console.print(\n",
    "        Panel(\n",
    "            \"[bold cyan]Fetching initial weather data[/bold cyan]\\n\"\n",
    "            f\"Location: ({lat:.3f} N, {lon:.3f} W)\\n\"\n",
    "            f\"Date Range: {start_date} to {end_date}\",\n",
    "            border_style=\"cyan\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cache_session = requests_cache.CachedSession(\".cache\", expire_after=-1)\n",
    "    retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "    openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"hourly\": [\n",
    "            \"temperature_2m\",\n",
    "            \"precipitation\",\n",
    "            \"rain\",\n",
    "            \"apparent_temperature\",\n",
    "            \"weather_code\",\n",
    "            \"is_day\",\n",
    "        ],\n",
    "        \"daily\": [\n",
    "            \"sunrise\",\n",
    "            \"daylight_duration\",\n",
    "            \"sunshine_duration\",\n",
    "            \"precipitation_hours\",\n",
    "            \"rain_sum\",\n",
    "            \"temperature_2m_mean\",\n",
    "            \"weather_code\",\n",
    "        ],\n",
    "        \"timezone\": \"America/New_York\",\n",
    "        \"temperature_unit\": \"fahrenheit\",\n",
    "    }\n",
    "\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "    response = responses[0]\n",
    "\n",
    "    # Hourly\n",
    "    hourly = response.Hourly()\n",
    "    hourly_df = pd.DataFrame(\n",
    "        {\n",
    "            \"datetime\": pd.date_range(\n",
    "                start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "                inclusive=\"left\",\n",
    "            ),\n",
    "            \"temp_f\": hourly.Variables(0).ValuesAsNumpy(),\n",
    "            \"precip_in\": hourly.Variables(1).ValuesAsNumpy(),\n",
    "            \"rain_in\": hourly.Variables(2).ValuesAsNumpy(),\n",
    "            \"apparent_temp_f\": hourly.Variables(3).ValuesAsNumpy(),\n",
    "            \"weather_code\": hourly.Variables(4).ValuesAsNumpy(),\n",
    "            \"is_daylight\": hourly.Variables(5).ValuesAsNumpy().astype(int),\n",
    "        }\n",
    "    )\n",
    "    hourly_df[\"datetime\"] = (\n",
    "        hourly_df[\"datetime\"].dt.tz_convert(\"America/New_York\").dt.tz_localize(None)\n",
    "    )\n",
    "\n",
    "    # Daily\n",
    "    daily = response.Daily()\n",
    "    daily_df = pd.DataFrame(\n",
    "        {\n",
    "            \"date\": pd.date_range(\n",
    "                start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=daily.Interval()),\n",
    "                inclusive=\"left\",\n",
    "            ),\n",
    "            \"sunrise\": daily.Variables(0).ValuesInt64AsNumpy(),\n",
    "            \"daylight_duration_sec\": daily.Variables(1).ValuesAsNumpy(),\n",
    "            \"sunshine_duration_sec\": daily.Variables(2).ValuesAsNumpy(),\n",
    "            \"precip_hours\": daily.Variables(3).ValuesAsNumpy(),\n",
    "            \"rain_sum_in\": daily.Variables(4).ValuesAsNumpy(),\n",
    "            \"temp_mean_f\": daily.Variables(5).ValuesAsNumpy(),\n",
    "            \"weather_code\": daily.Variables(6).ValuesAsNumpy(),\n",
    "        }\n",
    "    )\n",
    "    daily_df[\"date\"] = (\n",
    "        daily_df[\"date\"]\n",
    "        .dt.tz_convert(\"America/New_York\")\n",
    "        .dt.tz_localize(None)\n",
    "        .dt.date\n",
    "    )\n",
    "\n",
    "    hourly_df.to_csv(HOURLY_WEATHER_PATH, index=False)\n",
    "    daily_df.to_csv(DAILY_WEATHER_PATH, index=False)\n",
    "\n",
    "    console.print(f\"[green]âœ“ Saved hourly:[/green] {HOURLY_WEATHER_PATH}\")\n",
    "    console.print(f\"[green]âœ“ Saved daily:[/green] {DAILY_WEATHER_PATH}\")\n",
    "    console.print(\n",
    "        f\"[green]âœ“ Total rows: {len(hourly_df):,} hourly, {len(daily_df):,} daily[/green]\"\n",
    "    )\n",
    "\n",
    "    return hourly_df, daily_df\n",
    "\n",
    "\n",
    "fetch_atlanta_weather()\n",
    "log_step(\"Step 2: Initial Atlanta weather (2021â€“2024) saved to data/external\", pd.DataFrame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc453710",
   "metadata": {},
   "source": [
    "#### Section 2: Standardize & Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ad5d950",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def standardize_column_name(col: str) -> str:\n",
    "    \"\"\"Convert column name to snake_case format.\"\"\"\n",
    "    col = re.sub(r\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", col)\n",
    "    col = re.sub(r\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", col)\n",
    "    col = col.lower()\n",
    "    col = re.sub(r\"[\\s\\-\\.\\,\\(\\)\\[\\]\\{\\}]+\", \"_\", col)\n",
    "    col = re.sub(r\"[^\\w]\", \"\", col)\n",
    "    col = re.sub(r\"_+\", \"_\", col).strip(\"_\")\n",
    "    return col\n",
    "\n",
    "\n",
    "def combine_and_deduplicate(files: List[Path], dedupe_key: str) -> pd.DataFrame:\n",
    "    \"\"\"Combine multiple CSVs, standardize columns, and drop duplicates.\"\"\"\n",
    "    console.print(\"\\n[bold cyan]â•â•â• Combining data files â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "    dfs = []\n",
    "    for filepath in files:\n",
    "        console.print(f\"[cyan]Reading and standardizing:[/cyan] {filepath.name}\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        df.columns = [standardize_column_name(c) for c in df.columns]\n",
    "        dfs.append(df)\n",
    "\n",
    "    df_combined = pd.concat(dfs, ignore_index=True)\n",
    "    total_rows = len(df_combined)\n",
    "\n",
    "    # New logger step: Ingest Raw Data\n",
    "    log_step(\"Ingest Raw Data (standardized columns, pre-dedup)\", df_combined)\n",
    "\n",
    "    if dedupe_key not in df_combined.columns:\n",
    "        raise KeyError(\n",
    "            f\"dedupe_key='{dedupe_key}' not found in columns: {df_combined.columns.tolist()}\"\n",
    "        )\n",
    "\n",
    "    df_dedup = df_combined.drop_duplicates(subset=[dedupe_key])\n",
    "    duplicates = total_rows - len(df_dedup)\n",
    "\n",
    "    console.print(f\"[yellow]Combined:[/yellow] {total_rows:,} rows\")\n",
    "    console.print(f\"[red]Removed:[/red] {duplicates:,} duplicate rows\")\n",
    "\n",
    "    log_step(\"Step 3: Standardize & Combine, deduplicated by incident_number\", df_dedup)\n",
    "    return df_dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0ca8ed6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Found </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> CSV file(s).</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mFound \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;36m CSV \u001b[0m\u001b[1;36mfile\u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36ms\u001b[0m\u001b[1;36m)\u001b[0m\u001b[1;36m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">â•â•â• Combining data files â•â•â•</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mâ•â•â• Combining data files â•â•â•\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Reading and standardizing:</span> apd_2020_2024.csv\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mReading and standardizing:\u001b[0m apd_2020_2024.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Reading and standardizing:</span> apd_2023_2025.csv\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mReading and standardizing:\u001b[0m apd_2023_2025.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Reading and standardizing:</span> apd_2021_2024.csv\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mReading and standardizing:\u001b[0m apd_2021_2024.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Ingest Raw Data </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">standardized columns, pre-dedup</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span> <span style=\"color: #008080; text-decoration-color: #008080\">â†’ shape: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">372</span><span style=\"color: #008080; text-decoration-color: #008080\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">864</span><span style=\"color: #008080; text-decoration-color: #008080\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Ingest Raw Data \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mstandardized columns, pre-dedup\u001b[0m\u001b[1;32m)\u001b[0m \u001b[36mâ†’ shape: \u001b[0m\u001b[1;36m372\u001b[0m\u001b[36m,\u001b[0m\u001b[1;36m864\u001b[0m\u001b[36m x \u001b[0m\u001b[1;36m28\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Combined:</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">372</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">864</span> rows\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mCombined:\u001b[0m \u001b[1;36m372\u001b[0m,\u001b[1;36m864\u001b[0m rows\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">Removed:</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">106</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">903</span> duplicate rows\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31mRemoved:\u001b[0m \u001b[1;36m106\u001b[0m,\u001b[1;36m903\u001b[0m duplicate rows\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Step </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000\">: Standardize &amp; Combine, deduplicated by incident_number</span> <span style=\"color: #008080; text-decoration-color: #008080\">â†’ shape: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">265</span><span style=\"color: #008080; text-decoration-color: #008080\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">961</span><span style=\"color: #008080; text-decoration-color: #008080\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Step \u001b[0m\u001b[1;32m3\u001b[0m\u001b[32m: Standardize & Combine, deduplicated by incident_number\u001b[0m \u001b[36mâ†’ shape: \u001b[0m\u001b[1;36m265\u001b[0m\u001b[36m,\u001b[0m\u001b[1;36m961\u001b[0m\u001b[36m x \u001b[0m\u001b[1;36m28\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function Execution: Combine Raw Data CSVs\n",
    "\n",
    "input_files = list(RAW_DATA_FOLDER.glob(\"*.csv\"))\n",
    "\n",
    "if not input_files:\n",
    "    console.print(f\"[bold red]Warning:[/bold red] No CSV files found in '{RAW_DATA_FOLDER}'\")\n",
    "    df_combined = pd.DataFrame()\n",
    "else:\n",
    "    console.print(f\"[bold cyan]Found {len(input_files)} CSV file(s).[/bold cyan]\")\n",
    "    df_combined = combine_and_deduplicate(\n",
    "        files=input_files, dedupe_key=\"incident_number\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085ec148",
   "metadata": {},
   "source": [
    "#### Section 3: Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c60c867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">â•â•â• Cleaning APD data â•â•â•</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mâ•â•â• Cleaning APD data â•â•â•\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Dropped </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">11</span><span style=\"color: #808000; text-decoration-color: #808000\"> columns:</span> report_number, zone, fire_arm_involved, occurred_from_date, occurred_to_date, part, vic_count, \n",
       "is_bias_motivation_involved, x, y, beat_text\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mDropped \u001b[0m\u001b[1;33m11\u001b[0m\u001b[33m columns:\u001b[0m report_number, zone, fire_arm_involved, occurred_from_date, occurred_to_date, part, vic_count, \n",
       "is_bias_motivation_involved, x, y, beat_text\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">One-hot encoding:</span> <span style=\"color: #008000; text-decoration-color: #008000\">'event_watch'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mOne-hot encoding:\u001b[0m \u001b[32m'event_watch'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Created </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000\"> standardized one-hot columns</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Created \u001b[0m\u001b[1;32m3\u001b[0m\u001b[32m standardized one-hot columns\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Step </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000\">: Clean and drop selected columns</span> <span style=\"color: #008080; text-decoration-color: #008080\">â†’ shape: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">265</span><span style=\"color: #008080; text-decoration-color: #008080\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">961</span><span style=\"color: #008080; text-decoration-color: #008080\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Step \u001b[0m\u001b[1;32m4\u001b[0m\u001b[32m: Clean and drop selected columns\u001b[0m \u001b[36mâ†’ shape: \u001b[0m\u001b[1;36m265\u001b[0m\u001b[36m,\u001b[0m\u001b[1;36m961\u001b[0m\u001b[36m x \u001b[0m\u001b[1;36m19\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_apd_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Column drops, normalization, and one-hot encoding.\"\"\"\n",
    "    console.print(\"\\n[bold cyan]â•â•â• Cleaning APD data â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Define and drop unnecessary columns\n",
    "    columns_to_drop = [\n",
    "        \"report_number\",\n",
    "        \"zone\",\n",
    "        \"fire_arm_involved\",\n",
    "        \"object_id\",\n",
    "        \"occurred_from_date\",\n",
    "        \"occurred_to_date\",\n",
    "        \"part\",\n",
    "        \"vic_count\",\n",
    "        \"is_bias_motivation_involved\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        \"beat_text\",\n",
    "    ]\n",
    "\n",
    "    existing_drops = [col for col in columns_to_drop if col in df.columns]\n",
    "    if existing_drops:\n",
    "        df = df.drop(columns=existing_drops)\n",
    "        console.print(f\"[yellow]Dropped {len(existing_drops)} columns:[/yellow] {', '.join(existing_drops)}\")\n",
    "\n",
    "    # 2. Rename columns\n",
    "    if \"objectid\" in df.columns:\n",
    "        df = df.rename(columns={\"objectid\": \"object_id\"})\n",
    "    \n",
    "    # 3. One-hot encode 'event_watch'\n",
    "    if \"event_watch\" in df.columns:\n",
    "        console.print(\"[cyan]One-hot encoding:[/cyan] 'event_watch'\")\n",
    "        one_hot = pd.get_dummies(df[\"event_watch\"], prefix=\"event_watch\", dummy_na=False)\n",
    "        one_hot.columns = [standardize_column_name(col) for col in one_hot.columns]\n",
    "        df = pd.concat([df.drop(columns=[\"event_watch\"]), one_hot], axis=1)\n",
    "        console.print(f\"[green]âœ“ Created {len(one_hot.columns)} standardized one-hot columns[/green]\")\n",
    "\n",
    "    # 4. Convert text to lowercase\n",
    "    text_columns = [\"location_type\", \"street_address\", \"nibrs_offense\"]\n",
    "    for col in text_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.lower()\n",
    "\n",
    "    log_step(\"Step 4: Clean and drop selected columns\", df)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_clean = clean_apd_data(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1dc0ea",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">â•â•â• Standardizing </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'report_date'</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> column with robust parser â•â•â•</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mâ•â•â• Standardizing \u001b[0m\u001b[1;36m'report_date'\u001b[0m\u001b[1;36m column with robust parser â•â•â•\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Date Standardization\n",
    "\n",
    "console.print(\"\\n[bold cyan]â•â•â• Standardizing 'report_date' column with robust parser â•â•â•[/bold cyan]\\n\")\n",
    "total_rows = len(df_clean)\n",
    "\n",
    "# Preserve raw values for comparison\n",
    "df_clean[\"_raw_report_date\"] = df_clean[\"report_date\"].astype(str).str.strip()\n",
    "\n",
    "\n",
    "def parse_report_date(x):\n",
    "    \"\"\"Safely parse mixed APD/NIBRS date formats.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return pd.NaT\n",
    "\n",
    "    x = str(x).strip()\n",
    "\n",
    "    # Explicit patterns: MM/DD/YYYY HH:MM:SS AM/PM, etc.\n",
    "    if re.match(r\"^\\d{1,2}/\\d{1,2}/\\d{4} \\d{1,2}:\\d{2}:\\d{2} [APMapm]{2}$\", x):\n",
    "        return pd.to_datetime(x, format=\"%m/%d/%Y %I:%M:%S %p\", errors=\"coerce\")\n",
    "    if re.match(r\"^\\d{1,2}/\\d{1,2}/\\d{4} \\d{1,2}:\\d{2} [APMapm]{2}$\", x):\n",
    "        return pd.to_datetime(x, format=\"%m/%d/%Y %I:%M %p\", errors=\"coerce\")\n",
    "    if re.match(r\"^\\d{1,2}/\\d{1,2}/\\d{4} \\d{1,2}:\\d{2}$\", x):\n",
    "        return pd.to_datetime(x, format=\"%m/%d/%Y %H:%M\", errors=\"coerce\")\n",
    "\n",
    "    # Fallback\n",
    "    try:\n",
    "        return parser.parse(x, fuzzy=True)\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "\n",
    "df_clean[\"report_date\"] = df_clean[\"_raw_report_date\"].apply(parse_report_date)\n",
    "invalid = df_clean[\"report_date\"].isna().sum()\n",
    "parsed = total_rows - invalid\n",
    "\n",
    "console.print(f\"[cyan]Total rows:[/cyan] {total_rows:,}\")\n",
    "console.print(f\"[green]Successfully standardized:[/green] {parsed:,}\")\n",
    "console.print(f\"[yellow]Unrecoverable dates dropped:[/yellow] {invalid:,}\")\n",
    "\n",
    "df_clean = df_clean.dropna(subset=[\"report_date\"]).copy()\n",
    "df_clean[\"report_date\"] = pd.to_datetime(df_clean[\"report_date\"])\n",
    "\n",
    "log_step(\"Step 5: Robust date standardization\", df_clean)\n",
    "\n",
    "console.print(\"\\n[bold blue]Examples of corrected formats:[/bold blue]\")\n",
    "changed = df_clean[\n",
    "    df_clean[\"_raw_report_date\"] != df_clean[\"report_date\"].astype(str)\n",
    "]\n",
    "if len(changed) > 0:\n",
    "    console.print(\n",
    "        changed.sample(min(5, len(changed)))[[\"_raw_report_date\", \"report_date\"]]\n",
    "    )\n",
    "else:\n",
    "    console.print(\"[yellow]All dates already standardized.[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58db5f10",
   "metadata": {},
   "source": [
    "#### Section 4: Geospatial Enrichment (NPU, Zone, Campus Footprints, City, Neighborhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd09ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geospatial Helper Functions (NPU STANDARDIZED TO 'npu')\n",
    "\n",
    "def haversine_distance(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"Great-circle distance between two points in miles (R=3956).\"\"\"\n",
    "    R = 3956\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "\n",
    "    return c * R\n",
    "\n",
    "\n",
    "def to_gdf(df: pd.DataFrame, lon_col: str = \"longitude\", lat_col: str = \"latitude\"):\n",
    "    \"\"\"Convert a DataFrame with lon/lat into a GeoDataFrame in EPSG:4326.\"\"\"\n",
    "    for c in (lon_col, lat_col):\n",
    "        if c not in df.columns:\n",
    "            raise KeyError(f\"Expected coordinate column '{c}' not found.\")\n",
    "    return gpd.GeoDataFrame(\n",
    "        df,\n",
    "        geometry=gpd.points_from_xy(df[lon_col], df[lat_col]),\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "\n",
    "\n",
    "def load_shapefile(path: Path, target_crs) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Load shapefile and ensure CRS alignment.\"\"\"\n",
    "    try:\n",
    "        gdf = gpd.read_file(path)\n",
    "    except Exception:\n",
    "        console.print(f\"[bold red]FATAL: Could not load shapefile:[/bold red] {path}\")\n",
    "        raise\n",
    "\n",
    "    if gdf.crs is None:\n",
    "        gdf = gdf.set_crs(\"EPSG:4326\")\n",
    "    return gdf.to_crs(target_crs)\n",
    "\n",
    "\n",
    "def attach_campus(gdf: gpd.GeoDataFrame, campus_shp: Path) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Attach closest campus footprint label where available (MTFCC S1400).\"\"\"\n",
    "    console.print(\"[cyan]Attaching campus footprint labels...[/cyan]\")\n",
    "    campus = load_shapefile(campus_shp, gdf.crs)\n",
    "    if \"MTFCC\" in campus.columns:\n",
    "        campus = campus[campus[\"MTFCC\"] == \"S1400\"]\n",
    "    name_col = \"FULLNAME\" if \"FULLNAME\" in campus.columns else campus.columns[0]\n",
    "    campus = campus[[name_col, \"geometry\"]].rename(columns={name_col: \"campus_label\"})\n",
    "\n",
    "    gdf = gpd.sjoin_nearest(gdf, campus, how=\"left\")\n",
    "    gdf = gdf.drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def attach_neighborhood(\n",
    "    gdf: gpd.GeoDataFrame, nhood_shp: Path\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"Attach neighborhood labels (within polygon).\"\"\"\n",
    "    console.print(\"[cyan]Attaching neighborhood labels...[/cyan]\")\n",
    "    nhoods = load_shapefile(nhood_shp, gdf.crs)\n",
    "    name_col = \"NAME\" if \"NAME\" in nhoods.columns else nhoods.columns[0]\n",
    "    nhoods = nhoods[[name_col, \"geometry\"]].rename(\n",
    "        columns={name_col: \"neighborhood_label\"}\n",
    "    )\n",
    "\n",
    "    gdf = gpd.sjoin(gdf, nhoods, how=\"left\", predicate=\"within\")\n",
    "    gdf = gdf.drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def attach_city(gdf: gpd.GeoDataFrame, cities_shp: Path) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Attach city labels.\"\"\"\n",
    "    console.print(\"[cyan]Attaching city labels...[/cyan]\")\n",
    "    cities = load_shapefile(cities_shp, gdf.crs)\n",
    "    name_col = \"NAME\" if \"NAME\" in cities.columns else cities.columns[0]\n",
    "    cities = cities[[name_col, \"geometry\"]].rename(columns={name_col: \"city_label\"})\n",
    "\n",
    "    gdf = gpd.sjoin(gdf, cities, how=\"left\", predicate=\"within\")\n",
    "    gdf = gdf.drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# FIX: Standardize NPU column output name to 'npu' and handle duplicates\n",
    "def attach_npu(gdf: gpd.GeoDataFrame, npu_shp: Path) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Attach NPU labels using nearest polygon, naming the output column 'npu'.\"\"\"\n",
    "    console.print(\"[cyan]Attaching NPU labels (output column: 'npu')...[/cyan]\")\n",
    "    npu_gdf = load_shapefile(npu_shp, gdf.crs)\n",
    "\n",
    "    candidate_cols = [\"NPU\", \"npu\", \"NPU_ID\", \"NPU_NUM\", \"NPU_NAME\", \"NAME\"]\n",
    "    name_col = None\n",
    "    for c in candidate_cols:\n",
    "        if c in npu_gdf.columns:\n",
    "            name_col = c\n",
    "            break\n",
    "    if name_col is None:\n",
    "        name_col = npu_gdf.columns[0]\n",
    "\n",
    "    # Rename the source NPU column to a temporary name to avoid conflicts\n",
    "    npu_gdf = npu_gdf[[name_col, \"geometry\"]].rename(columns={name_col: \"npu_from_shp\"})\n",
    "\n",
    "    # Perform spatial join\n",
    "    gdf = gpd.sjoin_nearest(gdf, npu_gdf, how=\"left\")\n",
    "    \n",
    "    # Clean up: rename to final 'npu' and drop unnecessary columns\n",
    "    if \"npu_from_shp\" in gdf.columns:\n",
    "        gdf[\"npu\"] = gdf[\"npu_from_shp\"]\n",
    "        gdf = gdf.drop(columns=[\"npu_from_shp\"], errors=\"ignore\")\n",
    "    \n",
    "    # Drop spatial join artifacts\n",
    "    gdf = gdf.drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "    \n",
    "    # Fill missing NPU with 0 (for unmatched coordinates)\n",
    "    if \"npu\" in gdf.columns:\n",
    "        gdf[\"npu\"] = gdf[\"npu\"].fillna(0)\n",
    "        unmatched = (gdf[\"npu\"] == 0).sum()\n",
    "        if unmatched > 0:\n",
    "            console.print(f\"[yellow]  âš  {unmatched:,} records could not be matched to NPU (set to 0)[/yellow]\")\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "def attach_apd_zone(gdf: gpd.GeoDataFrame, zone_shp: Path) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Attach APD Zone numbers by predicate 'within'.\"\"\"\n",
    "    console.print(\"[cyan]Attaching APD zones...[/cyan]\")\n",
    "    zones = load_shapefile(zone_shp, gdf.crs)\n",
    "\n",
    "    zone_col = None\n",
    "    for c in [\"ZONE\", \"zone\", \"Zone\"]:\n",
    "        if c in zones.columns:\n",
    "            zone_col = c\n",
    "            break\n",
    "    if zone_col is None:\n",
    "        zone_col = zones.columns[0]\n",
    "\n",
    "    zones = zones[[zone_col, \"geometry\"]].rename(columns={zone_col: \"zone_raw\"})\n",
    "\n",
    "    gdf = gpd.sjoin(gdf, zones, how=\"left\", predicate=\"within\")\n",
    "    gdf = gdf.drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "\n",
    "    gdf[\"zone_int\"] = pd.to_numeric(\n",
    "        gdf[\"zone_raw\"].astype(str).str.extract(r\"(\\d+)\", expand=False),\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# FIX: Updated to use 'npu' as the grouping column\n",
    "def impute_missing_zone_from_npu(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Impute missing zone_int using the modal zone_int within its NPU.\"\"\"\n",
    "    if \"zone_int\" not in df.columns or \"npu\" not in df.columns:\n",
    "        return df\n",
    "\n",
    "    missing_before = df[\"zone_int\"].isna().sum()\n",
    "    if missing_before == 0:\n",
    "        return df\n",
    "\n",
    "    mapping = (\n",
    "        df.dropna(subset=[\"zone_int\"])\n",
    "        .groupby(\"npu\")[\"zone_int\"]\n",
    "        .apply(lambda s: s.mode().iloc[0] if not s.mode().empty else np.nan)\n",
    "    )\n",
    "\n",
    "    df[\"zone_int_cleaned\"] = df[\"zone_int\"].fillna(df[\"npu\"].map(mapping))\n",
    "    missing_after = df[\"zone_int_cleaned\"].isna().sum()\n",
    "\n",
    "    df = df.drop(columns=[\"zone_int\"]).rename(columns={\"zone_int_cleaned\": \"zone_int\"})\n",
    "    console.print(\n",
    "        f\"[green]âœ“ Imputed {missing_before - missing_after} zone_int values via NPU mode.[/green]\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_location_label(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create location_label with priority campus > neighborhood > city.\"\"\"\n",
    "    conditions = [\n",
    "        df.get(\"campus_label\").notna() if \"campus_label\" in df.columns else False,\n",
    "        df.get(\"neighborhood_label\").notna()\n",
    "        if \"neighborhood_label\" in df.columns\n",
    "        else False,\n",
    "        df.get(\"city_label\").notna() if \"city_label\" in df.columns else False,\n",
    "    ]\n",
    "    choices = [\n",
    "        df.get(\"campus_label\"),\n",
    "        df.get(\"neighborhood_label\"),\n",
    "        df.get(\"city_label\"),\n",
    "    ]\n",
    "    df[\"location_label\"] = np.select(conditions, choices, default=\"Other\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def enrich_spatial(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Spatial enrichment pipeline.\"\"\"\n",
    "    console.print(\"\\n[bold cyan]â•â•â• Spatial enrichment â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "    initial_count = len(df)\n",
    "    df_temp = df.dropna(subset=[\"longitude\", \"latitude\"]).copy()\n",
    "    dropped_count = initial_count - len(df_temp)\n",
    "    if dropped_count > 0:\n",
    "        console.print(f\"[yellow]Dropped {dropped_count} rows missing coordinates.[/yellow]\")\n",
    "\n",
    "    gdf = to_gdf(df_temp, lon_col=\"longitude\", lat_col=\"latitude\")\n",
    "\n",
    "    TARGET_PCS = \"EPSG:3857\"\n",
    "    gdf = gdf.to_crs(TARGET_PCS)\n",
    "    console.print(f\"[green]âœ“ Data projected to {TARGET_PCS} for joins.[/green]\")\n",
    "\n",
    "    # NPU is attached as 'npu'\n",
    "    gdf = attach_npu(gdf, NPU_SHP)\n",
    "    gdf = attach_apd_zone(gdf, APD_ZONE_SHP)\n",
    "    gdf = attach_campus(gdf, CAMPUS_SHP)\n",
    "    gdf = attach_neighborhood(gdf, NEIGHBORHOOD_SHP)\n",
    "    gdf = attach_city(gdf, CITIES_SHP)\n",
    "\n",
    "    df_enriched = pd.DataFrame(gdf.drop(columns=[\"geometry\", \"zone_raw\"], errors=\"ignore\"))\n",
    "    df_enriched = impute_missing_zone_from_npu(df_enriched)\n",
    "    df_enriched = build_location_label(df_enriched)\n",
    "\n",
    "    log_step(\"Step 6: Spatial enrichment (NPU, zone, campus footprints, city, neighborhood)\", df_enriched)\n",
    "    return df_enriched\n",
    "\n",
    "\n",
    "df_spatial = enrich_spatial(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e9ac8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Fix raw date column name if present\n",
    "if \"_raw_report_date\" in df_spatial.columns:\n",
    "    df_spatial = df_spatial.rename(columns={\"_raw_report_date\": \"raw_report_date\"})\n",
    "    log_step(\"Step 7: Rename raw date column\", df_spatial)\n",
    "    console.print(\"[green]âœ“ Renamed '_raw_report_date' to 'raw_report_date'.[/green]\")\n",
    "else:\n",
    "    console.print(\"[yellow]No raw date column to rename.[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965aff9c",
   "metadata": {},
   "source": [
    "#### Section 5: Weather + Date/Context Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5b8286",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def add_holiday_flag(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str = \"report_date\",\n",
    "    country: str = \"US\",\n",
    "    subdiv: str = \"GA\",\n",
    "    years: Optional[List[int]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Add 'is_holiday' boolean column.\"\"\"\n",
    "    df = df.copy()\n",
    "    if years is None:\n",
    "        years = sorted(df[date_col].dt.year.unique().tolist())\n",
    "\n",
    "    console.print(f\"[cyan]Generating holidays for {country}-{subdiv}, years={years}[/cyan]\")\n",
    "    holiday_dates = holidays.country_holidays(country=country, subdiv=subdiv, years=years)\n",
    "    holiday_set = set(holiday_dates.keys())\n",
    "\n",
    "    df[\"_date_only\"] = df[date_col].dt.date\n",
    "    df[\"is_holiday\"] = df[\"_date_only\"].isin(holiday_set)\n",
    "    df = df.drop(columns=[\"_date_only\"])\n",
    "    console.print(\"[green]âœ“ 'is_holiday' column created.[/green]\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4644dc5e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Feature Engineering: Date & Contextual Features\n",
    "def engineer_date_context_features(\n",
    "    df: pd.DataFrame, date_col: str = \"report_date\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Create core temporal + contextual features. Standardized to 6 x 4-hour bins.\"\"\"\n",
    "    df = df.copy()\n",
    "    console.print(\"\\n[bold cyan]â•â•â• Engineering date and contextual features (Standardizing to 6 Bins) â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    dt = df[date_col].dt\n",
    "\n",
    "    # Core temporal features\n",
    "    df[\"incident_datetime\"] = df[date_col]\n",
    "    df[\"incident_date\"] = dt.date\n",
    "    df[\"incident_hour\"] = dt.hour\n",
    "    df[\"incident_datetime_hour\"] = dt.to_period(\"h\").astype(str)\n",
    "    df[\"day_of_week\"] = dt.day_name()\n",
    "    df[\"day_number\"] = dt.weekday + 1  # Monday=1\n",
    "    df[\"year\"] = dt.year\n",
    "    df[\"day_of_year\"] = dt.dayofyear\n",
    "    df[\"month\"] = dt.month\n",
    "    df[\"week_number\"] = dt.isocalendar().week.astype(int)\n",
    "\n",
    "    df = add_holiday_flag(df, date_col=date_col)\n",
    "\n",
    "    if \"nibrs_ucr_code\" in df.columns:\n",
    "        df[\"nibrs_code\"] = df[\"nibrs_ucr_code\"]\n",
    "\n",
    "    df[\"offense_category\"] = np.select(\n",
    "        [\n",
    "            df[\"nibrs_offense\"].str.contains(\"burglary|robbery\", case=False, na=False),\n",
    "            df[\"nibrs_offense\"].str.contains(\n",
    "                \"motor vehicle theft\", case=False, na=False\n",
    "            ),\n",
    "            df[\"nibrs_offense\"].str.contains(\n",
    "                \"theft|larceny|shoplift|fraud|swindle|embezzelment|stolen property|false pretenses\",\n",
    "                case=False,\n",
    "                na=False,\n",
    "            ),\n",
    "            df[\"nibrs_offense\"].str.contains(\n",
    "                \"assault|murder|rape|battery|intimidation|extortion|kidnapping\",\n",
    "                case=False,\n",
    "                na=False,\n",
    "            ),\n",
    "        ],\n",
    "        [\n",
    "            \"Burglary/Robbery\",\n",
    "            \"Motor Vehicle Theft\",\n",
    "            \"Theft/Larceny/Fraud\",\n",
    "            \"Violent Crime\",\n",
    "        ],\n",
    "        default=\"Other/Misc.\",\n",
    "    )\n",
    "\n",
    "    month = dt.month\n",
    "    df[\"semester\"] = np.select(\n",
    "        [month.isin([8, 9, 10, 11, 12]), month.isin([1, 2, 3, 4, 5])],\n",
    "        [\"Fall\", \"Spring\"],\n",
    "        default=\"Summer\",\n",
    "    )\n",
    "\n",
    "    # STANDARDIZED TO SIX 4-HOUR BINS (0-4, 5-8, ..., 21-24)\n",
    "    df['hour'] = df['incident_datetime'].dt.hour\n",
    "    bins = [0, 5, 9, 13, 17, 21, 25]\n",
    "    labels = [\n",
    "        \"Early Night (0â€“4)\",\n",
    "        \"Early Morning (5â€“8)\",\n",
    "        \"Late Morning (9â€“12)\",\n",
    "        \"Afternoon (13â€“16)\",\n",
    "        \"Evening (17â€“20)\",\n",
    "        \"Late Night (21â€“24)\",\n",
    "    ]\n",
    "    # This is the single, consistent binning column\n",
    "    df[\"hour_block\"] = pd.cut(df[\"hour\"], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "    \n",
    "    df[\"is_weekend\"] = dt.weekday >= 5\n",
    "\n",
    "    df[\"loc_acc\"] = np.where(\n",
    "        df[\"latitude\"].isna() | df[\"longitude\"].isna(), 1, 0\n",
    "    )\n",
    "\n",
    "    log_step(\"Step 8: Date and context features (Standardized to 6-bin hour_block)\", df)\n",
    "    return df\n",
    "\n",
    "df_features = engineer_date_context_features(df_spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1635405",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def merge_weather_data_basic(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Only merge if weather columns don't exist yet.\"\"\"\n",
    "    if 'temp_f' in df.columns and df['temp_f'].notna().any():\n",
    "        console.print(\"[yellow]Weather data already present, skipping initial merge.[/yellow]\")\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    console.print(\"\\n[bold cyan]â•â•â• Merging initial weather data (external CSVs) â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "    try:\n",
    "        hourly_df = pd.read_csv(HOURLY_WEATHER_PATH)\n",
    "        daily_df = pd.read_csv(DAILY_WEATHER_PATH)\n",
    "    except FileNotFoundError:\n",
    "        console.print(\"[bold red]Warning:[/bold red] Initial weather CSVs not found. Skipping this merge.\")\n",
    "        return df\n",
    "\n",
    "    df[\"report_date_dt\"] = pd.to_datetime(df[\"report_date\"])\n",
    "    hourly_df[\"datetime\"] = pd.to_datetime(hourly_df[\"datetime\"])\n",
    "    daily_df[\"date\"] = pd.to_datetime(daily_df[\"date\"])\n",
    "\n",
    "    # Drop potential existing weather columns before merge\n",
    "    weather_cols_to_drop = [col for col in df.columns if any(c in col for c in hourly_df.columns if c != 'datetime') or any(c in col for c in daily_df.columns if c != 'date')]\n",
    "    df = df.drop(columns=weather_cols_to_drop, errors='ignore')\n",
    "\n",
    "    df = df.merge(\n",
    "        hourly_df,\n",
    "        left_on=df[\"report_date_dt\"].dt.floor(\"H\"),\n",
    "        right_on=hourly_df[\"datetime\"].dt.floor(\"H\"),\n",
    "        how=\"left\",\n",
    "    )\n",
    "    df = df.drop(columns=[\"key_0\"], errors=\"ignore\")\n",
    "\n",
    "    df[\"date_only\"] = df[\"report_date_dt\"].dt.date\n",
    "    daily_df[\"date_only\"] = daily_df[\"date\"].dt.date\n",
    "\n",
    "    df = df.merge(\n",
    "        daily_df,\n",
    "        on=\"date_only\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_hourly\", \"_daily\"),\n",
    "    )\n",
    "\n",
    "    df = df.drop(columns=[\"report_date_dt\", \"date_only\", \"datetime\", \"date\"], errors=\"ignore\")\n",
    "    console.print(\"[green]âœ“ Initial hourly + daily weather merged (2021-2024).[/green]\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df_features = merge_weather_data_basic(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e0e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_weather_flags(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create boolean flags is_raining, is_hot, is_cold based on merged columns.\"\"\"\n",
    "    df = df.copy()\n",
    "    console.print(\"\\n[bold cyan]â•â•â• Deriving basic weather flags â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "    temp_col = \"apparent_temp_f\"\n",
    "    precip_col = \"precip_in\"\n",
    "\n",
    "    if temp_col not in df.columns or precip_col not in df.columns:\n",
    "        console.print(\n",
    "            \"[yellow]Weather columns missing or incomplete; skipping flag derivation.[/yellow]\"\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    df[\"is_raining\"] = (df[precip_col].fillna(0) > 0.01).astype(int)\n",
    "    df[\"is_hot\"] = (df[temp_col].fillna(-999) >= 90).astype(int)\n",
    "    df[\"is_cold\"] = (df[temp_col].fillna(999) <= 40).astype(int)\n",
    "\n",
    "    log_step(\"Step 9: Basic weather flags\", df)\n",
    "    console.print(\"[green]âœ“ Weather flags (hot/cold/raining) created.[/green]\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df_features = derive_weather_flags(df_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c50fee",
   "metadata": {},
   "source": [
    "#### Section 6: Filter Target Offenses & Interim Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1aa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\"\\n[bold cyan]â•â•â• Final filtering for target offenses and interim export â•â•â•[/bold cyan]\\n\")\n",
    "\n",
    "TARGET_OFFENSES = [\n",
    "    \"larceny\",\n",
    "    \"theft\",\n",
    "    \"robbery\",\n",
    "    \"burglary\",\n",
    "    \"prowling\",\n",
    "    \"shoplifting\",\n",
    "    \"fraud\",\n",
    "    \"swindle\",\n",
    "    \"embezzelment\",\n",
    "    \"credit card\",\n",
    "    \"wire fraud\",\n",
    "    \"impersonation\",\n",
    "]\n",
    "\n",
    "if \"nibrs_offense\" not in df_features.columns:\n",
    "    raise KeyError(\"'nibrs_offense' column not found; cannot filter TARGET_OFFENSES.\")\n",
    "\n",
    "mask = df_features[\"nibrs_offense\"].str.contains(\n",
    "    \"|\".join(TARGET_OFFENSES), case=False, na=False\n",
    ")\n",
    "df_model = df_features[mask].copy()\n",
    "\n",
    "if \"raw_report_date\" in df_model.columns:\n",
    "    df_model = df_model.drop(columns=[\"raw_report_date\"])\n",
    "    console.print(\"[yellow]Dropped debug column 'raw_report_date' prior to export.[/yellow]\")\n",
    "\n",
    "log_step(\"Step 10: Filter for target crimes\", df_model)\n",
    "console.print(\n",
    "    f\"[bold yellow]Filtered for modeling:[/bold yellow] {len(df_model):,} rows match target offenses.\"\n",
    ")\n",
    "\n",
    "#Step 10b: Parquet Export for Interim Data\n",
    "console.print(Panel(\"[bold yellow]STEP 10b: Export Interim Data to Parquet[/bold yellow]\", border_style=\"yellow\"))\n",
    "\n",
    "PARQUET_INTERIM_PATH = INTERIM_DATA_FOLDER / \"apd_model_data_target_crimes.parquet\"\n",
    "\n",
    "try:\n",
    "    df_final.to_parquet(PARQUET_INTERIM_PATH, index=False, compression='snappy')\n",
    "    \n",
    "    log_step(\"Interim Data Export (Parquet)\", df_final)\n",
    "\n",
    "    console.print(\n",
    "        Panel.fit(\n",
    "            f\"[bold yellow]âœ“ INTERIM PARQUET SAVED![/bold yellow]\\n\\n\"\n",
    "            f\"Saved to: [green]{PARQUET_INTERIM_PATH}[/green]\",\n",
    "            border_style=\"yellow\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]ERROR saving Parquet:[/bold red] {e}\\n[yellow]Check if 'pyarrow' is installed.[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43489e56",
   "metadata": {},
   "source": [
    "#### Section 7: Additional Spatial/Weather Repair (Load Interim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cc1f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\n",
    "    Panel.fit(\n",
    "        \"[bold cyan]STEP 11: Load interim data and repair spatial features[/bold cyan]\\n\\n\"\n",
    "        \"Tasks:\\n\"\n",
    "        \"â€¢ Load interim CSV\\n\"\n",
    "        \"â€¢ Show baseline missing data\\n\"\n",
    "        \"â€¢ Fill missing spatial data via spatial joins\\n\"\n",
    "        \"â€¢ Show before/after comparison\",\n",
    "        border_style=\"cyan\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load interim data\n",
    "INTERIM_PATH = INTERIM_DATA_FOLDER / \"apd_model_data_target_crimes.csv\"\n",
    "df = pd.read_csv(INTERIM_PATH)\n",
    "log_step(\"Step 11a: Loaded interim data\", df)\n",
    "\n",
    "# Show baseline missing data\n",
    "console.print(\"\\n[yellow]Missing data BEFORE spatial repair:[/yellow]\")\n",
    "missing_cols_initial = [\n",
    "    \"zone_int\", \n",
    "    \"npu\",      \n",
    "    \"neighborhood\",\n",
    "    \"neighborhood_label\",\n",
    "    \"campus_label\",\n",
    "    \"city_label\",\n",
    "]\n",
    "missing_table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "missing_table.add_column(\"Column\", style=\"cyan\")\n",
    "missing_table.add_column(\"Missing\", justify=\"right\", style=\"red\")\n",
    "missing_table.add_column(\"%\", justify=\"right\", style=\"yellow\")\n",
    "\n",
    "for col in missing_cols_initial:\n",
    "    if col in df.columns:\n",
    "        missing = df[col].isna().sum()\n",
    "        pct = (missing / len(df)) * 100\n",
    "        missing_table.add_row(col, f\"{missing:,}\", f\"{pct:.1f}%\")\n",
    "\n",
    "console.print(missing_table)\n",
    "\n",
    "spatial_cols_before: Dict[str, Any] = {}\n",
    "for col in [\"zone_int\", \"npu\", \"neighborhood\", \"neighborhood_label\", \"city_label\"]:\n",
    "    if col in df.columns:\n",
    "        missing = df[col].isna().sum()\n",
    "        pct = (missing / len(df)) * 100\n",
    "        spatial_cols_before[col] = (missing, pct)\n",
    "\n",
    "console.print(\"\\n[cyan]â†’ Beginning spatial joins to fill missing values...[/cyan]\")\n",
    "\n",
    "# Create GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df.dropna(subset=['longitude', 'latitude']).copy(),\n",
    "    geometry=gpd.points_from_xy(df.longitude, df.latitude), \n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232cb105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "console.print(Panel(\"[bold cyan]STEP 12: Fill missing spatial data via spatial joins (Stabilized)[/bold cyan]\", border_style=\"cyan\"))\n",
    "\n",
    "\n",
    "def show_missing_comparison(df_local, cols_snapshot, step_name):\n",
    "    table = Table(\n",
    "        title=f\"{step_name} - Missing Data Comparison\",\n",
    "        show_header=True,\n",
    "        header_style=\"bold magenta\",\n",
    "    )\n",
    "    table.add_column(\"Column\", style=\"cyan\")\n",
    "    table.add_column(\"Before\", justify=\"right\", style=\"red\")\n",
    "    table.add_column(\"Before %\", justify=\"right\", style=\"yellow\")\n",
    "    table.add_column(\"After\", justify=\"right\", style=\"green\")\n",
    "    table.add_column(\"After %\", justify=\"right\", style=\"blue\")\n",
    "    table.add_column(\"Filled\", justify=\"right\", style=\"white\")\n",
    "\n",
    "    for col, (before, before_pct) in cols_snapshot.items():\n",
    "        if col in df_local.columns:\n",
    "            after = df_local[col].isna().sum()\n",
    "            after_pct = (after / len(df_local)) * 100\n",
    "            filled = before - after\n",
    "            table.add_row(\n",
    "                col,\n",
    "                f\"{before:,}\",\n",
    "                f\"{before_pct:.1f}%\",\n",
    "                f\"{after:,}\",\n",
    "                f\"{after_pct:.1f}%\",\n",
    "                f\"{filled:,}\",\n",
    "            )\n",
    "\n",
    "    console.print(table)\n",
    "\n",
    "\n",
    "spatial_cols_before: Dict[str, Any] = {}\n",
    "for col in [\"zone_int\", \"npu\", \"neighborhood\", \"neighborhood_label\", \"city_label\"]:\n",
    "    if col in df.columns:\n",
    "        missing = df[col].isna().sum()\n",
    "        pct = (missing / len(df)) * 100\n",
    "        spatial_cols_before[col] = (missing, pct)\n",
    "\n",
    "# Create GeoDataFrame for ALL rows with valid coordinates\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df.dropna(subset=['longitude', 'latitude']).copy(),\n",
    "    geometry=gpd.points_from_xy(df.longitude, df.latitude), \n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "gdf_indices = gdf.index.intersection(df.index)\n",
    "\n",
    "# --- Fill NPU ---\n",
    "if NPU_SHP.exists() and \"npu\" in df.columns:\n",
    "    console.print(\"\\n[cyan]â†’ Filling NPU via spatial join (intersects)...[/cyan]\")\n",
    "    gdf_npu = gpd.read_file(NPU_SHP).to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Find NPU column in shapefile\n",
    "    candidate_npu_cols = [\"NPU\", \"NPU_ID\", \"NPU_NUM\", \"NPU_NAME\", \"NAME\"]\n",
    "    npu_col_shp = next((c for c in candidate_npu_cols if c in gdf_npu.columns), gdf_npu.columns[0])\n",
    "    \n",
    "    # Only repair rows where npu is missing or 0\n",
    "    missing_mask = df[\"npu\"].isna() | (df[\"npu\"] == 0)\n",
    "    \n",
    "    if missing_mask.sum() > 0:\n",
    "        gdf_missing = gdf[gdf.index.isin(df[missing_mask].index)].copy()\n",
    "        \n",
    "        # Spatial join\n",
    "        joined = gpd.sjoin(\n",
    "            gdf_missing, \n",
    "            gdf_npu[[npu_col_shp, 'geometry']], \n",
    "            how=\"left\", \n",
    "            predicate=\"intersects\"\n",
    "        ).drop(columns=['index_right'], errors='ignore')\n",
    "        \n",
    "        joined_clean = joined.groupby(joined.index).first()\n",
    "        \n",
    "        original_nans = missing_mask.sum()\n",
    "        valid_joins = joined_clean[npu_col_shp].dropna().index\n",
    "        \n",
    "        # Fill NPU values (loop to avoid pandas .values issue)\n",
    "        for idx in valid_joins:\n",
    "            if idx in df.index and (pd.isna(df.loc[idx, \"npu\"]) or df.loc[idx, \"npu\"] == 0):\n",
    "                df.loc[idx, \"npu\"] = joined_clean.loc[idx, npu_col_shp]\n",
    "        \n",
    "        # Still missing after spatial join? Set to 0\n",
    "        df[\"npu\"] = df[\"npu\"].fillna(0)\n",
    "        \n",
    "        filled = original_nans - ((df[\"npu\"].isna() | (df[\"npu\"] == 0)).sum())\n",
    "        remaining_zero = (df[\"npu\"] == 0).sum()\n",
    "        \n",
    "        console.print(f\"  [green]âœ“ Filled {filled:,} NPU values[/green]\")\n",
    "        if remaining_zero > 0:\n",
    "            console.print(f\"  [yellow]âš  {remaining_zero:,} records remain unmatched (NPU = 0)[/yellow]\")\n",
    "\n",
    "# --- Fill zone_int (District) ---\n",
    "if APD_ZONE_SHP.exists() and \"zone_int\" in df.columns:\n",
    "    console.print(\"\\n[cyan]â†’ Filling Zone Integer (District) via spatial join (intersects)...[/cyan]\")\n",
    "    gdf_zones = gpd.read_file(APD_ZONE_SHP).to_crs(\"EPSG:4326\")\n",
    "    zone_col = 'ZONE' \n",
    "    \n",
    "    missing_mask = df['zone_int'].isna()\n",
    "    \n",
    "    if missing_mask.sum() > 0:\n",
    "        gdf_missing = gdf[gdf.index.isin(df[missing_mask].index)].copy()\n",
    "        \n",
    "        joined = gpd.sjoin(gdf_missing, gdf_zones[[zone_col, 'geometry']], \n",
    "                           how=\"left\", predicate=\"intersects\").drop(columns=['index_right'], errors='ignore')\n",
    "        joined_clean = joined.groupby(joined.index).first()\n",
    "        \n",
    "        original_nans = df['zone_int'].isna().sum()\n",
    "        \n",
    "        zone_int_filled = pd.to_numeric(\n",
    "            joined_clean[zone_col].astype(str).str.extract(r\"(\\d+)\", expand=False), \n",
    "            errors=\"coerce\"\n",
    "        )\n",
    "        \n",
    "        # FIX: Direct assignment without .fillna()\n",
    "        for idx in zone_int_filled.dropna().index:\n",
    "            if pd.isna(df.loc[idx, 'zone_int']):\n",
    "                df.loc[idx, 'zone_int'] = zone_int_filled.loc[idx]\n",
    "        \n",
    "        filled = original_nans - df['zone_int'].isna().sum()\n",
    "        console.print(f\"  [green]âœ“ Filled {filled:,} Zone Int values[/green]\")\n",
    "\n",
    "# --- Fill neighborhood ---\n",
    "if NEIGHBORHOOD_SHP.exists() and \"neighborhood\" in df.columns:\n",
    "    console.print(\"\\n[cyan]â†’ Filling Neighborhood via spatial join (intersects)...[/cyan]\")\n",
    "    gdf_neighborhoods = gpd.read_file(NEIGHBORHOOD_SHP).to_crs(\"EPSG:4326\")\n",
    "    name_col = 'NAME' \n",
    "    \n",
    "    missing_mask = df['neighborhood'].isna()\n",
    "    \n",
    "    if missing_mask.sum() > 0:\n",
    "        gdf_missing = gdf[gdf.index.isin(df[missing_mask].index)].copy()\n",
    "        \n",
    "        joined = gpd.sjoin(gdf_missing, gdf_neighborhoods[[name_col, 'geometry']], \n",
    "                           how=\"left\", predicate=\"intersects\").drop(columns=['index_right'], errors='ignore')\n",
    "        joined_clean = joined.groupby(joined.index).first()\n",
    "        \n",
    "        original_nans = df['neighborhood'].isna().sum()\n",
    "        valid_joins = joined_clean[name_col].dropna().index\n",
    "        \n",
    "        # FIX: Loop assignment\n",
    "        for idx in valid_joins:\n",
    "            if pd.isna(df.loc[idx, 'neighborhood']):\n",
    "                df.loc[idx, 'neighborhood'] = joined_clean.loc[idx, name_col].lower()\n",
    "            \n",
    "            if 'neighborhood_label' in df.columns and pd.isna(df.loc[idx, 'neighborhood_label']):\n",
    "                df.loc[idx, 'neighborhood_label'] = joined_clean.loc[idx, name_col].upper()\n",
    "        \n",
    "        filled = original_nans - df['neighborhood'].isna().sum()\n",
    "        console.print(f\"  [green]âœ“ Filled {filled:,} Neighborhood values[/green]\")\n",
    "\n",
    "# --- Fill city_label ---\n",
    "if CITIES_SHP.exists() and \"city_label\" in df.columns:\n",
    "    console.print(\"\\n[cyan]â†’ Filling City Label via spatial join (intersects)...[/cyan]\")\n",
    "    gdf_cities = gpd.read_file(CITIES_SHP).to_crs(\"EPSG:4326\")\n",
    "    city_col = 'NAME'\n",
    "    \n",
    "    missing_mask = df['city_label'].isna()\n",
    "    \n",
    "    if missing_mask.sum() > 0:\n",
    "        gdf_missing = gdf[gdf.index.isin(df[missing_mask].index)].copy()\n",
    "        \n",
    "        joined = gpd.sjoin(gdf_missing, gdf_cities[[city_col, 'geometry']], \n",
    "                           how=\"left\", predicate=\"intersects\").drop(columns=['index_right'], errors='ignore')\n",
    "        joined_clean = joined.groupby(joined.index).first()\n",
    "        \n",
    "        original_nans = df['city_label'].isna().sum()\n",
    "        valid_joins = joined_clean[city_col].dropna().index\n",
    "        \n",
    "        # FIX: Loop assignment without .values\n",
    "        for idx in valid_joins:\n",
    "            if pd.isna(df.loc[idx, 'city_label']):\n",
    "                df.loc[idx, 'city_label'] = joined_clean.loc[idx, city_col].upper()\n",
    "        \n",
    "        filled = original_nans - df['city_label'].isna().sum()\n",
    "        console.print(f\"  [green]âœ“ Filled {filled:,} City Label values[/green]\")\n",
    "\n",
    "\n",
    "console.print(\"\\n[green]âœ“ Spatial data repair complete.[/green]\")\n",
    "show_missing_comparison(df, spatial_cols_before, \"Step 11: Spatial Repair\")\n",
    "log_step(\"Step 12a: Spatial data repaired via joins\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074c9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(Panel(\"[bold cyan]STEP 12b: Clean up duplicate spatial columns[/bold cyan]\", border_style=\"cyan\"))\n",
    "\n",
    "# Identify and remove duplicate columns from spatial joins\n",
    "duplicate_suffixes = [\"_left\", \"_right\", \"_x\", \"_y\"]\n",
    "columns_to_drop = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if any(col.endswith(suffix) for suffix in duplicate_suffixes):\n",
    "        columns_to_drop.append(col)\n",
    "\n",
    "if columns_to_drop:\n",
    "    console.print(f\"[yellow]Found {len(columns_to_drop)} duplicate columns to remove:[/yellow]\")\n",
    "    for col in columns_to_drop:\n",
    "        console.print(f\"  â€¢ {col}\")\n",
    "    \n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    console.print(f\"[green]âœ“ Removed {len(columns_to_drop)} duplicate columns[/green]\")\n",
    "\n",
    "# Ensure 'npu' column exists and is clean\n",
    "if \"npu\" not in df.columns:\n",
    "    console.print(\"[red]ERROR: 'npu' column not found after spatial enrichment![/red]\")\n",
    "else:\n",
    "    # Fill any remaining NaN with 0\n",
    "    npu_missing_before = df[\"npu\"].isna().sum()\n",
    "    df[\"npu\"] = df[\"npu\"].fillna(0)\n",
    "    npu_missing_after = df[\"npu\"].isna().sum()\n",
    "    \n",
    "    console.print(f\"[green]âœ“ NPU column verified:[/green]\")\n",
    "    console.print(f\"  â€¢ Missing before cleanup: {npu_missing_before:,}\")\n",
    "    console.print(f\"  â€¢ Missing after cleanup: {npu_missing_after:,}\")\n",
    "    console.print(f\"  â€¢ Unique NPU values: {df['npu'].nunique()}\")\n",
    "\n",
    "log_step(\"Step 12b: Cleaned up duplicate spatial columns\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2881b384",
   "metadata": {},
   "source": [
    "#### Section 8: Campus Distance / Campus Code (Model-Friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(Panel(\"[bold cyan]STEP 13: Campus labels and distance features (Ensures campus_code is not missing)[/bold cyan]\", border_style=\"cyan\"))\n",
    "\n",
    "campus_before: Dict[str, Any] = {}\n",
    "for col in [\"campus_label\", \"campus_distance_m\", \"campus_code\"]:\n",
    "    if col in df.columns:\n",
    "        missing = df[col].isna().sum()\n",
    "        pct = (missing / len(df)) * 100\n",
    "        campus_before[col] = (missing, pct)\n",
    "\n",
    "# 1.5-mile threshold in meters\n",
    "DISTANCE_THRESHOLD_M = 2414.016\n",
    "console.print(f\"[cyan]Using 1.5-mile threshold ({DISTANCE_THRESHOLD_M:.0f} meters).[/cyan]\")\n",
    "\n",
    "# Haversine in meters (kept for reference, but not used in vectorized version)\n",
    "def haversine_meters(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "\n",
    "CAMPUS_ENCODING = {\n",
    "    \"none\": 0,\n",
    "    \"GSU\": 1,\n",
    "    \"GA_Tech\": 2,\n",
    "    \"Emory\": 3,\n",
    "    \"Clark\": 4,\n",
    "    \"Spelman\": 5,\n",
    "    \"Morehouse\": 6,\n",
    "    \"Morehouse_Med\": 7,\n",
    "    \"Atlanta_Metro\": 8,\n",
    "    \"Atlanta_Tech\": 9,\n",
    "    \"SCAD\": 10,\n",
    "    \"John_Marshall\": 11,\n",
    "}\n",
    "\n",
    "console.print(\"[cyan]Calculating distance to each campus and nearest campus (vectorized)...[/cyan]\")\n",
    "\n",
    "def calculate_nearest_campus_fully_vectorized(df):\n",
    "    \"\"\"Fully vectorized campus distance calculation using broadcasting.\"\"\"\n",
    "    # Filter valid coordinates\n",
    "    valid_mask = df['latitude'].notna() & df['longitude'].notna()\n",
    "    valid_df = df[valid_mask].copy()\n",
    "    \n",
    "    # Prepare results arrays\n",
    "    nearest_campus = pd.Series('none', index=df.index)\n",
    "    nearest_distance = pd.Series(np.nan, index=df.index)\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        return nearest_campus, nearest_distance\n",
    "    \n",
    "    # Convert campus centers to arrays\n",
    "    campus_names = list(SCHOOL_CENTERS.keys())\n",
    "    campus_lats = np.array([lat for lat, lon in SCHOOL_CENTERS.values()])\n",
    "    campus_lons = np.array([lon for lat, lon in SCHOOL_CENTERS.values()])\n",
    "    \n",
    "    # Calculate distances to all campuses at once (broadcasting)\n",
    "    crime_lats = valid_df['latitude'].values[:, np.newaxis]  # Shape: (n_crimes, 1)\n",
    "    crime_lons = valid_df['longitude'].values[:, np.newaxis]\n",
    "    \n",
    "    # Haversine formula vectorized\n",
    "    R = 6371000  # Earth radius in meters\n",
    "    lat1, lon1 = np.radians(crime_lats), np.radians(crime_lons)\n",
    "    lat2, lon2 = np.radians(campus_lats), np.radians(campus_lons)\n",
    "    \n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    distances = R * c  # Shape: (n_crimes, n_campuses)\n",
    "    \n",
    "    # Find nearest campus for each crime\n",
    "    min_dist_idx = distances.argmin(axis=1)\n",
    "    min_distances = distances[np.arange(len(distances)), min_dist_idx]\n",
    "    \n",
    "    # Apply threshold\n",
    "    within_threshold = min_distances <= DISTANCE_THRESHOLD_M\n",
    "    \n",
    "    nearest_campus.loc[valid_df.index[within_threshold]] = [\n",
    "        campus_names[idx] for idx in min_dist_idx[within_threshold]\n",
    "    ]\n",
    "    nearest_distance.loc[valid_df.index[within_threshold]] = min_distances[within_threshold]\n",
    "    \n",
    "    return nearest_campus, nearest_distance\n",
    "\n",
    "# Calculate campus labels and distances\n",
    "df['campus_label'], df['campus_distance_m'] = calculate_nearest_campus_fully_vectorized(df)\n",
    "df['campus_distance_m'] = df['campus_distance_m'].round(4)\n",
    "\n",
    "# âœ… ADD THESE MISSING LINES:\n",
    "# Numeric encoding (ensures no missing values for campus_code)\n",
    "df[\"campus_code\"] = df[\"campus_label\"].map(CAMPUS_ENCODING).fillna(0).astype(int)\n",
    "df[\"campus_distance_m\"] = df[\"campus_distance_m\"].fillna(0)\n",
    "\n",
    "# Binary flags for each campus\n",
    "binary_cols_created = []\n",
    "for campus in SCHOOL_CENTERS.keys():\n",
    "    col_name = f\"near_{campus.lower()}\"\n",
    "    df[col_name] = (df[\"campus_label\"] == campus).astype(int)\n",
    "    binary_cols_created.append(col_name)\n",
    "\n",
    "console.print(f\"[green]âœ“ Campus features created: {len(binary_cols_created)} near_* columns.[/green]\")\n",
    "console.print(f\"[green]âœ“ **campus_code** column is complete (non-proximal crimes = 0).[/green]\")\n",
    "\n",
    "# Distribution (including 'none')\n",
    "campus_table = Table(\n",
    "    title=\"Campus Distribution (including 'none')\",\n",
    "    show_header=True,\n",
    "    header_style=\"bold magenta\",\n",
    ")\n",
    "campus_table.add_column(\"Campus\", style=\"cyan\")\n",
    "campus_table.add_column(\"Count\", justify=\"right\", style=\"green\")\n",
    "campus_table.add_column(\"%\", justify=\"right\", style=\"yellow\")\n",
    "campus_table.add_column(\"Code\", justify=\"right\", style=\"white\")\n",
    "\n",
    "for campus, code in sorted(CAMPUS_ENCODING.items(), key=lambda x: x[1]):\n",
    "    count = (df[\"campus_label\"] == campus).sum()\n",
    "    pct = (count / len(df)) * 100\n",
    "    campus_table.add_row(campus, f\"{count:,}\", f\"{pct:.1f}%\", str(code))\n",
    "\n",
    "console.print(campus_table)\n",
    "\n",
    "show_missing_comparison(df, campus_before, \"Step 13\")\n",
    "log_step(\"Step 13: Campus distance and campus_code features\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c159e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "console.print(Panel(\"[bold cyan]BONUS: All Campus Buffers with NPU Overlay (2021-2024)[/bold cyan]\", border_style=\"cyan\"))\n",
    "\n",
    "# Filter data to 2021-2024 date range\n",
    "df_filtered = df[\n",
    "    (df['report_date'] >= '2021-01-01') & \n",
    "    (df['report_date'] <= '2024-12-31')\n",
    "].copy()\n",
    "\n",
    "console.print(f\"[yellow]Filtered to 2021-2024: {len(df_filtered):,} crimes[/yellow]\")\n",
    "\n",
    "# Load shapefiles\n",
    "gdf_npu = gpd.read_file(NPU_SHP).to_crs(3857)\n",
    "gdf_cities = gpd.read_file(CITIES_SHP).to_crs(3857)\n",
    "atlanta_boundary = gdf_cities[gdf_cities[\"NAME\"] == \"Atlanta\"].iloc[0].geometry\n",
    "\n",
    "# Identify NPU column\n",
    "candidate_npu_cols = [\"NPU\", \"NPU_ID\", \"NPU_NUM\", \"NPU_NAME\", \"NAME\"]\n",
    "npu_col_shp = next((c for c in candidate_npu_cols if c in gdf_npu.columns), gdf_npu.columns[0])\n",
    "\n",
    "# Create campus points and buffers (1.5 mile = 2414 meters)\n",
    "# Short names for map labels\n",
    "CAMPUS_SHORT_NAMES = {\n",
    "    \"GSU\": \"GSU\",\n",
    "    \"GA_Tech\": \"GA Tech\",\n",
    "    \"Emory\": \"Emory\",\n",
    "    \"Clark\": \"Clark\",\n",
    "    \"Spelman\": \"Spelman\",\n",
    "    \"Morehouse\": \"Morehouse\",\n",
    "    \"Morehouse_Med\": \"More. Med\",\n",
    "    \"Atlanta_Metro\": \"Atl Metro\",\n",
    "    \"Atlanta_Tech\": \"Atl Tech\",\n",
    "    \"SCAD\": \"SCAD\",\n",
    "    \"John_Marshall\": \"J. Marshall\",\n",
    "}\n",
    "\n",
    "campus_buffers = []\n",
    "campus_points = []\n",
    "\n",
    "for campus_name, (lat, lon) in SCHOOL_CENTERS.items():\n",
    "    pt = Point(lon, lat)\n",
    "    pt_3857 = gpd.GeoSeries([pt], crs=4326).to_crs(3857).iloc[0]\n",
    "    buffer = pt_3857.buffer(2414.016)\n",
    "    \n",
    "    campus_buffers.append({\n",
    "        'campus': campus_name,\n",
    "        'campus_short': CAMPUS_SHORT_NAMES[campus_name],\n",
    "        'geometry': buffer,\n",
    "        'lat': lat,\n",
    "        'lon': lon\n",
    "    })\n",
    "    \n",
    "    campus_points.append({\n",
    "        'campus': campus_name,\n",
    "        'campus_short': CAMPUS_SHORT_NAMES[campus_name],\n",
    "        'geometry': pt_3857\n",
    "    })\n",
    "\n",
    "gdf_buffers = gpd.GeoDataFrame(campus_buffers, crs=3857)\n",
    "gdf_points = gpd.GeoDataFrame(campus_points, crs=3857)\n",
    "\n",
    "# Count crimes per campus buffer (using filtered data)\n",
    "gdf_crimes = gpd.GeoDataFrame(\n",
    "    df_filtered.dropna(subset=['longitude', 'latitude']),\n",
    "    geometry=gpd.points_from_xy(df_filtered.longitude, df_filtered.latitude),\n",
    "    crs=4326\n",
    ").to_crs(3857)\n",
    "\n",
    "buffer_counts = []\n",
    "for idx, row in gdf_buffers.iterrows():\n",
    "    crimes_in_buffer = gdf_crimes[gdf_crimes.geometry.within(row.geometry)]\n",
    "    buffer_counts.append(len(crimes_in_buffer))\n",
    "\n",
    "gdf_buffers['crime_count'] = buffer_counts\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(22, 22))\n",
    "\n",
    "# 1. NPU boundaries with labels\n",
    "gdf_npu.boundary.plot(ax=ax, edgecolor='black', linewidth=1.5, alpha=0.7, zorder=1)\n",
    "\n",
    "# Add NPU labels (e.g., \"NPU A\", \"NPU B\")\n",
    "for idx, row in gdf_npu.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    npu_label = row[npu_col_shp]\n",
    "    \n",
    "    # Format as \"NPU X\" if not already formatted\n",
    "    if not str(npu_label).upper().startswith('NPU'):\n",
    "        npu_label = f\"NPU {npu_label}\"\n",
    "    \n",
    "    ax.text(\n",
    "        centroid.x,\n",
    "        centroid.y,\n",
    "        npu_label,\n",
    "        fontsize=8,\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        style='italic',\n",
    "        color='#333333',\n",
    "        alpha=0.6,\n",
    "        zorder=2\n",
    "    )\n",
    "\n",
    "# 2. Atlanta city boundary\n",
    "gpd.GeoSeries([atlanta_boundary]).boundary.plot(ax=ax, edgecolor='black', linewidth=2.5, zorder=3)\n",
    "\n",
    "# 3. Campus buffers with distinct colors\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(gdf_buffers)))\n",
    "\n",
    "for idx, (_, row) in enumerate(gdf_buffers.iterrows()):\n",
    "    gpd.GeoSeries([row.geometry]).plot(\n",
    "        ax=ax,\n",
    "        facecolor=colors[idx],\n",
    "        edgecolor=colors[idx],\n",
    "        linewidth=2,\n",
    "        alpha=0.25,\n",
    "        zorder=4,\n",
    "        label=row['campus_short']  # Use short name for legend\n",
    "    )\n",
    "\n",
    "# 4. Campus center points\n",
    "gdf_points.plot(\n",
    "    ax=ax,\n",
    "    color='red',\n",
    "    markersize=180,\n",
    "    marker='*',\n",
    "    edgecolor='white',\n",
    "    linewidth=2,\n",
    "    zorder=6\n",
    ")\n",
    "\n",
    "# 5. Campus labels with crime counts (use short names)\n",
    "for idx, row in gdf_buffers.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    crime_count = row['crime_count']\n",
    "    label = f\"{row['campus_short']}\\n({crime_count:,})\"\n",
    "    \n",
    "    ax.text(\n",
    "        centroid.x,\n",
    "        centroid.y,\n",
    "        label,\n",
    "        fontsize=8,\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        fontweight='bold',\n",
    "        bbox=dict(boxstyle='round,pad=0.4', facecolor='white', edgecolor='black', alpha=0.85, linewidth=1),\n",
    "        zorder=7\n",
    "    )\n",
    "\n",
    "# 6. Add basemap\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron, zoom=12)\n",
    "\n",
    "ax.set_title(\n",
    "    \"Atlanta Campus Crime Zones (1.5-mile radius)\\n\"\n",
    "    f\"Period: January 1, 2021 - December 31, 2024 | Total Crimes: {len(df_filtered):,}\",\n",
    "    fontsize=16,\n",
    "    fontweight='bold',\n",
    "    pad=20\n",
    ")\n",
    "ax.axis('off')\n",
    "\n",
    "# Add compact legend (2 columns to save space)\n",
    "legend = ax.legend(\n",
    "    title='Campus Zones (1.5mi radius)',\n",
    "    loc='upper left',\n",
    "    fontsize=9,\n",
    "    framealpha=0.95,\n",
    "    ncol=2,\n",
    "    columnspacing=1,\n",
    "    handlelength=1.5,\n",
    "    title_fontsize=10\n",
    ")\n",
    "\n",
    "# Add additional legend elements at bottom\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='black', linewidth=1.5, label='NPU Boundaries'),\n",
    "    Line2D([0], [0], color='black', linewidth=2.5, label='Atlanta City Limit'),\n",
    "    Line2D([0], [0], marker='*', color='w', markerfacecolor='red', \n",
    "           markersize=12, markeredgecolor='white', markeredgewidth=1.5, label='Campus Center'),\n",
    "]\n",
    "\n",
    "legend2 = ax.legend(\n",
    "    handles=legend_elements,\n",
    "    loc='lower left',\n",
    "    fontsize=9,\n",
    "    framealpha=0.95,\n",
    "    title='Map Elements'\n",
    ")\n",
    "\n",
    "# Add first legend back (matplotlib removes it when adding second)\n",
    "ax.add_artist(legend)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = FIGURES_DIR / \"12_all_campus_buffers_with_npu.png\"\n",
    "plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "console.print(f\"[dim cyan]  ğŸ’¾ Saved: {fig_path.name}[/dim cyan]\")\n",
    "plt.show()\n",
    "\n",
    "# Print summary table (sorted by crime count)\n",
    "console.print(\"\\n[bold cyan]Campus Zone Crime Summary (2021-2024)[/bold cyan]\")\n",
    "summary_table = Table(title=\"Crimes within 1.5 miles of each campus\", show_header=True)\n",
    "summary_table.add_column(\"Campus\", style=\"cyan\", no_wrap=True)\n",
    "summary_table.add_column(\"Short Name\", style=\"yellow\")\n",
    "summary_table.add_column(\"Crime Count\", justify=\"right\", style=\"green\")\n",
    "summary_table.add_column(\"% of Total\", justify=\"right\", style=\"magenta\")\n",
    "\n",
    "gdf_buffers_sorted = gdf_buffers.sort_values('crime_count', ascending=False)\n",
    "\n",
    "for _, row in gdf_buffers_sorted.iterrows():\n",
    "    pct = (row['crime_count'] / len(df_filtered)) * 100\n",
    "    summary_table.add_row(\n",
    "        row['campus'],\n",
    "        row['campus_short'],\n",
    "        f\"{row['crime_count']:,}\",\n",
    "        f\"{pct:.1f}%\"\n",
    "    )\n",
    "\n",
    "console.print(summary_table)\n",
    "\n",
    "# Overlap analysis\n",
    "console.print(\"\\n[bold yellow]Zone Overlap Analysis (2021-2024)[/bold yellow]\")\n",
    "crime_zone_counts = []\n",
    "for idx, crime in gdf_crimes.iterrows():\n",
    "    zones = sum(1 for _, buffer_row in gdf_buffers.iterrows() if crime.geometry.within(buffer_row.geometry))\n",
    "    crime_zone_counts.append(zones)\n",
    "\n",
    "crimes_in_no_zone = sum(1 for c in crime_zone_counts if c == 0)\n",
    "crimes_in_one_zone = sum(1 for c in crime_zone_counts if c == 1)\n",
    "crimes_in_multiple_zones = sum(1 for c in crime_zone_counts if c > 1)\n",
    "max_overlaps = max(crime_zone_counts) if crime_zone_counts else 0\n",
    "\n",
    "overlap_info = Table(show_header=False, title=\"Zone Overlap Statistics\")\n",
    "overlap_info.add_column(\"Metric\", style=\"cyan\")\n",
    "overlap_info.add_column(\"Value\", style=\"green\")\n",
    "\n",
    "overlap_info.add_row(\"Crimes outside all zones\", f\"{crimes_in_no_zone:,} ({(crimes_in_no_zone/len(df_filtered))*100:.1f}%)\")\n",
    "overlap_info.add_row(\"Crimes in exactly 1 zone\", f\"{crimes_in_one_zone:,} ({(crimes_in_one_zone/len(df_filtered))*100:.1f}%)\")\n",
    "overlap_info.add_row(\"Crimes in multiple zones\", f\"{crimes_in_multiple_zones:,} ({(crimes_in_multiple_zones/len(df_filtered))*100:.1f}%)\")\n",
    "overlap_info.add_row(\"Max zones overlapping\", str(max_overlaps))\n",
    "\n",
    "console.print(overlap_info)\n",
    "\n",
    "log_step(\"Bonus: Campus buffers visualization (2021-2024) with NPU labels\", df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ca741c",
   "metadata": {},
   "source": [
    "#### Section 9: Full 2021â€“2025 Weather Refresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f3e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\n",
    "    Panel(\"[bold cyan]STEP 14: Fetch and merge full weather dataset (2021â€“2025) - Fixes missing 2025 data[/bold cyan]\", border_style=\"cyan\")\n",
    ")\n",
    "\n",
    "weather_before: Dict[str, Any] = {}\n",
    "weather_cols_check = [\n",
    "    \"temp_f\",\n",
    "    \"precip_in\",\n",
    "    \"rain_in\",\n",
    "    \"apparent_temp_f\",\n",
    "    \"weather_code_hourly\",\n",
    "    \"is_daylight\",\n",
    "    \"daylight_duration_sec\",\n",
    "    \"sunshine_duration_sec\",\n",
    "]\n",
    "for col in weather_cols_check:\n",
    "    # Ensure checking against the column names created in the prior merge\n",
    "    current_col = col if col in df.columns else (\n",
    "        'weather_code_hrly' if col == 'weather_code_hourly' else col\n",
    "    )\n",
    "    if current_col in df.columns:\n",
    "        missing = df[current_col].isna().sum()\n",
    "        pct = (missing / len(df)) * 100\n",
    "        weather_before[col] = (missing, pct)\n",
    "\n",
    "df[\"report_date\"] = pd.to_datetime(df[\"report_date\"])\n",
    "\n",
    "cache_session = requests_cache.CachedSession(\".cache\", expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "params = {\n",
    "    \"latitude\": 33.749,\n",
    "    \"longitude\": -84.388,\n",
    "    \"start_date\": \"2021-01-01\",\n",
    "    \"end_date\": \"2025-11-30\", # Extended to ensure all 2025 data is captured\n",
    "    \"hourly\": [\n",
    "        \"temperature_2m\",\n",
    "        \"precipitation\",\n",
    "        \"rain\",\n",
    "        \"apparent_temperature\",\n",
    "        \"weather_code\",\n",
    "        \"is_day\",\n",
    "    ],\n",
    "    \"daily\": [\n",
    "        \"sunrise\",\n",
    "        \"daylight_duration\",\n",
    "        \"sunshine_duration\",\n",
    "        \"precipitation_hours\",\n",
    "        \"rain_sum\",\n",
    "        \"temperature_2m_mean\",\n",
    "        \"weather_code\",\n",
    "    ],\n",
    "    \"timezone\": \"America/New_York\",\n",
    "    \"temperature_unit\": \"fahrenheit\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    console.print(\"[cyan]Fetching weather from Open-Meteo API...[/cyan]\")\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "    response = responses[0]\n",
    "\n",
    "    hourly = response.Hourly()\n",
    "    hourly_df = pd.DataFrame(\n",
    "        {\n",
    "            \"datetime\": pd.date_range(\n",
    "                start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "                inclusive=\"left\",\n",
    "            ),\n",
    "            \"temp_f\": hourly.Variables(0).ValuesAsNumpy(),\n",
    "            \"precip_in\": hourly.Variables(1).ValuesAsNumpy(),\n",
    "            \"rain_in\": hourly.Variables(2).ValuesAsNumpy(),\n",
    "            \"apparent_temp_f\": hourly.Variables(3).ValuesAsNumpy(),\n",
    "            \"weather_code_hourly\": hourly.Variables(4).ValuesAsNumpy(),\n",
    "            \"is_daylight\": hourly.Variables(5).ValuesAsNumpy().astype(int),\n",
    "        }\n",
    "    )\n",
    "    hourly_df[\"datetime\"] = (\n",
    "        hourly_df[\"datetime\"].dt.tz_convert(\"America/New_York\").dt.tz_localize(None)\n",
    "    )\n",
    "\n",
    "    daily = response.Daily()\n",
    "    daily_df = pd.DataFrame(\n",
    "        {\n",
    "            \"date\": pd.date_range(\n",
    "                start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=daily.Interval()),\n",
    "                inclusive=\"left\",\n",
    "            ),\n",
    "            \"sunrise\": daily.Variables(0).ValuesInt64AsNumpy(),\n",
    "            \"daylight_duration_sec\": daily.Variables(1).ValuesAsNumpy(),\n",
    "            \"sunshine_duration_sec\": daily.Variables(2).ValuesAsNumpy(),\n",
    "            \"precip_hours\": daily.Variables(3).ValuesAsNumpy(),\n",
    "            \"rain_sum_in\": daily.Variables(4).ValuesAsNumpy(),\n",
    "            \"temp_mean_f\": daily.Variables(5).ValuesAsNumpy(),\n",
    "            \"weather_code_daily\": daily.Variables(6).ValuesAsNumpy(),\n",
    "        }\n",
    "    )\n",
    "    daily_df[\"date\"] = (\n",
    "        daily_df[\"date\"]\n",
    "        .dt.tz_convert(\"America/New_York\")\n",
    "        .dt.tz_localize(None)\n",
    "        .dt.date\n",
    "    )\n",
    "\n",
    "    console.print(\n",
    "        f\"[green]âœ“ Fetched {len(hourly_df):,} hourly and {len(daily_df):,} daily weather records.[/green]\"\n",
    "    )\n",
    "    \n",
    "    # === FIX: Save the newly fetched data to the external folder ===\n",
    "    hourly_df.to_csv(HOURLY_WEATHER_PATH, index=False)\n",
    "    daily_df.to_csv(DAILY_WEATHER_PATH, index=False)\n",
    "    console.print(f\"[green]âœ“ Saved updated hourly data to: {HOURLY_WEATHER_PATH}[/green]\")\n",
    "    console.print(f\"[green]âœ“ Saved updated daily data to: {DAILY_WEATHER_PATH}[/green]\")\n",
    "    # ====================================================================\n",
    "\n",
    "    # Merge hourly weather\n",
    "    df[\"weather_datetime\"] = df[\"report_date\"].dt.floor(\"H\")\n",
    "\n",
    "    overlapping_hourly_cols = [\n",
    "        col for col in df.columns if col in hourly_df.columns and col != \"weather_datetime\"\n",
    "    ]\n",
    "    if overlapping_hourly_cols:\n",
    "        df = df.drop(columns=overlapping_hourly_cols)\n",
    "\n",
    "    df = df.merge(\n",
    "        hourly_df,\n",
    "        left_on=\"weather_datetime\",\n",
    "        right_on=\"datetime\",\n",
    "        how=\"left\",\n",
    "    ).drop(columns=[\"datetime\"])\n",
    "\n",
    "    # Merge daily weather\n",
    "    df[\"report_date_only\"] = df[\"report_date\"].dt.date\n",
    "    overlapping_daily_cols = [\n",
    "        col for col in df.columns if col in daily_df.columns and col != \"date\"\n",
    "    ]\n",
    "    if overlapping_daily_cols:\n",
    "        df = df.drop(columns=overlapping_daily_cols)\n",
    "\n",
    "    df = df.merge(\n",
    "        daily_df,\n",
    "        left_on=\"report_date_only\",\n",
    "        right_on=daily_df['date'].dt.date,\n",
    "        how=\"left\",\n",
    "    ).drop(columns=[\"date\", \"report_date_only\", \"key_0\"])\n",
    "\n",
    "    df = df.drop(columns=['weather_datetime'], errors='ignore')\n",
    "\n",
    "    console.print(\"[green]âœ“ Merged full 2021â€“2025 weather successfully.[/green]\")\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]Error fetching weather:[/bold red] {e}\")\n",
    "\n",
    "show_missing_comparison(df, weather_before, \"Step 14\")\n",
    "log_step(\"Step 14: Fetched and merged full weather dataset (2021â€“2025)\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6985e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(Panel(\"[bold cyan]STEP 15: Recalculate temperature-based flags[/bold cyan]\", border_style=\"cyan\"))\n",
    "\n",
    "if \"temp_f\" in df.columns:\n",
    "    p85 = df[\"temp_f\"].quantile(0.85)\n",
    "    p15 = df[\"temp_f\"].quantile(0.15)\n",
    "\n",
    "    console.print(\"[cyan]Temperature percentile thresholds:[/cyan]\")\n",
    "    console.print(f\"Â  85th percentile: [yellow]{p85:.1f}[/yellow] F\")\n",
    "    console.print(f\"Â  15th percentile: [yellow]{p15:.1f}[/yellow] F\")\n",
    "\n",
    "    df[\"is_hot\"] = (df[\"temp_f\"] >= p85).astype(int)\n",
    "    df[\"is_cold\"] = (df[\"temp_f\"] <= p15).astype(int)\n",
    "\n",
    "    hot_pct = (df[\"is_hot\"] == 1).sum() / len(df) * 100\n",
    "    cold_pct = (df[\"is_cold\"] == 1).sum() / len(df) * 100\n",
    "\n",
    "    console.print(\"[green]âœ“ Recalculated temperature flags:[/green]\")\n",
    "    console.print(f\"Â  is_hot (>= {p85:.1f} F): {hot_pct:.1f}%\")\n",
    "    console.print(f\"Â  is_cold (<= {p15:.1f} F): {cold_pct:.1f}%\")\n",
    "\n",
    "log_step(\"Step 15: Recalculated temperature-based flags\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0ad622",
   "metadata": {},
   "source": [
    "#### Section 10: Final Verification, Save, and Core EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(Panel(\"[bold cyan]STEP 16: Final verification and save[/bold cyan]\", border_style=\"cyan\"))\n",
    "\n",
    "# Final missing data summary\n",
    "all_cols_to_check = ['campus_distance_m', 'neighborhood', 'neighborhood_label', 'zone_int', 'npu', 'city_label', 'weather_code_hrly', 'is_daylight', 'temp_f', 'precip_in'] # Top 10 columns only\n",
    "final_missing: Dict[str, Any] = {}\n",
    "\n",
    "console.print(\"\\n[yellow]Final missing data summary (Top 10 columns only):[/yellow]\")\n",
    "final_table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "final_table.add_column(\"Column\", style=\"cyan\")\n",
    "final_table.add_column(\"Missing\", justify=\"right\", style=\"red\")\n",
    "final_table.add_column(\"%\", justify=\"right\", style=\"yellow\")\n",
    "\n",
    "for col in all_cols_to_check:\n",
    "    # Use fallback column names if needed\n",
    "    current_col = col if col in df.columns else (\n",
    "        'zone_int' if col == 'district' else (\n",
    "            'npu_label' if col == 'npu' else col\n",
    "        )\n",
    "    )\n",
    "    if current_col in df.columns:\n",
    "        missing = df[current_col].isna().sum()\n",
    "        pct = (missing / len(df)) * 100\n",
    "        final_table.add_row(col, f\"{missing:,}\", f\"{pct:.1f}%\")\n",
    "\n",
    "console.print(final_table)\n",
    "\n",
    "# Save final processed dataset\n",
    "OUTPUT_PATH = PROCESSED_DATA_FOLDER / \"target_crimes.csv\"\n",
    "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "# New Step 16b: Parquet Conversion and Export\n",
    "\n",
    "console.print(Panel(\"[bold yellow]STEP 16b: Convert and Export to Parquet (for Git/ML efficiency)[/bold yellow]\", border_style=\"yellow\"))\n",
    "\n",
    "# 1. Define Parquet output path\n",
    "PARQUET_OUTPUT_PATH = PROCESSED_DATA_FOLDER / \"target_crimes.parquet\"\n",
    "\n",
    "# 2. Save to Parquet format (using fast compression)\n",
    "\n",
    "try:\n",
    "    df.to_parquet(PARQUET_OUTPUT_PATH, index=False, compression='snappy')\n",
    "    \n",
    "    # 3. Update the log\n",
    "    log_step(\"Processed Data Export (Parquet)\", df)\n",
    "\n",
    "    console.print(\n",
    "        Panel.fit(\n",
    "            f\"[bold yellow]âœ“ PARQUET CONVERSION COMPLETE![/bold yellow]\\n\\n\"\n",
    "            f\"Saved to: [green]{PARQUET_OUTPUT_PATH}[/green]\\n\"\n",
    "            \"Status: CSV kept for MS Data Wrangler, PARQUET used for Git/ML.\",\n",
    "            border_style=\"yellow\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]ERROR saving Parquet:[/bold red] {e}\\n[yellow]Check if 'pyarrow' is installed.[/yellow]\")\n",
    "\n",
    "log_step(\"Processed Data Export\", df)\n",
    "\n",
    "console.print(\n",
    "    Panel.fit(\n",
    "        \"[bold green]âœ“ CLEANING COMPLETE![/bold green]\\n\\n\"\n",
    "        f\"Saved to: [yellow]{OUTPUT_PATH}[/yellow]\\n\"\n",
    "        f\"Total records: [cyan]{len(df):,}[/cyan]\\n\"\n",
    "        f\"Total columns: [cyan]{len(df.columns)}[/cyan]\",\n",
    "        border_style=\"green\",\n",
    "    )\n",
    ")\n",
    "\n",
    "show_pipeline_table()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a9c811",
   "metadata": {},
   "source": [
    "## âœ… Pipeline Complete - Next Steps\n",
    "\n",
    "**Data is now ready!** Proceed to:\n",
    "- **`02_explorer.ipynb`** - Visualize patterns and gain insights\n",
    "- **`03_modelt.ipynb`** - Build predictive models (requires exploration first)\n",
    "\n",
    "### Output Files Created:\n",
    "```\n",
    "data/\n",
    "â”œâ”€â”€ interim/apd/\n",
    "â”‚   â””â”€â”€ apd_model_data_target_crimes.csv  (Step 10 - filtered for target offenses)\n",
    "â”œâ”€â”€ processed/apd/\n",
    "â”‚   â””â”€â”€ target_crimes.csv                 (Step 16 - final analysis-ready dataset)\n",
    "â””â”€â”€ external/\n",
    "    â”œâ”€â”€ atlanta_hourly_weather_2024.csv\n",
    "    â””â”€â”€ atlanta_daily_weather_2024.csv\n",
    "```\n",
    "\n",
    "### Quality Checks Passed:\n",
    "âœ… No duplicate incident numbers  \n",
    "âœ… All dates standardized and valid  \n",
    "âœ… Spatial features enriched (NPU, zones, campuses)  \n",
    "âœ… Weather data merged (2021-2025)  \n",
    "âœ… Campus proximity calculated  \n",
    "âœ… Temporal features engineered  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d982adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary Statistics\n",
    "console.print(\"\\n[bold magenta]â•â•â• Key Statistics â•â•â•[/bold magenta]\\n\")\n",
    "\n",
    "stats_table = Table(show_header=False, show_lines=True)\n",
    "stats_table.add_column(\"Metric\", style=\"cyan\")\n",
    "stats_table.add_column(\"Value\", style=\"green\")\n",
    "\n",
    "stats_table.add_row(\"Total crimes processed\", f\"{len(df):,}\")\n",
    "stats_table.add_row(\"Date range\", f\"{df['report_date'].min().date()} to {df['report_date'].max().date()}\")\n",
    "stats_table.add_row(\"Features created\", f\"{len(df.columns)} columns\")\n",
    "stats_table.add_row(\"Target offenses\", \"Larceny, theft, robbery, burglary, fraud, etc.\")\n",
    "\n",
    "console.print(stats_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c848b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final data preview\n",
    "console.print(\"\\n[bold cyan]â•â•â• Final Dataset Preview â•â•â•[/bold cyan]\\n\")\n",
    "console.print(df.head(10))\n",
    "console.print(f\"\\n[bold green]âœ“ 01_wrangler.ipynb complete![/bold green]\")\n",
    "console.print(f\"[cyan]Next: Run 02_explorer.ipynb for visualizations[/cyan]\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "dsci_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
